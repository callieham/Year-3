{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Prerequisites\n",
        "\n",
        "- Basic familiarity with [Numpy](https://numpy.org/doc/stable/user/quickstart.html)\n",
        "- Basic familiarity with [Pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html)"
      ],
      "metadata": {
        "id": "3wfZ-jB19q_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outline\n",
        "\n",
        "- [Section 0](#section-0): NumPy Tips and Code Clarity\n",
        "- [Section 1](#section-1): Intro to Linear Regression\n",
        "- [Section 2](#section-2): Least Squared Loss and Maximum Likelihood\n",
        "- [Section 3](#section-3): Ridge Regression\n",
        "- [Section 4](#section-4): LASSO Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "yW1CvhQ5G9c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-0\"></a>\n",
        "\n",
        "# Section 0: NumPy Tips and Code Clarity\n",
        "\n",
        "\n",
        "There are multiple ways in NumPy to do each of the following basic operations:\n",
        "\n",
        "- Matrix-matrix and matrix-vector product.\n",
        "- Matrix-matrix and vector-vector element-wise product.\n",
        "- vector-vector inner and outer products.\n",
        "\n",
        "Avoid using general functions such as `np.dot` that handles most of these operations depending on the shapes of the input parameters.\n",
        "\n",
        "\n",
        "Note: to check a function documentation, you can do that inside a Nootbook cell using `?<function_name>`.\n"
      ],
      "metadata": {
        "id": "CcgaQJ0GIegF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "?np.dot"
      ],
      "metadata": {
        "id": "LQsn3a3bImnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape: (2, 2)\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "\n",
        "# shape: (2, 2)\n",
        "B = np.array([[5, 6], [7, 8]])\n",
        "\n",
        "# shape: (2, )\n",
        "v1 =  np.array([5, 6])\n",
        "\n",
        "# shape: (3, )\n",
        "v2 = np.array([4, 5, 6])\n",
        "\n",
        "# shape: (2, )\n",
        "v3 = np.arange(8, 11)\n",
        "\n"
      ],
      "metadata": {
        "id": "ImU9lXymIm-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_matrix_matrix = np.dot(A, B)\n",
        "# result_matrix_matrix = A @ B # Better\n",
        "\n",
        "\n",
        "result_matrix_matrix"
      ],
      "metadata": {
        "id": "jPJvKRjmPDYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result_matrix_vector = np.dot(A, v1)\n",
        "# result_matrix_vector = A @ v1 # Better\n",
        "result_matrix_vector"
      ],
      "metadata": {
        "id": "4h13M_1MN2_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_inner_product = np.dot(v2, v3)\n",
        "# result_inner_product = np.inner(v2, v3)\n",
        "result_inner_product"
      ],
      "metadata": {
        "id": "6jTafZCkN3Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are several issues in using `np.dot` in the previous cell**. It can be very confusing to read a code with `np.dot` as you are trying to understand what operation is actually intended. The reader is required first to read the documentation of `np.dot` and then probe the shape of the input parameters to interpret the expression.\n",
        "\n",
        "\n",
        "To avoid confusion, consider the following practice:\n",
        "\n",
        "- Use the explicit `@` operator for matrix-matrix and matrix-vector products.\n",
        "- Use the explicit `*` operator for element-wise products or [broadcasted products](https://numpy.org/doc/stable/user/basics.broadcasting.html).\n",
        "- Use the explicit `np.inner` function for innter product between 1-D vectors, same for `np.outer` function."
      ],
      "metadata": {
        "id": "rvEL7m-sMdbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you intend to work with an object as a 1-D vector, make sure you don't have excessive dimensions with size 1 of your array."
      ],
      "metadata": {
        "id": "WtFPrcKXSjR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shape: (1, 2)\n",
        "v4 = np.array([[5, 6]])\n",
        "\n",
        "# shape: (2, 1)\n",
        "v5 = np.array([[5], [6]])\n",
        "\n",
        "# This works due to broadcasting rules.\n",
        "v4 * v5"
      ],
      "metadata": {
        "id": "7SgGI74oTIZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f'v4.shape: {v4.shape}, v5.shape: {v5.shape},  v4.ndim: {v4.ndim},  v5.ndim: {v5.ndim}'"
      ],
      "metadata": {
        "id": "PL74DMtdT0Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fix this and deal with them as vectors:"
      ],
      "metadata": {
        "id": "gBsFJJ5HTef5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v4 = v4.squeeze() # Now shape: (2, )\n",
        "v5 = v5.squeeze() # Now shape: (2, )\n",
        "\n",
        "f'v4.shape: {v4.shape}, v5.shape: {v5.shape},  v4.ndim: {v4.ndim},  v5.ndim: {v5.ndim}'\n"
      ],
      "metadata": {
        "id": "0QeY9-QgTbdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v4 * v5"
      ],
      "metadata": {
        "id": "G-1rcHHfULV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "General practice to improve readability and avoid unexpected behaviour:\n",
        "\n",
        "- When possible use explicit expressions instead of general functions like `np.dot`.\n",
        "- Ensure objects representing 1-D vectors have `ndim==1`. If you are writing a function that deals with vectors and parameters use `assert` statements to make sure the input parameters match what you expect.\n",
        "- Never use the deprectated `np.matrix` class but always `np.array`."
      ],
      "metadata": {
        "id": "lB1Mr5nfUSJL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzLYzhtDt9Gl"
      },
      "source": [
        "# Linear Regression\n",
        "\n",
        "\n",
        "<a name=\"section-1\"></a>\n",
        "\n",
        "##  Section 1: Intro to Linear regression\n",
        "\n",
        "*Partly adapted from [Deisenroth, Faisal, Ong (2020)](https://mml-book.github.io/).*\n",
        "\n",
        "The purpose of this notebook is to practice implementing some linear algebra (equations provided) and to explore some properties of linear regression.\n",
        "\n",
        "We will solely rely on the Python packages numpy and matplotlib, and you are not allowed to use any package that has a complete linear regression framework implemented (e.g., scikit-learn).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgdhMY0gu00z"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Initial global configuration for matplotlib\n",
        "SMALL_SIZE = 12\n",
        "MEDIUM_SIZE = 16\n",
        "BIGGER_SIZE = 20\n",
        "\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrdWPxiMrRvu"
      },
      "source": [
        "We consider a linear regression problem of the form\n",
        "$$\n",
        "y = \\boldsymbol x^T\\boldsymbol\\beta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n",
        "$$\n",
        "where $\\boldsymbol x\\in\\mathbb{R}^{(p+1)}$ are inputs and $y\\in\\mathbb{R}$ are noisy observations. The parameter vector $\\boldsymbol\\beta\\in\\mathbb{R}^{(p+1)}$ parametrizes the function.\n",
        "\n",
        "We assume we have a training set $(\\boldsymbol x_n, y_n)$, $n=1,\\ldots, N$. We summarize the sets of training inputs in $\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T$ and corresponding training targets $\\boldsymbol y = [y_1, \\ldots, y_N]^T$, respectively.\n",
        "\n",
        "In this tutorial, we are interested in finding parameters $\\boldsymbol\\beta$ that map the inputs well to the ouputs.\n",
        "\n",
        "From our lectures, we know that the parameters $\\boldsymbol\\beta$ found by the following equation are optimal:\n",
        "$$\n",
        "\\underset{\\boldsymbol\\beta}{\\text{min}} \\| \\boldsymbol y - \\boldsymbol X \\boldsymbol\\beta \\|^2 = \\underset{\\boldsymbol\\beta}{\\text{min}} \\ \\text{L}_{\\text{LS}} (\\boldsymbol\\beta)\n",
        "$$\n",
        "where $\\text{L}_{\\text{LS}}$ is the (ordinary) least squares loss function.\n",
        "### Dataset generation\n",
        "We will start with a simple training set, that we define by ourselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvAyeBZort-_"
      },
      "source": [
        "# Define training set\n",
        "X = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1\n",
        "y = np.array([-1.2, -0.7, 0.14, 0.67, 1.67]).reshape(-1,1) # 5x1 vector\n",
        "\n",
        "# Plot the training set\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(X, y, '+', markersize=10, label='Data')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.axvline(0, color='black')\n",
        "plt.axhline(0, color='black')\n",
        "plt.ylim(-5, 5)\n",
        "plt.xlim(-5,5)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "o1tkhkamE_f0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXcgW-H4sJJ1"
      },
      "source": [
        "<a name=\"section-2\"></a>\n",
        "\n",
        "## Section 2: Least squares loss and Maximum likelihood\n",
        "\n",
        "From our lectures, we know that the parameters $\\boldsymbol\\beta$ found by optimizing following equation:\n",
        "$$\n",
        "\\underset{\\boldsymbol\\beta}{\\text{min}} \\| \\boldsymbol y - \\boldsymbol X \\boldsymbol\\beta \\|^2 = \\underset{\\boldsymbol\\beta}{\\text{min}} \\ \\text{L}_{\\text{LS}} (\\boldsymbol\\beta)\n",
        "$$\n",
        "where $\\text{L}_{\\text{LS}}$ is the (ordinary) least squares loss function. The solution is\n",
        "$$\n",
        "\\boldsymbol\\beta^{*} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y \\ \\in\\mathbb{R}^{(p+1)}\\,,\n",
        "$$\n",
        "where\n",
        "$$\n",
        "\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times (p+1)}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n",
        "$$\n",
        "\n",
        "\n",
        "The same estimate of $\\boldsymbol\\beta$ we can be obtained by maximum liklihood estimation which gives statistical interpretation of linear regression. In maximum likelihood estimation, we can find the parameters $\\boldsymbol\\beta^{\\mathrm{ML}}$ that maximize the likelihood\n",
        "$$\n",
        "p(\\boldsymbol y | \\boldsymbol X, \\boldsymbol\\beta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\beta)\\,.\n",
        "$$\n",
        "From the lecture we know that the maximum likelihood estimator is given by\n",
        "$$\n",
        "\\boldsymbol\\beta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y \\, .\n",
        "$$\n",
        "\n",
        "Let us compute the maximum likelihood estimate for the given training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "517e-cosr6Om"
      },
      "source": [
        "## EDIT THIS FUNCTION\n",
        "def max_lik_estimate(X, y):\n",
        "\n",
        "    # X: N x D matrix of training inputs\n",
        "    # y: N x 1 vector of training targets/observations\n",
        "    # returns: maximum likelihood parameters (D x 1)\n",
        "\n",
        "    N, D = X.shape\n",
        "    beta_ml = np.zeros((D,1)) ## <-- EDIT THIS LINE\n",
        "    return beta_ml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZr2lngBuAo6"
      },
      "source": [
        "# get maximum likelihood estimate\n",
        "beta_ml = max_lik_estimate(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjUdOCVht9aW"
      },
      "source": [
        "Now, make a prediction using the maximum likelihood estimate that we just found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBErjQspt93g"
      },
      "source": [
        "## EDIT THIS FUNCTION\n",
        "def predict_with_estimate(X_test, beta):\n",
        "\n",
        "    # X_test: K x D matrix of test inputs\n",
        "    # beta: D x 1 vector of parameters\n",
        "    # returns: prediction of f(X_test); K x 1 vector\n",
        "\n",
        "    prediction = X_test ## <-- EDIT THIS LINE\n",
        "\n",
        "    return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-jS04fBugPO"
      },
      "source": [
        "Let's see whether we got something useful:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJVfgk7gujGu"
      },
      "source": [
        "# define a test set\n",
        "X_test = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n",
        "#beta_ml = np.array([[0.2]]) -> changes slope\n",
        "# predict the function values at the test points using the maximum likelihood estimator\n",
        "ml_prediction = predict_with_estimate(X_test, beta_ml)\n",
        "\n",
        "# plot\n",
        "plt.figure()\n",
        "plt.plot(X, y, '+', markersize=10, label='Data')\n",
        "plt.plot(X_test, ml_prediction, label='Linear Regression Fit')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.axvline(0, color='black')\n",
        "plt.axhline(0, color='black')\n",
        "plt.ylim(-5, 5)\n",
        "plt.xlim(-5,5)\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DNSk4tXvEhO"
      },
      "source": [
        "#### Questions\n",
        "1. Does the solution above look reasonable?\n",
        "2. Play around with different values of $\\beta$. How do the corresponding functions change?\n",
        "3. Modify the training targets $\\mathcal Y$ and re-run your computation. What changes?\n",
        "\n",
        "Let us now look at a different training set, where we add 2.0 to every $y$-value, and compute the maximum likelihood estimate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXsZCHgAvE-6"
      },
      "source": [
        "ynew = y + 2.0\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(X, ynew, '+', markersize=10, label='Data')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.axvline(0, color='black')\n",
        "plt.axhline(0, color='black')\n",
        "plt.ylim(-5, 5)\n",
        "plt.xlim(-5,5)\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8qYt3V_vPlG"
      },
      "source": [
        "# get maximum likelihood estimate\n",
        "beta_ml = max_lik_estimate(X, ynew)\n",
        "print(beta_ml)\n",
        "\n",
        "# define a test set\n",
        "X_test = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n",
        "\n",
        "# predict the function values at the test points using the maximum likelihood estimator\n",
        "ml_prediction = predict_with_estimate(X_test, beta_ml)\n",
        "\n",
        "# plot\n",
        "plt.figure()\n",
        "plt.plot(X, ynew, '+', markersize=10, label='Data')\n",
        "plt.plot(X_test, ml_prediction, label='Linear Regression Fit')\n",
        "plt.axvline(0, color='black')\n",
        "plt.axhline(0, color='black')\n",
        "plt.ylim(-5, 5)\n",
        "plt.xlim(-5,5)\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAobMbOava5j"
      },
      "source": [
        "#### Question:\n",
        "1. This maximum likelihood estimate doesn't look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\n",
        "2. How can we fix this problem?\n",
        "\n",
        "Let us now define a linear regression model that is slightly more flexible:\n",
        "$$\n",
        "y = \\beta_0 + \\boldsymbol x^T \\boldsymbol\\beta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n",
        "$$\n",
        "\n",
        "Here, we added an offset (also called intercept) parameter $\\beta_0$ to our original model.\n",
        "\n",
        "#### Question:\n",
        "1. What is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n",
        "\n",
        "If we now define the inputs to be the augmented vector $\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}$, we can write the new linear regression model as\n",
        "$$\n",
        "y = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\beta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\beta_{\\text{aug}} = \\begin{bmatrix}\n",
        "\\beta_0\\\\\n",
        "\\boldsymbol\\beta_1\n",
        "\\end{bmatrix}\\,.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ztZ-WZovcKh"
      },
      "source": [
        "N, D = X.shape\n",
        "X_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\n",
        "beta_aug = np.zeros((D+1, 1)) # new beta vector of size (D+1) x 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be_eR51IxS3X"
      },
      "source": [
        "Let us now compute the maximum likelihood estimator for this setting.\n",
        "\n",
        "_Hint:_ If possible, re-use code that you have already written."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFvLRt5KxQGz"
      },
      "source": [
        "## EDIT THIS FUNCTION\n",
        "def max_lik_estimate_aug(X_aug, y):\n",
        "\n",
        "    beta_aug_ml = np.zeros((D+1,1)) ## <-- EDIT THIS LINE\n",
        "\n",
        "    return beta_aug_ml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVSuOa27xQDz"
      },
      "source": [
        "beta_aug_ml = max_lik_estimate_aug(X_aug, ynew)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxggOnL7xcSP"
      },
      "source": [
        "Now, we can make predictions again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvjd3MQRxQA8"
      },
      "source": [
        "# define a test set (we also need to augment the test inputs with ones)\n",
        "X_test_aug = np.hstack([np.ones((X_test.shape[0],1)), X_test]) # 100 x (D + 1) vector of test inputs\n",
        "\n",
        "# predict the function values at the test points using the maximum likelihood estimator\n",
        "ml_prediction = predict_with_estimate(X_test_aug, beta_aug_ml)\n",
        "\n",
        "# plot\n",
        "plt.figure()\n",
        "plt.plot(X, ynew, '+', markersize=10, label='Data')\n",
        "plt.plot(X_test, ml_prediction, label='Linear Regression Fit')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.axvline(0, color='black')\n",
        "plt.axhline(0, color='black')\n",
        "plt.ylim(-5, 5)\n",
        "plt.xlim(-5,5)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pTpinmQxi9g"
      },
      "source": [
        "It seems this has solved our problem!\n",
        "#### Question:\n",
        "1. Play around with the first parameter of $\\boldsymbol\\beta_{\\text{aug}}$ and see how the fit of the function changes.\n",
        "2. Play around with the second parameter of $\\boldsymbol\\beta_{\\text{aug}}$ and see how the fit of the function changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K-gWeLFP15K"
      },
      "source": [
        "<a name=\"section-3\"></a>\n",
        "\n",
        "## Section 3: Ridge regression\n",
        "\n",
        "From our lectures, we know that ridge regression is an extension of linear regression with least squares loss function, including a (usually small) positive penalty term $\\lambda$:\n",
        "$$\n",
        "\\underset{\\boldsymbol\\beta}{\\text{min}} \\| \\boldsymbol y - \\boldsymbol X \\boldsymbol\\beta \\|^2 + \\lambda \\| \\boldsymbol\\beta \\|^2 = \\underset{\\boldsymbol\\beta}{\\text{min}} \\ \\text{L}_{\\text{ridge}} (\\boldsymbol\\beta)\n",
        "$$\n",
        "where $\\text{L}_{\\text{ridge}}$ is the ridge loss function. The solution is\n",
        "$$\n",
        "\\boldsymbol\\beta^{*}_{\\text{ridge}} = (\\boldsymbol X^T\\boldsymbol X + \\lambda I)^{-1}\\boldsymbol X^T\\boldsymbol y \\, .\n",
        "$$\n",
        "\n",
        "\n",
        "This time, we will define a very small training set of only two observations to demonstrate the advantages of ridge regression over least squares linear regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDZPkTKK_Ylu"
      },
      "source": [
        "X_train = np.array([0.5, 1]).reshape(-1,1)\n",
        "y_train = np.array([0.5, 1])\n",
        "X_test = np.array([0, 2]).reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RvrNp3oP9K5"
      },
      "source": [
        "Let's define function similar to the one for least squares, but taking one additional argument, our penalty term $\\lambda$.\n",
        "\n",
        "_Hint_: we apply the same augmentation as above with least squares, so the offset is accurately captured."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhKvkUNeP6vz"
      },
      "source": [
        "## EDIT THIS FUNCTION\n",
        "def ridge_estimate(X, y, penalty):\n",
        "\n",
        "    # X: N x D matrix of training inputs\n",
        "    # y: N x 1 vector of training targets/observations\n",
        "    # returns: maximum likelihood parameters (D x 1)\n",
        "\n",
        "    N, D = X.shape\n",
        "    X_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\n",
        "    N_aug, D_aug = X_aug.shape\n",
        "    I = np.identity(D_aug)\n",
        "    I[0, 0] = 0\n",
        "    beta_ridge = np.zeros((D+1,1)) ## <-- EDIT THIS LINE\n",
        "    return beta_ridge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPBRj7opQXGk"
      },
      "source": [
        "Now, we add a bit of Gaussian noise to our training set and apply ridge regression. We should do it a couple of times to be sure about the results (here 10 times)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu0nBqDGQThT"
      },
      "source": [
        "penalty_term = 0.1\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "X_test_aug = np.hstack([np.ones((X_test.shape[0],1)), X_test])\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    this_X = 0.1 * np.random.normal(size=(2, 1)) + X_train\n",
        "\n",
        "    beta_ridge = ridge_estimate(this_X, y_train, penalty=penalty_term)\n",
        "    ridge_prediction = predict_with_estimate(X_test_aug, beta_ridge)\n",
        "\n",
        "    ax.plot(X_test, ridge_prediction, color='gray',\n",
        "            label='Ridge Regression Fit (on Noisy Data)' if i == 0 else '')\n",
        "    ax.scatter(this_X, y_train, c='gray', marker='+', zorder=10,\n",
        "               label='Noisy Data' if i == 0 else '')\n",
        "\n",
        "beta_ridge = ridge_estimate(X_train, y_train, penalty=penalty_term)\n",
        "ridge_prediction_X = predict_with_estimate(X_test_aug, beta_ridge)\n",
        "\n",
        "ax.plot(X_test, ridge_prediction_X, linewidth=2, color='blue', label='Ridge Regression Fit')\n",
        "ax.scatter(X_train, y_train, c='red', marker='+', zorder=10, label='Data')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.ylim(0, 2)\n",
        "plt.xlim(0,2)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvahgeXMQddU"
      },
      "source": [
        "Let's compare this to ordinary least squares:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8R95vf6QTeV"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "plt.xlim(0.0, 2)\n",
        "plt.ylim(0.0, 2)\n",
        "\n",
        "X_train_aug = np.hstack([np.ones((X_test.shape[0],1)), X_train])\n",
        "\n",
        "X_test_aug = np.hstack([np.ones((X_test.shape[0],1)), X_test])\n",
        "\n",
        "for i in range(10):\n",
        "    this_X = 0.1 * np.random.normal(size=(2, 1)) + X_train\n",
        "    N, D = this_X.shape\n",
        "    this_X_aug = np.hstack([np.ones((N,1)), this_X])\n",
        "\n",
        "    beta_aug_ml = max_lik_estimate_aug(this_X_aug, y_train)\n",
        "    ml_prediction = predict_with_estimate(X_test_aug, beta_aug_ml)\n",
        "\n",
        "\n",
        "    ax.plot(X_test, ml_prediction, color='gray',\n",
        "            label='Unregularised Linear Regression Fit (on Noisy Data)' if i == 0 else '')\n",
        "    ax.scatter(this_X, y_train, c='gray', marker='+', zorder=10,\n",
        "               label = 'Noisy Data' if i == 0 else '')\n",
        "\n",
        "beta_aug_ml = max_lik_estimate_aug(X_train_aug, y_train)\n",
        "ml_prediction_X = predict_with_estimate(X_test_aug, beta_aug_ml)\n",
        "\n",
        "ax.plot(X_test, ml_prediction_X, linewidth=2, color='blue',\n",
        "        label='Unregularised Linear Regression Fit')\n",
        "ax.scatter(X_train, y_train, c='red', marker='+', zorder=10,\n",
        "           label='Data')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.ylim(0, 2)\n",
        "plt.xlim(0,2)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yOoy_6RQiQ-"
      },
      "source": [
        "#### Questions\n",
        "1. What differences between the two solutions above can you see?\n",
        "2. **Optional**:\n",
        "    - play around with different values of the penalty term $\\lambda$. How do the corresponding functions change? Which values provide the most reasonable results?\n",
        "    - Can you replicate your results using [`sklearn.linear_model.Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)?\n",
        "    - Based on sklearn's documentation, can you see any differences in the algorithms that are implemented in sklearn?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-4\"></a>\n",
        "\n",
        "## Section 4: LASSO regression\n",
        "\n",
        "As opposed to the ridge regression which has a penalty term  $\\| \\boldsymbol\\beta \\|^2$, LASSO regression introduces $ \\| \\boldsymbol\\beta \\|_1 $, (also known as $L_1$ loss). $L_1$ loss is often preferred if we are interested in sparse parameters, i.e. few non-zero paramters. This is generally regarded as a feature selection task, and in high-dimensional problems it helps interpret the learned parameters and their relevance.\n",
        "However, no closed-form solution exists for LASSO regression as in the standard and ridge regression, so we can use the iterative gradient-descent algorithm.\n",
        "\n",
        "\n",
        "In LASSO regression the aim is to minimize the following loss:\n",
        "\n",
        "$L_\\text{LASSO}(\\boldsymbol{\\beta}) = \\frac{1}{2N}|| \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta}||^2 + \\lambda ||\\boldsymbol{\\beta}||_1$\n",
        "\n",
        "Where $||\\boldsymbol{\\beta}||_1 = \\sum_{i=1}^p |\\beta_i|$\n",
        "\n",
        "\n",
        "The absolute function $|.|$ adds nonsmoothness to the loss function, which can prevent the gradient-descent to converge properly to the optimal solution, and will keep bouncing around it instead. To solve this, we can use the Huber loss as an alternative to the absolute function. It combines the behaviour of $L_1$ loss except around the zero.\n",
        "\n",
        "A relaxed optimization can be made by replacing $||\\boldsymbol{\\beta}||_1$ with the Huber Loss $\\sum_{i=1}^p L_c(\\beta_i)$, where  $L_c(\\beta)$ is defined as:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$L_c (\\beta) =\n",
        "  \\begin{cases}\n",
        " \\frac{1}{2}{\\beta^2}                   & \\text{for } |\\beta| \\le c, \\\\\n",
        " c (|\\beta| - \\frac{1}{2}c), & \\text{otherwise.}\n",
        "\\end{cases}\n",
        "$\n",
        "\n",
        "The $c$ parameter in Huber determines the range around zero with $L_2$-like behaviour to ensure smoothness and, hence, better convergence.\n",
        "\n",
        "\n",
        "The piecewise function smooth $L_c (\\beta)$ has the gradient:\n",
        "\n",
        "\n",
        "\n",
        "$\\frac{dL_c (\\beta)}{d\\beta} =\n",
        "  \\begin{cases}\n",
        " \\beta                   & \\text{for } |\\beta| \\le c, \\\\\n",
        " c\\, \\text{sgn}(\\beta) , & \\text{otherwise.}\n",
        "\\end{cases}\n",
        "$\n",
        "\n",
        "\n",
        "Now we can minimize the following relaxed function by gradient descent:\n",
        "\n",
        "$\n",
        "\\begin{align} L_\\text{LASSO-Huber}(\\boldsymbol{\\beta})\n",
        "&= \\frac{1}{2N}|| \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta}||^2 + \\lambda \\sum_i^p L_c(\\beta) \\\\\n",
        "&= \\frac{1}{2N}(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta})^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta})  + \\lambda \\sum_i^p L_c(\\beta) \\\\\n",
        "&= \\frac{1}{2N}\\left(\\boldsymbol{y}^T\\boldsymbol{y} - \\boldsymbol{y}^T\\boldsymbol{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\boldsymbol{X}^T  \\boldsymbol{y} + \\boldsymbol{\\beta}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\beta}\\right) + \\lambda \\sum_i^p L_c(\\beta)\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "Which has the gradient\n",
        "\n",
        "$\n",
        "\\begin{align} \\nabla_\\boldsymbol{\\beta} L_\\text{LASSO-Huber}\n",
        "&= \\frac{1}{N}\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\beta} - \\boldsymbol{X}^T\\boldsymbol{y}\\right) + \\lambda \\nabla_{\\boldsymbol{\\beta}}L_c(\\boldsymbol{\\beta})\n",
        "\\end{align}$\n",
        "\n",
        "Optimization method:\n",
        "- **Initialize** $\\boldsymbol{\\beta}$ with zeros.\n",
        "- Use Gradient-descent for tuning $\\boldsymbol{\\beta}$\n"
      ],
      "metadata": {
        "id": "TPMo_S-v9d8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementated in Python as:"
      ],
      "metadata": {
        "id": "aNv5eHEoFk4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "rFzs4YxZfRTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def huber(beta, c = 1e-6):\n",
        "    return np.where(np.abs(beta) < c,\n",
        "                    (beta**2)/2.,\n",
        "                    c*(np.abs(beta) - c/2))\n",
        "\n",
        "def grad_huber(beta, c = 1e-6):\n",
        "    g = np.empty_like(beta)\n",
        "    return  np.where(np.abs(beta) < c,\n",
        "                    beta,\n",
        "                    c * np.sign(beta))"
      ],
      "metadata": {
        "id": "88D42ZKp-jre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.linspace(-1, 1, 1000)\n",
        "plt.plot(a, huber(a, c=0.1), label='$L_{LASSO-Huber}(x)$')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.ylim(0, 0.1)\n",
        "plt.xlim(-1, 1)\n",
        "plt.axvline(0, color='black')\n",
        "plt.axhline(0, color='black')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mFqwFuVMFttr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.linspace(-1, 1, 1000)\n",
        "plt.plot(a, grad_huber(a, c=0.1), label=r'$ \\nabla L_{LASSO-Huber}(x)$')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.ylim(-0.15, 0.15)\n",
        "plt.xlim(-1, 1)\n",
        "plt.axvline(0, color='black')\n",
        "plt.axhline(0, color='black')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xjkfe7FUGnPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try different $c$ values and observe the difference**"
      ],
      "metadata": {
        "id": "Xo8FNl7rGe5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimization with gradient-descent\n",
        "\n",
        "We next implement gradient-descent to solve the optimisation for the LASSO model."
      ],
      "metadata": {
        "id": "y5oTp7s7HGvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minimize_ls_huber(X, y, lambd, n_iters = 10000, step_size=5e-5, c_huber=1e-4):\n",
        "    \"\"\"\n",
        "    This function estimates the regression parameters with the relaxed version\n",
        "    of LASSO regression.\n",
        "    Args:\n",
        "    X (np.array): The augmented data matrix with shape (N, p + 1).\n",
        "    y (np.array): The response column with shape (N, 1).\n",
        "    lambd (float): The multiplier of the relaxed L1 term.\n",
        "    n_iters (int): Number of gradient descent iterations.\n",
        "    step_size (float): The step size in the updating step.\n",
        "    \"\"\"\n",
        "    n, p = X.shape\n",
        "    # Precomputed products to avoid redundant computations.\n",
        "    XX = X.T @ X / n\n",
        "    Xy = X.T @ y / n\n",
        "    # Initialize beta params with zeros\n",
        "    beta = np.zeros(p)\n",
        "\n",
        "    for i in range(n_iters):\n",
        "        # Compute the gradient of the relaxed LASSO, Huber.\n",
        "        grad_c = grad_huber(beta, c=c_huber)\n",
        "\n",
        "        # Should Bias term is not involved in the regularisation.\n",
        "        grad_c[-1] = 0\n",
        "\n",
        "        # Compute the gradient of the regularised loss.\n",
        "        grad = None # <-- EDIT THIS LINE\n",
        "\n",
        "\n",
        "        # Update beta\n",
        "        beta = beta - 0 # <-- EDIT THIS LINE\n",
        "\n",
        "    return beta"
      ],
      "metadata": {
        "id": "J34BcLIkF6TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To study the feature selection capability of LASSO, we generate a 3 dimensional synthetic data set $X$ where the second dimension does not contribute significantly to the target $y$."
      ],
      "metadata": {
        "id": "_xd-l1fgA6Ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Generate random data with three features\n",
        "X1 = np.random.rand(100)\n",
        "X2 = np.random.rand(100)  # Insignificant feature (with small theta below)\n",
        "X3 = np.random.rand(100)\n",
        "true_theta = np.array([8, 0.5, 9])  # Weights for features\n",
        "y = X1 * true_theta[0] + X2 * true_theta[1] + X3 * true_theta[2] + np.random.randn(100)"
      ],
      "metadata": {
        "id": "C35ZSCXDKMLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compare the ground truth coefficients used to create the synthetic data set to the optimal LASSO coefficients."
      ],
      "metadata": {
        "id": "1K_9sYRnBXqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add bias term\n",
        "X_aug = np.stack((X1, X2, X3, np.ones(100)), axis=1)\n",
        "\n",
        "# Run LASSO regression\n",
        "theta_lasso = minimize_ls_huber(X_aug, y, 5e3, n_iters=15000,\n",
        "                                step_size=1e-3,\n",
        "                                c_huber=1e-5)\n",
        "\n",
        "# Print the result\n",
        "print(\"LASSO Regression Coefficients:\")\n",
        "print(\"Theta (Intercept):\", theta_lasso[3])\n",
        "print(\"Theta 1:\", theta_lasso[0])\n",
        "print(\"Theta 2:\", theta_lasso[1])\n",
        "print(\"Theta 3:\", theta_lasso[2])"
      ],
      "metadata": {
        "id": "76hFOV9yKOSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for theta, label in ((true_theta, 'True'), (theta_lasso[:-1], 'LASSO')):\n",
        "    plt.bar([1,2,3], theta, color='blue' if label == 'True' else 'green')\n",
        "    plt.xlabel('Feature Index')\n",
        "    plt.xticks([1,2,3])\n",
        "    plt.ylabel(f'{label} Coefficients')\n",
        "    plt.title(f'{label} Coefficients of Features')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kGCySex0g05L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions\n",
        "\n",
        "1. Try adding more **insignicant** variables and repeat the experiments. Do you still get the expected sparse solution? If not, what hyperparameters might need a re-tune?\n",
        "\n",
        "2. Can you observe a clear pattern in how coefficients change as the penalty term $\\lambda$ varies? Can you create a plot that shows the trajectory of each coefficient as $\\lambda$ changes?\n",
        "\n",
        "3. **Optional**:\n",
        "    - Can you replicate your results using [`sklearn.linear_model.Lasso`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)?\n",
        "    - Based on sklearn's documentation, can you see any differences in the algorithms that are implemented in sklearn?\n",
        "\n"
      ],
      "metadata": {
        "id": "ZDkTcY7xR2zC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fVES-I37f8yS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}