{"cells":[{"cell_type":"markdown","metadata":{"id":"5o4LqdKHIrIG"},"source":["#### Prerequisites\n","\n","- Basic familiarity with [Numpy](https://numpy.org/doc/stable/user/quickstart.html)\n","- Basic familiarity with [Pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html)"]},{"cell_type":"markdown","metadata":{"id":"DTqKQ9khIvvz"},"source":["<a name=\"outline\"></a>\n","\n","## Outline\n","\n","- [Section 1](#section-1): Intro to kNN\n","- [Section 2](#section-2): Classification with kNN\n","- [Section 3](#section-3): Hyperparameter Tuning for $k$ with _T-fold_ Cross Validation\n","- [Section 4](#section-4): Regression with kNN\n"]},{"cell_type":"markdown","metadata":{"id":"aQYOryXBIuJG"},"source":["# _k_ nearest neighbours (kNN)\n","\n","The purpose of this notebook is to understand and implement the kNN algorithm. You are not allowed to use any package that has a complete kNN framework implemented (e.g., scikit-learn).\n","\n","<a name=\"section-1\"></a>\n","\n","## Section 1: Intro to kNN [^](#outline)\n","\n","The kNN algorithm can be used both for classification and  regression. Broadly speaking, it start with calculating the distance of a given point $x$ to all other points in the data set. Then, it finds the _k_ nearest points closest to $x$, and assigns the new point $x$ to the majority class of the _k_ nearest points _(classification)_. So, for example, if two of the _k_=3 closest points to $x$ were red while one is blue, $x$ would be classified as red.\n","\n","On the other hand in _regression_, we see the labels as continuous variables and assign the label of a data point $x$ as the mean of the labels of its _k_ nearest neighbours. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nGLDpvh2JLD5"},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import make_classification, fetch_california_housing\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap"]},{"cell_type":"markdown","metadata":{"id":"g3NNOP8ckB6s"},"source":["Important things first: You already know that the kNN algorithm is based on computing distances between data points. So, let's start with defining a function that computes such a distance. For simplicity, we will only work with **Euclidean distances** in this notebook, but other distances can be chosen interchangably, of course.\n","\n","Implement in the following cell the Euclidean distance $d$, defined as\n","$$\n","d(\\boldsymbol p, \\boldsymbol q) = \\sqrt{\\sum_{i=1}^D{(q_i-p_i)^2}} \\, ,\n","$$\n","where $\\boldsymbol p$ and $\\boldsymbol q$ are the two points in our $D$-dimensional Euclidean space.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yjGhYvoVkBU8"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def euclidian_distance(p, q):\n","    return ... ## <-- EDIT THIS LINE"]},{"cell_type":"markdown","metadata":{"id":"eQ_TLVhzlNph"},"source":["<a name=\"section-2\"></a>\n","\n","## Section 2: Classification with kNN [^](#outline)\n","\n","We start with using kNN for classification tasks, and create a dataset with sklearn's [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function and standardise the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vg3-00Wkn77v"},"outputs":[],"source":["X_class, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=3, random_state=15)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pefXybUUpVzM"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def standardise(X):\n","  mu = np.mean(X, 0)\n","  sigma = np.std(X, 0)\n","  X_std = ... ## <-- EDIT THIS LINE\n","  return X_std"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VotULOwmpjxn"},"outputs":[],"source":["X = standardise(X_class)"]},{"cell_type":"markdown","metadata":{"id":"TL7c00_JpDtx"},"source":["As with any other supervised machine learning method, we create a training and test set to learn and evaluate our model, respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BL-xX8gboj8z"},"outputs":[],"source":["# shuffling the rows in X and y\n","p = np.random.permutation(len(y))\n","X = X[p]\n","y = y[p]\n","\n","# we split train to test as 70:30\n","split_rate = 0.7\n","X_train, X_test = np.split(X, [int(split_rate*(X.shape[0]))])\n","y_train, y_test = np.split(y, [int(split_rate*(y.shape[0]))])"]},{"cell_type":"markdown","metadata":{"id":"Me2LXr200JjL"},"source":["We visualise the data set with points in the training set being fully coloured and points in the test being half-transparent."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2hM5H8Ui0Zqe"},"outputs":[],"source":["# define colormaps\n","cm = plt.cm.RdBu\n","cm_bright = ListedColormap(['blue', 'red', 'green'])\n","\n","# visual exploration\n","plt.figure(figsize=(12,8))\n","plt.xlabel(r'$X^{(1)}$', size=24)\n","plt.ylabel(r'$X^{(2)}$', size=24)\n","plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n","plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.25)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"C0GCsan8q2Uv"},"source":["We try to find the _k_ nearest neighbours in our training set for every test data point. The majority of labels of the _k_ closest training points determines the label of the test point. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0waZKvI9oj5s"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def k_neighbours(X_train, X_test, k=5, return_distance=False):\n","  n_neighbours = k\n","  dist = []\n","  neigh_ind = []\n","  \n","  # compute distance from each point x_text in X_test to all points in X_train (hint: use python's list comprehension: [... for x_test in X_test])\n","  point_dist = ... ## <-- EDIT THIS LINE\n","\n","  # determine which k training points are closest to each test point\n","  for row in point_dist:\n","      enum_neigh = enumerate(row)\n","      sorted_neigh = sorted(enum_neigh, key=lambda x: x[1])[:k]\n","\n","      ind_list = [tup[0] for tup in sorted_neigh]\n","      dist_list = [tup[1] for tup in sorted_neigh]\n","\n","      dist.append(dist_list)\n","      neigh_ind.append(ind_list)\n","  \n","  # return distances together with indices of k nearest neighbouts\n","  if return_distance:\n","      return np.array(dist), np.array(neigh_ind)\n","  \n","  return np.array(neigh_ind)"]},{"cell_type":"markdown","metadata":{"id":"c23MEsfNt3KR"},"source":["Once we know which _k_ neighbours are closest to our test points, we can predict the labels of these test points.\n","\n","We implement this in a \"pythonic\" way and call the previous function `k_neighbours` _within_ the next function `predict`.\n","\n","Our `predict` function determines how any point $x_\\text{test}$ in the test set is classified. Here, we only consider the case where each of the *k* neighbours contributes equally to the classification of $x_\\text{test}$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Ll7dfHtoj2S"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def predict(X_train, y_train, X_test, k=5):\n","  # each of the k neighbours contributes equally to the classification of any data point in X_test  \n","  neighbours = k_neighbours(X_test, k=k)\n","  # count number of occurences of label with np.bincount and choose the label that has most with np.argmax (hint: use python's list comprehension: np.array([... for neighbour in neighbours]))\n","  y_pred = ... ## <-- EDIT THIS LINE\n","\n","  return y_pred"]},{"cell_type":"markdown","metadata":{"id":"_5tbmvo7x2QZ"},"source":["To evaluate the algorithm in a more principled way, we need to implement a function that computes the mean accuracy by counting how many of the test points have been classified correctly and dividing this number by the total number of data points in our test set.\n","\n","Again, we do this is in a pythonic way and call the previous `predict` function _within_ the next function `score`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trOotUZWj-a0"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def score(X_train, y_train, X_test, y_test, k=5):\n","  y_pred = ... ## <-- EDIT THIS LINE\n","  return np.mean(y_pred==y_test)"]},{"cell_type":"markdown","metadata":{"id":"77rOIU0V395Q"},"source":["It is quite common to print both the training and test set accuracies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GAPceMtej-Yo"},"outputs":[],"source":["k = 8\n","print('Training set mean accuracy:', score(X_train, y_train, X_train, y_train, k=k))\n","print('Test set mean accuracy:', score(X_train, y_train, X_test, y_test, k=k))"]},{"cell_type":"markdown","metadata":{"id":"2j8ryojdZ07O"},"source":["#### Questions\n","1. Does the solution above look reasonable?\n","2. Play around with different values for _k_. How does it influence the classification mean accuracy?\n","3. Compare the training and test set accuracy. Is there a difference? If so, what does the difference tell you?\n","4. Choose different ratios for the split between training and test set, and re-run the entire algorithm. What can you learn from different ratios?\n","5. Considering an accuracy estimate on a test-split wich contains 30% of the dataset examples is 0.86, do we guarantee to obtain the same accuracy estimate when we apply our model on infinetely large unseen test examples, i.e. does the accuracy of your model on the test-split generalize well on unseen data? From this week's lecture notes, what would you suggest to improve our confidence in the accuracy estimate, so it is more closer estimate to the true accuracy when testing on unseen examples? \n","\n","<a name=\"section-3\"></a>\n","\n","### 3 Hyperparameter Tuning for $k$ with _T-fold_ Cross [^](#outline)\n","\n","Let's consider a systematic way to help select the best $k$ (i.e. the hyperparameter of $k$-NN). In previous cells, we splitted our data into 70%:30% for training:test examples. Now we need to choose the best $k$, but without looking at the accuracy on the test set (which should be used at the end to assess the predictive power of the model with the chosen $k$). To this end, we perform $T$-fold cross validation, where, **importantly**, we don't evaluate the accuracy of the model using the same examples on which we trained our model, rather on a held out validation set. We can achieve this by running $T$ experiments, and in each one we use disjoint partitions for the training and accuracy examples. By averaging the accuracy estimates over the $T$ experiments, we get a more precise and reliable accuracy estimate than before. If we consider $T=3$ folds, then we can run the three experiments and evaluate the average accuracy as in the figure below:\n","\n","![cv](https://drive.google.com/uc?export=view&id=1aTZu9xNdgPb7_nBzs1cHrEpRjF31vK2M)\n","\n","\n","Finaly, we need to isolate a separate set, before using cross-validation for hyperparameter tuning, to test our model after selecting the best performing $k$ for $k$-NN, so we further consider the following partitioning, which we will need to adhere with throughout the future notebooks and courseworks (i.e. in the problems when we need to use cross-validation for hyperparameter tuning then test on completely unseen data that are not involved in hyperparamter tuning):\n","\n","![cv2](https://drive.google.com/uc?export=view&id=1d2fIBBhT0lzcUCYSfKcvCEaJT2df9-9K)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8biZvqBaZ8D6"},"source":["Let's consider a ratio 80:20 for training and test splits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jE2unDhHZ49f"},"outputs":[],"source":["# shuffling the rows in X and y\n","p = np.random.permutation(len(y))\n","X = X[p]\n","y = y[p]\n","\n","\n","# we split train to test as 80:20\n","split_rate = 0.8\n","X_train, X_test = np.split(X, [int(split_rate*(X.shape[0]))])\n","y_train, y_test = np.split(y, [int(split_rate*(y.shape[0]))])"]},{"cell_type":"markdown","metadata":{"id":"_6fCbHrHaBGQ"},"source":["Now let's partition our training split into 5-folds. We could store the corresponding indices only:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8l7akHSbaBti"},"outputs":[],"source":["# Now we have a list of five index arrays, each correspond to one of the five folds.\n","folds_indexes = np.split(np.arange(len(y_train)), 5)\n","folds_indexes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KgZG44BlaF6E"},"outputs":[],"source":["# EDIT THIS FUNCTION\n","def cross_validation_score(X_train, y_train, folds, k):\n","  scores = []\n","  for i in range(len(folds)):\n","    val_indexes = folds[i]\n","    train_indexes = list(set(range(y_train.shape[0])) - set(val_indexes))\n","    \n","    X_train_i = X_train[train_indexes, :]\n","    y_train_i = y_train[train_indexes]\n","\n","\n","    X_val_i = X_train[42, :] # <- EDIT THIS LINE\n","    y_val_i = y_train[13] # <- EDIT THIS LINE\n","\n","    score_i = score(edit, this, function, arguments, k=k) # <- EDIT THIS LINE\n","    scores.append(score_i)\n","\n","  # Return the average score\n","  return max(scores) # <- EDIT THIS LINE"]},{"cell_type":"markdown","metadata":{"id":"j5LrxV6CbEe7"},"source":["Let's scan a range of $k$ in $[1, 30]$ and select the one with the best cross-validation accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ent4--QDbIIY"},"outputs":[],"source":["def choose_best_k(X_train, y_train, folds, k_range):\n","  k_scores = np.zeros((len(k_range),))\n","  \n","  for i, k in enumerate(k_range):\n","    k_scores[i] = cross_validation_score(X_train, y_train, folds, k)\n","    print(f'CV_ACC@k={k}: {k_scores[i]:.3f}')\n","\n","  best_k_index = np.argmax(k_scores)\n","  return k_range[best_k_index]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUdt3WZWbMKi"},"outputs":[],"source":["best_k = choose_best_k(X_train, y_train, folds_indexes, np.arange(1, 31))\n","\n","print('best_k:', best_k)"]},{"cell_type":"markdown","metadata":{"id":"e2ZMAV0l4Pup"},"source":["\n","<a name=\"section-4\"></a>\n","\n","## Section 4:  Regression with kNN [^](#outline)\n","\n","The kNN algorithm is mostly used for classification, but we can also utilise it for (non-linear) regression. Here, we calculate the label of every point in the test set as the mean of the _k_ nearest neighbours.\n","\n","We start with defining a training set with sklearn's Californa housing data set. Note that this data set has normally 8 features, but we only extract the first feature, which corresponds to the median income in the district. The label is the median house value in the district.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sLrzaHvj-Vw"},"outputs":[],"source":["data = fetch_california_housing(return_X_y=True)\n","X = data[0][:,0].reshape((-1, 1))\n","y = data[1]\n","\n","X_std = standardise(X)"]},{"cell_type":"markdown","metadata":{"id":"HZSFFaom85XT"},"source":["As before, we first divide the data into training and test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOUrZpEpj-T4"},"outputs":[],"source":["# shuffling the rows in X and y\n","p = np.random.permutation(len(y))\n","X = X_std[p]\n","y = y[p]\n","\n","# we split train to test as 70:30\n","split_rate = 0.7\n","X_train, X_test = np.split(X, [int(split_rate*(X.shape[0]))])\n","y_train, y_test = np.split(y, [int(split_rate*(y.shape[0]))])"]},{"cell_type":"markdown","metadata":{"id":"lyaH2SKx-qvz"},"source":["Let's plot it to get a sense how we can proceed. This time, we plot training examples in blue and test examples in red."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzmty5H5-qVe"},"outputs":[],"source":["# visual exploration\n","plt.figure(figsize=(12,8))\n","plt.xlabel(r\"median income in '000 USD\", size=20)\n","plt.ylabel(r\"median house value in '00.000 USD\", size=20)\n","plt.scatter(X_train, y_train, c='blue', alpha=0.25)\n","plt.scatter(X_test, y_test, c='red', alpha=0.25)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"D_NiqpEXBkF9"},"source":["As before, we need to define a predicting function which we call `reg_predict`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WrjhUkFj-Oj"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def reg_predict(X_train, y_train, X_test, k=20):\n","  # each of the k neighbours contributes equally to the classification of any data point in X_test  \n","  neighbours = k_neighbours(X_train, X_test, k=k)\n","  # compute mean over neighbours labels (hint: use python's list comprehension: np.array([... for neighbour in neighbours]))\n","  y_pred = ... ## <-- EDIT THIS LINE\n","\n","  return y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mwVChY_aETF6"},"outputs":[],"source":["# computing predictions... (takes a few minutes due to the high sample size)\n","k = 20\n","y_pred = reg_predict(X_train, y_train, X_test, k=k)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gq6CgjUrETDU"},"outputs":[],"source":["# ... and plotting them\n","plt.figure(figsize=(12,8))\n","plt.xlabel(r\"median income (standardised)\", size=20)\n","plt.ylabel(r\"median house value in '00.000 USD\", size=20)\n","#plt.scatter(X_train, y_train, c='blue', alpha=0.25)\n","plt.scatter(X_test, y_test, c='red', alpha=0.25)\n","plt.scatter(X_test, y_pred, c='yellow', alpha=0.25)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"mbBUhK8NRjqG"},"source":["To determine how well the prediction was, let us determine the $R^2$ score. The labels of the test set will be called $y$ and the predictions on the test data $\\hat{y}$.\n","$$\n","R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\, ,\n","$$\n","where $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbhMVuCGRic0"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def r2_score(y_test, y_pred):\n","  numerator = ... ## <-- EDIT THIS LINE\n","  y_avg = ... ## <-- EDIT THIS LINE\n","  denominator = ... ## <-- EDIT THIS LINE\n","  return 1 - numerator/denominator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"faowTZRXTgG7"},"outputs":[],"source":["print(r'R2 score:', r2_score(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"EMFsnTNxQpGc"},"source":["#### Questions\n","1. Does the solution above look reasonable? What does your $R^2$ value tell you?\n","2. Play around with different values for _k_. How does it influence the regression?\n","3. Like we did in classification, excercise with implementing cross-validation to find the best performing k on the regression task.\n","4. Compare the training and test set accuracy. Is there a difference? If so, what does the difference tell you?\n","5. Choose different ratios for the split between training and test set, and re-run the entire algorithm. What can you learn from different ratios?\n","6. Can you replicate your results using [sklearn](https://scikit-learn.org/stable/modules/neighbors.html)?\n","7. Based on sklearn's documentation, can you see any differences in the algorithms that are implemented in sklearn?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"icDWYVX1JkFe"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":0}