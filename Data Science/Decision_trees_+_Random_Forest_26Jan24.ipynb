{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Prerequisites\n",
        "\n",
        "- Basic familiarity with [Numpy](https://numpy.org/doc/stable/user/quickstart.html)\n",
        "- Basic familiarity with [Pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html)\n",
        "- Basic familiarity with [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)\n",
        "\n",
        "\n",
        "## Outline\n",
        "\n",
        "<a name=\"outline\"></a>\n",
        "\n",
        "- [Section 1](#section-1): Decision Tree Classifier\n",
        "  - [(1.a)](#section-1a) Dataset Preparation\n",
        "  - [(1.b)](#section-1b) Intro to Decision Trees and GINI-index\n",
        "  - [(1.c)](#section-1c) Decision Tree Training\n",
        "  - [(1.d)](#section-1d) Decision Tree Classfication Algorithm\n",
        "- [Section 2](#section-2): From Decision Tree to Random Forest Algorithm\n",
        "  - [(2.a)](#section-2a) Intro to Random Forest\n",
        "  - [(2.b)](#section-2b) Training: Feature Bagging\n",
        "  - [(2.c)](#section-2c) Training: Bootstrapping\n",
        "  - [(2.d)](#section-2d) Classification: Aggregation\n"
      ],
      "metadata": {
        "id": "VelgvQCYQjVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixes\n",
        "\n",
        "- 10:00 AM, 26 Jan 2024:\n",
        "  - Docstring of `split_samples` function. It returns two tuples corresponding to the left and right splits.\n",
        "\n"
      ],
      "metadata": {
        "id": "KXsJCrnOL-U4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcr6esW4Fv6_"
      },
      "source": [
        "<a name=\"section-1\"></a>\n",
        "\n",
        "# Section 1: Decision Tree Classifier\n",
        "Decision trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of the label of unseen data by learning simple decision rules inferred from the data features.\n",
        "\n",
        "You can understand it as using a set of _if-then-else_ decision rules, e.g., _if_ it snowed in London, _then_ many Londoners would ski on Primrose Hill. _Else_, they would walk in Hyde Park.\n",
        "Generally speaking, the deeper the tree, i.e., the more _if-then-else_ decisions are subsequently made in our model, the more complex the decision rules and the fitter the model. However, note that decision trees are prone to overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initial global configuration for matplotlib\n",
        "SMALL_SIZE = 12\n",
        "MEDIUM_SIZE = 16\n",
        "BIGGER_SIZE = 20\n",
        "\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
      ],
      "metadata": {
        "id": "oZznZxN9qP0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<a name=\"section-1a\"></a>\n",
        "\n",
        "## (1a) Dataset Preparation [^](#outline)\n",
        "\n",
        "In this notebook, we will use decision trees as a classification algorithm with the GINI-index, and work with the famous [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set). It contains four biological characteristics _(features)_ of 150 samples that belong to three species _(classes)_ of the family of Iris flowers (Iris setosa, Iris virginica and Iris versicolor). The data set provides 50 samples for each species."
      ],
      "metadata": {
        "id": "oa5KlLwlCQUE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxlpS7eZFpdk"
      },
      "source": [
        "# import packages\n",
        "from collections import defaultdict\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIZVtIbyhNEe"
      },
      "source": [
        "# load data\n",
        "data = load_iris()\n",
        "# print data to see how it is structured\n",
        "#print(data)\n",
        "X, y, column_names = data['data'], data['target'], data['feature_names']\n",
        "# combining all information in one data frame\n",
        "X_y = pd.DataFrame(X, columns=column_names)\n",
        "X_y['label'] = y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWCJ9D5PhRQr"
      },
      "source": [
        "# check\n",
        "X_y.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMp0Q3eEhyFi"
      },
      "source": [
        "It is always a good idea to see whether the features are correlated. The python package `seaborn` has a nice one-line command called `pairplot` to explore this visually. It directly prints the feature names (sepal length, sepal width, petal length, petal width) and labels as axis titles. The `pairplot` plot below is used to visualize the distribution across samples of each descriptor and the correlation between descriptors (this helps identify collinear features)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQsi2D6ThtKU"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.pairplot(X_y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J8wLjEaMrFhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe some pairs of features are correlated with various degrees while others are not. We specifically observe that *petal width* and *petal length* are strongly correlated, while *petal width* is correlated *sepal width* with a lesser degree. We also observe that labels are balanced in this dataset."
      ],
      "metadata": {
        "id": "0E-cfsH6rg9y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UjOk2mjjooA"
      },
      "source": [
        "As with any other supervised machine learning method, we create a train and test set to learn and evaluate our model, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTMU873Dh_G-"
      },
      "source": [
        "# stacking data X and labels y into one matrix\n",
        "X_y_shuff = X_y.iloc[np.random.permutation(len(X_y))]\n",
        "\n",
        "# we split train to test as 70:30\n",
        "split_rate = 0.7\n",
        "train, test = np.split(X_y_shuff, [int(split_rate*(X_y_shuff.shape[0]))])\n",
        "\n",
        "\n",
        "# Separate the training features into X_train by extracting all columns except the last one,\n",
        "# which represent the labels, and vice versa for y_train.\n",
        "X_train = train[train.columns[:-1]]\n",
        "y_train = train[train.columns[-1]]\n",
        "\n",
        "# Simlarly to test split.\n",
        "X_test = test[test.columns[:-1]]\n",
        "y_test = test[test.columns[-1]]\n",
        "\n",
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "# We assume all examples have the same weight.\n",
        "training_weights = np.ones_like(y_train) / len(y_train)\n",
        "\n",
        "# We need a dictionary indicating whether the column index maps to a\n",
        "# categorical feature or numerical\n",
        "# In this example, all features are numerical (categorical=False)\n",
        "columns_dict = {index: False for index in range(X_train.shape[1])}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD7QG1voKIkF"
      },
      "source": [
        "We will build up our decision tree algorithm in the pythonic way how we have done it previously as well by calling functions we define first in functions we define later. For quick evaluations of your implementations, however, we have also included one-line commands after each cell to see whether your implementation results in errors.\n",
        "\n",
        "\n",
        "<a name=\"section-1b\"></a>\n",
        "\n",
        "\n",
        "\n",
        "## (1b) Intro to Decision Trees and GINI-index [^](#outline)\n",
        "\n",
        "\n",
        "In our lectures, we have learnt that:\n",
        "\n",
        "- Decision tree algorithm is a _greedy algorithm_ that splits the data samples $\\boldsymbol y$ into _left_ $\\boldsymbol y_l$ and _right_ $\\boldsymbol y_r$ samples, and the splitting is applied recursively on each side, resulting in a binary-tree like of splittings.\n",
        "- Samples can be split given two parameters: the feature index $j$ and a value $s$.\n",
        "  - **If the feature is a continuous variable**, the $s$ is used as a threshold such that samples with feature value less than $s$ are assigned to the left, and vice versa.\n",
        "  - **if the feature is a categorical variable**, the $s$ is used as a predicate such that samples with feature value equals to $s$ are assigned to left, and vice versa.\n",
        "- To determine $j$ and $s$ at each split node, we may use _GINI-index_ to search for $j$ and $s$ that minimizes the weighted sum of _GINI-index_ of the left side samples and right side samples:\n",
        "  \n",
        "  $$GI(\\boldsymbol y; j, s) = p_l \\times GI(\\boldsymbol y_l) + p_r \\times GI(\\boldsymbol y_r)$$\n",
        "\n",
        "where $p_l$ and $p_r$ are, respectively, the cumulative weights of samples on the left and on the right, while $GI(\\boldsymbol y)$ is defined as:\n",
        "\n",
        "$$\n",
        "\\text{GI}(\\boldsymbol y) = 1 - \\sum_{i=1}^Q \\mathbb P (y = c_i)^2\n",
        "$$\n",
        "\n",
        "where $c_i$ is the i-th class out of $Q$ distinct classes, so $\\mathbb P (y = c_i)$ reads the weight of the class $i$ in the current sample $\\boldsymbol y$.\n",
        "\n",
        "\n",
        "There are other alternatives to _GINI-index_ like _Information Gain_ but we will stick to _GINI-index_ in this notebook.\n",
        "\n",
        "\n",
        "It's your turn to implement it in the next cell. We want to allow the code to consider samples with different weights, hence introduce an additional argument called `sample_weights`. The Iris data set we work with in this notebook has uniform sample weights, but other data sets you work with in the future could be different."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def gini_index(y, sample_weights):\n",
        "    \"\"\"\n",
        "    Calculate the GINI-index for labels.\n",
        "    Arguments:\n",
        "        y: vector of training labels, of shape (N,).\n",
        "        sample_weights: weights for each samples, of shape (N,).\n",
        "    Returns:\n",
        "        (float): the GINI-index for y.\n",
        "    \"\"\"\n",
        "\n",
        "    # count different labels in y，and store in label_weights\n",
        "    # initialize with zero for each distinct label.\n",
        "    label_weights = {yi: 0 for yi in set(y)}\n",
        "    for yi, wi in zip(y, sample_weights):\n",
        "        label_weights[yi] += wi\n",
        "\n",
        "    # The normalization constant\n",
        "    total_weight = 1 # <-- EDIT THIS LINE\n",
        "\n",
        "    sum_p_squared = 0\n",
        "    for label, weight in label_weights.items():\n",
        "        sum_p_squared += 0 # <-- EDIT THIS LINE\n",
        "\n",
        "    # Return GINI-Index\n",
        "    return 49.5 * sum_p_squared # <-- EDIT THIS LINE"
      ],
      "metadata": {
        "id": "vyXEN6wWRIH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OoieamNKKjJ"
      },
      "source": [
        "# evaluate labels y\n",
        "gini_index(y_train.to_numpy(), training_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByxQZ5BHOJIN"
      },
      "source": [
        "<a name=\"section-1c\"></a>\n",
        "\n",
        "\n",
        "\n",
        "## (1c) Decision Tree Training [^](#outline)\n",
        "\n",
        "\n",
        "Next, we define a function to split the data samples based on a feature (column) index and a value (recall how this value will be used whether the feature is categorical or numerical).\n",
        "\n",
        "This has not much use yet, but we will call it in later functions, e.g., in the next cell in `gini_split_value`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd8WFZLKF6iT"
      },
      "source": [
        "# EDIT THIS FUNCTION\n",
        "\n",
        "def split_samples(X, y, sample_weights, column, value, categorical):\n",
        "    \"\"\"\n",
        "    Return the split of data whose column-th feature:\n",
        "      1. equals value, in case `column` is categorical, or\n",
        "      2. less than value, in case `column` is not categorical (i.e. numerical)\n",
        "\n",
        "    Arguments:\n",
        "        X: training features, of shape (N, p).\n",
        "        y: vector of training labels, of shape (N,).\n",
        "        sample_weights: weights for each samples, of shape (N,).\n",
        "        column: the column of the feature for splitting.\n",
        "        value: splitting threshold  the samples\n",
        "        categorical: boolean value indicating whether column is a categorical variable or numerical.\n",
        "    Returns:\n",
        "        tuple(np.array, np.array, np.array): tuple of the left split data (X_l, y_l, w_l).\n",
        "        tuple(np.array, np.array, np.array): tuple of the right split data (X_l, y_l, w_l)\n",
        "    \"\"\"\n",
        "\n",
        "    if categorical:\n",
        "        left_mask =(X[:, column] == value)\n",
        "    else:\n",
        "        left_mask = (X[:, column] < value)\n",
        "\n",
        "    # Using the binary masks `left_mask`, we split X, y, and sample_weights.\n",
        "    # for reference: https://numpy.org/doc/stable/user/basics.indexing.html#boolean-array-indexing\n",
        "    X_l, y_l, w_l = X, y, sample_weights # <- EDIT THIS LINE\n",
        "    X_r, y_r, w_r = X, y, sample_weights # <- EDIT THIS LINE\n",
        "\n",
        "    return (X_l, y_l, w_l), (X_r, y_r, w_r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EMkU0OTPGJ5"
      },
      "source": [
        "For a given feature index, we need to estimate the best value $s$ to use as threshold (for numerical variables) or predicate (for categorical variables). We need to implement the function the searches for $s$ that minimizes the _GINI-index_. Let's do this in the following cell by calling our previously defined two functions `split_samples` and `gini_index`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def gini_split_value(X, y, sample_weights, column, categorical):\n",
        "    \"\"\"\n",
        "    Calculate the GINI-index based on `column` with the split that minimizes the GINI-index.\n",
        "    Arguments:\n",
        "        X: training features, of shape (N, p).\n",
        "        y: vector of training labels, of shape (N,).\n",
        "        sample_weights: weights for each samples, of shape (N,).\n",
        "        column: the column of the feature for calculating. 0 <= column < D\n",
        "        categorical: boolean value indicating whether column is a categorical variable or numerical.\n",
        "    Returns:\n",
        "        (float, float): the resulted GINI-index and the corresponding value used in splitting.\n",
        "    \"\"\"\n",
        "\n",
        "    unique_vals = np.unique(X[:, column])\n",
        "\n",
        "    assert len(unique_vals) > 1, f\"There must be more than one distinct feature value. Given: {unique_vals}.\"\n",
        "\n",
        "    gini_index_val, threshold = np.inf, None\n",
        "\n",
        "    # split the values of i-th feature and calculate the cost\n",
        "    for value in unique_vals:\n",
        "        (X_l, y_l, w_l), (X_r, y_r, w_r) = (X, y, sample_weights), (X, y, sample_weights) ## <-- EDIT THIS LINE\n",
        "\n",
        "        # if one of the two sides is empty, skip this split.\n",
        "        if len(y_l) == 0 or len(y_r) == 0:\n",
        "            continue\n",
        "\n",
        "        # Weights for combining GINI-index from two branches\n",
        "        p_left = sum(w_l)/(sum(w_l) + sum(w_r))\n",
        "        p_right = 1 - p_left\n",
        "        new_cost = 0 ## <-- EDIT THIS LINE\n",
        "        if new_cost < gini_index_val:\n",
        "              gini_index_val, threshold = new_cost, value\n",
        "\n",
        "    return gini_index_val, threshold"
      ],
      "metadata": {
        "id": "0X87xfpLaNur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fWSvdhqP0iY"
      },
      "source": [
        "# evaluate for feature sepal width (cm)\n",
        "gini_split_value(X_train.to_numpy(), y_train.to_numpy(), training_weights, 3, columns_dict[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVkI3wBVQ12B"
      },
      "source": [
        "It's now time to choose the best feature to split by calling the function `gini_split_value` for each feature."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## EDIT THIS FUNCTION\n",
        "def gini_split(X, y, sample_weights, columns_dict):\n",
        "    \"\"\"\n",
        "    Choose the best feature to split according to criterion.\n",
        "    Args:\n",
        "        X: training features, of shape (N, p).\n",
        "        y: vector of training labels, of shape (N,).\n",
        "        sample_weights: weights for each samples, of shape (N,).\n",
        "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
        "    Returns:\n",
        "        (int, float): the best feature index and value used in splitting.\n",
        "        If the feature index is None, then no valid split for the current Node.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize `split_column` to None, so if None returned this means there is no valid split at the current node.\n",
        "    min_gini_index = np.inf\n",
        "    split_column = None\n",
        "    split_val = np.nan\n",
        "\n",
        "    for column, categorical in columns_dict.items():\n",
        "        # skip column if samples are not seperable by that column.\n",
        "        if len(np.unique(X[:, column])) < 2:\n",
        "            continue\n",
        "        gini_index, current_split_val = 13.5  ## <-- EDIT THIS LINE\n",
        "\n",
        "\n",
        "        # To scan for the best split corresponding the minimum gini_index\n",
        "        if False: ## <-- EDIT THIS LINE\n",
        "            # Keep track with:\n",
        "\n",
        "            # 1. the current minimum gini-index value,\n",
        "            min_gini_index = None ## <-- EDIT THIS LINE\n",
        "\n",
        "            # 2. corresponding column,\n",
        "            split_column = None ## <-- EDIT THIS LINE\n",
        "\n",
        "            # 3. corresponding split threshold.\n",
        "            split_val = None ## <-- EDIT THIS LINE\n",
        "\n",
        "    return split_column, split_val"
      ],
      "metadata": {
        "id": "dMHkUVxedtjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxY8uqmHRc-N"
      },
      "source": [
        "# evaluate which feature is best\n",
        "gini_split(X_train.to_numpy(), y_train.to_numpy(), training_weights, columns_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trLa8CV7SF2I"
      },
      "source": [
        "Now, we need a function that returns the label which appears the most in our label variable `y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F32t-yHzG864"
      },
      "source": [
        "def majority_vote(y, sample_weights):\n",
        "    \"\"\"\n",
        "    Return the label which appears the most in y.\n",
        "    Args:\n",
        "        y: vector of training labels, of shape (N,).\n",
        "        sample_weights: weights for each samples, of shape (N,).\n",
        "    Returns:\n",
        "        (int): the majority label\n",
        "    \"\"\"\n",
        "    majority_label = {yi: 0 for yi in set(y)}\n",
        "\n",
        "    for yi, wi in zip(y, sample_weights):\n",
        "        majority_label[yi] += wi\n",
        "\n",
        "    label_prediction = y[0] ## <-- EDIT THIS LINE\n",
        "\n",
        "    return label_prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZzFMoQtS4tR"
      },
      "source": [
        "# evaluate it\n",
        "majority_vote(y_train.to_numpy(), training_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify\n",
        "y_train.value_counts()"
      ],
      "metadata": {
        "id": "9b2uvRuMzBuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHihzl6CS9X0"
      },
      "source": [
        "Finally, we can build the decision tree by using `choose_best_feature` to find the best feature to split the `X`, and `split_dataset` to get sub-trees.\n",
        "\n",
        "Note: If you are not familiar with recursion, check the following tutorial [Python Recursion](https://www.datamentor.io/python/recursive-function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxsU-BWeEZRu"
      },
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def build_tree(X, y, sample_weights, columns_dict, feature_names, depth,  max_depth=10, min_samples_leaf=2):\n",
        "    \"\"\"Build the decision tree according to the data.\n",
        "    Args:\n",
        "        X: (np.array) training features, of shape (N, p).\n",
        "        y: (np.array) vector of training labels, of shape (N,).\n",
        "        sample_weights: weights for each samples, of shape (N,).\n",
        "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
        "        feature_names (list): record the name of features in X in the original dataset.\n",
        "        depth (int): current depth for this node.\n",
        "    Returns:\n",
        "        (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n",
        "          1. 'feature_name': The column name of the split.\n",
        "          2. 'feature_index': The column index of the split.\n",
        "          3. 'value': The value used for the split.\n",
        "          4. 'categorical': indicator for categorical/numerical variables.\n",
        "          5. 'majority_label': For leaf nodes, this stores the dominant label. Otherwise, it is None.\n",
        "          6. 'left': The left sub-tree with the same structure.\n",
        "          7. 'right' The right sub-tree with the same structure.\n",
        "        Example:\n",
        "            mytree = {\n",
        "                'feature_name': 'petal length (cm)',\n",
        "                'feature_index': 2,\n",
        "                'value': 3.0,\n",
        "                'categorical': False,\n",
        "                'majority_label': None,\n",
        "                'left': {\n",
        "                    'feature_name': str,\n",
        "                    'feature_index': int,\n",
        "                    'value': float,\n",
        "                    'categorical': bool,\n",
        "                    'majority_label': None,\n",
        "                    'left': {..etc.},\n",
        "                    'right': {..etc.}\n",
        "                }\n",
        "                'right': {\n",
        "                    'feature_name': str,\n",
        "                    'feature_index': int,\n",
        "                    'value': float,\n",
        "                    'categorical': bool,\n",
        "                    'majority_label': None,\n",
        "                    'left': {..etc.},\n",
        "                    'right': {..etc.}\n",
        "                }\n",
        "            }\n",
        "    \"\"\"\n",
        "    # include a clause for the cases where (i) no feature, (ii) all labels are the same, (iii) depth exceed, or (iv) X is too small\n",
        "    if len(np.unique(y))==1 or depth>=max_depth or len(X)<=min_samples_leaf:\n",
        "        return {'majority_label': majority_vote(y, sample_weights)}\n",
        "\n",
        "    split_index, split_val = 0, 0  ## <-- EDIT THIS LINE\n",
        "\n",
        "    # If no valid split at this node, use majority vote.\n",
        "    if split_index is None:\n",
        "        return {'majority_label': majority_vote(y, sample_weights)}\n",
        "\n",
        "    categorical = columns_dict[split_index]\n",
        "\n",
        "    # Split samples (X, y, sample_weights) given column, split-value, and categorical flag.\n",
        "    (X_l, y_l, w_l), (X_r, y_r, w_r) = (X, y, sample_weights, split_index), (X, y, sample_weights, split_index) ## <-- EDIT THIS LINE\n",
        "    return {\n",
        "        'feature_name': feature_names[split_index],\n",
        "        'feature_index': split_index,\n",
        "        'value': split_val,\n",
        "        'categorical': categorical,\n",
        "        'majority_label': None,\n",
        "        'left': build_tree(X_l, y_l, w_l, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf),\n",
        "        'right': build_tree(X_r, y_r, w_r, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VZ5LpqAUqaa"
      },
      "source": [
        "We define a wrapper function that we call `train` to call this `build_tree` function with the appropriate arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwqMWBa3EZOW"
      },
      "source": [
        "def train(X, y,  columns_dict, sample_weights=None):\n",
        "    \"\"\"\n",
        "    Build the decision tree according to the training data.\n",
        "    Args:\n",
        "        X: (pd.Dataframe) training features, of shape (N, p). Each X[i] is a training sample.\n",
        "        y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n",
        "        an integer in the range 0 <= y[i] <= C. Here C = 1.\n",
        "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
        "        sample_weights: weights for each samples, of shape (N,).\n",
        "    \"\"\"\n",
        "    if sample_weights is None:\n",
        "        # if the sample weights is not provided, we assume the samples have uniform weights\n",
        "        sample_weights = np.ones(X.shape[0]) / X.shape[0]\n",
        "    else:\n",
        "        sample_weights = np.array(sample_weights) / np.sum(sample_weights)\n",
        "\n",
        "    feature_names = X.columns.tolist()\n",
        "    X = X.to_numpy()\n",
        "    y = y.to_numpy()\n",
        "    return build_tree(X, y, sample_weights, columns_dict, feature_names, depth=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1T3PFpjEZLd"
      },
      "source": [
        "# fit the decision tree with training data\n",
        "tree = train(X_train, y_train, columns_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree"
      ],
      "metadata": {
        "id": "nlMbFn9Sox0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAE19BbTXYJb"
      },
      "source": [
        "<a name=\"section-1d\"></a>\n",
        "\n",
        "## (1d) Decision Tree Classification Algorithm [^](#outline)\n",
        "\n",
        "\n",
        "Now, we want to use this fitted decision tree to make predictions for our test set `X_test`. To do so, we first define a function `classify` that takes each single data point `x` as an argument. We will write a wrapper function `predict` that calls this `classify` function. In the following implementation, we will traverse a given decision tree basen on the current feature vector that we have. Traversing tree-like objects in programming is straightforwardly implemented using recursion. If you are not familiar with recursion, check the following tutorial [Python Recursion](https://www.datamentor.io/python/recursive-function).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIJYH7hNWOd-"
      },
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def classify(tree, x):\n",
        "    \"\"\"\n",
        "    Classify a single sample with the fitted decision tree.\n",
        "    Args:\n",
        "        x: ((pd.Dataframe) a single sample features, of shape (D,).\n",
        "    Returns:\n",
        "        (int): predicted testing sample label.\n",
        "    \"\"\"\n",
        "    if tree['majority_label'] is not None:\n",
        "        return tree['majority_label']\n",
        "\n",
        "    elif tree['categorical']:\n",
        "        if x[tree['feature_index']] == tree['value']:\n",
        "            # go to left branch\n",
        "            return classify(tree['left'], x)\n",
        "        else:\n",
        "            # go to right branch\n",
        "            return classify(tree['right'], x)\n",
        "\n",
        "    else:\n",
        "\n",
        "        if 2 + 2 == 5: # <- EDIT THIS LINE\n",
        "            # go to left branch\n",
        "            return classify(None, x)  # <- EDIT THIS LINE\n",
        "        else:\n",
        "            # go to right branch\n",
        "            return classify(None, x)  # <- EDIT THIS LINE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_WNC05qWOYG"
      },
      "source": [
        "def predict(tree, X):\n",
        "    \"\"\"\n",
        "    Predict classification results for X.\n",
        "    Args:\n",
        "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
        "    Returns:\n",
        "        (np.array): predicted testing sample labels, of shape (N,).\n",
        "    \"\"\"\n",
        "    if len(X.shape) == 1:\n",
        "        return classify(tree, X)\n",
        "    else:\n",
        "        return np.array([classify(tree, x) for x in X])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yiJR4aQX2RI"
      },
      "source": [
        "To evaluate how well the tree can generalise to unseen data in `X_test`, we define a short function that computes the mean accuracy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## EDIT THIS FUNCTION\n",
        "def tree_score(tree, X_test, y_test):\n",
        "    y_pred = y_test ## <-- EDIT THIS LINE\n",
        "    return np.mean(y_pred==y_test)"
      ],
      "metadata": {
        "id": "y-xx90u9qySr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX4nyXObWjI0"
      },
      "source": [
        "print('Training accuracy:', tree_score(tree, X_train.to_numpy(), y_train.to_numpy()))\n",
        "print('Test accuracy:', tree_score(tree, X_test.to_numpy(), y_test.to_numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaWlcZAZYQrv"
      },
      "source": [
        "## Questions on Decision Trees:\n",
        "\n",
        "1. What do the results above tell you? Has the decision tree you implemented overfitted to the training data?\n",
        "2. If so, what can you change to counteract this problem?\n",
        "3. Can you think of other information criteria than the GINI-index? If so, try implementing them and compare it to the decision tree with the GINI-index.\n",
        "4. Recall ROC-curve (see Notebooks of Logistic Regression) that we used to compute the AUC as a more robust performance estimate, it required that each decision is associated with a probability (which can also be any kind of score representing the decision confidence). **Suggest** what can be used as score (or confidence) to be associated with each decision. Try to adapt for ROC-curve-compatible decisions and plot the ROC-curve indicating the AUC.\n",
        "5. Can you provide another implementation of `classify` without using recursion?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-2\"></a>\n",
        "\n",
        "# Section 2: From Decision Tree to Random Forest [^](#outline)\n",
        "\n",
        "**To avoid confusion in the text here**:\n",
        "  - **training-samples** used to mean the rows in `X_train` in other occasions, and we will use **training-examples** or **training-instances** instead to avoid confusion with\n",
        "  - a **random-sample**, which means a selection of instances by chance from a group of instances.\n"
      ],
      "metadata": {
        "id": "4QB29VGJsMOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<a name=\"section-2a\"></a>\n",
        "\n",
        "## (2a) Intro to Random Forest [^](#outline)\n",
        "\n",
        "It is now the time to build ensemble model from individual models for the first time in this course, applied to ensemble of decision trees.\n",
        "\n",
        "\n",
        "One approach for ensemble methods is [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)). In particular we can apply bootstrapping for multiple decision trees upon on levels:\n",
        "\n",
        "1. Bootstrap on the training-instances: each model is trained on a random-sample (**with replacement**) from the training-instances. Eventually, as learned from lecture notes, decisions are aggregated across the models, hence the name _bagging_ (Bootstrap Aggregate).\n",
        "\n",
        "  Note that for each decision tree we randomly-sample from training-instances **with replacement** and the size of each random-sample is the same size of the training-instances. This will basically allow some training-instances to be duplicated and other instances to be excluded.\n",
        "2. Feature bagging: at each split, a subset of features are considered before searching for the best split column $j$ and value $s$.\n",
        "\n",
        "\n",
        "The image below illustrates random forest construction from individual decision trees. Notice the random-sampling that occurs at two levels:\n",
        "\n",
        "1. At red-stars: bootstrapping (random-sampling with replacement) of training-instances.\n",
        "2. At blue-circles: random-sampling without replacement of features at each split node.\n",
        "\n",
        "![rf](https://drive.google.com/uc?export=view&id=19LdaNVHlSp7WlHw7DfaMxu38fEJ7RIxW)\n",
        "\n",
        "This design will leave us two hyperparameters for random forest:\n",
        "\n",
        "1. $B$: number of decision trees.\n",
        "2. `n_features`: number of features (columns) sampled, **without replacement**, at each split before searching for the best split column $j$ and value $s$. For classification task `n_features` is recommended to be the number of all columns divided by 3.\n",
        "3. Add to these the decision trees hyperparameters like `max_depth` and `min_leaf_sample`.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The second layer of sampling, i.e. _feature bagging_, requires us to re-implement the `gini_split` function to subsample from the feature columns before searching for the best split.\n",
        "\n",
        "\n",
        "Modify the following function that will be employed for random forest decision trees to find the best split column $j$ (out from sampled `n_features` columns) and value $s$. Consider using the `gini_split_value` function that was already implemented for individual decision tree implementation."
      ],
      "metadata": {
        "id": "fYFqIY-pCxhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-2b\"></a>\n",
        "\n",
        "## (2b) Training: Feature Bagging Implementation [^](#outline)\n",
        "\n",
        "Remember that we need to sample `n_features` from the column at each split, before we scan for the optimal splitting column.  This will require us to make  adapted versions of two of the decision tree functions:\n",
        "\n",
        "1. `gini_split`: We need a new version `gini_split_rf` to add sampling step here before scanning for the optimal splitting column.\n",
        "2. `build_tree`: because it depends on `gini_split`. We make another version `build_tree_rf` that calls `gini_split_rf`."
      ],
      "metadata": {
        "id": "ZyQmhyLtDoTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## EDIT THIS FUNCTION\n",
        "def gini_split_rf(n_features, X, y, sample_weights, columns_dict):\n",
        "    \"\"\"\n",
        "    Choose the best feature to split according to criterion.\n",
        "    Args:\n",
        "        n_features: number of sampled features.\n",
        "        X: training features, of shape (N, p).\n",
        "        y: vector of training labels, of shape (N,).\n",
        "        sample_weights: weights for each samples, of shape (N,).\n",
        "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
        "    Returns:\n",
        "        (float, int, float): the minimized gini-index, the best feature index and value used in splitting.\n",
        "    \"\"\"\n",
        "\n",
        "    # The added sampling step.\n",
        "    columns = np.random.choice(list(columns_dict.keys()), n_features, replace=False)\n",
        "    columns_dict = {c: columns_dict[c] for c in columns}\n",
        "\n",
        "    min_gini_index, split_column, split_val = np.inf, 0, 0\n",
        "\n",
        "    # Only scan through the sampled columns in `columns_dict`.\n",
        "    for column, categorical in columns_dict.items():\n",
        "        # skip column if samples are not seperable by that column.\n",
        "        if len(np.unique(X[:, column])) < 2:\n",
        "            continue\n",
        "\n",
        "        # search for the best splitting value for the given column.\n",
        "        gini_index, val = 0, 0 ## <-- EDIT THIS LINE\n",
        "        if gini_index < min_gini_index:\n",
        "            min_gini_index, split_column, split_val = gini_index, column, val\n",
        "\n",
        "    return min_gini_index, split_column, split_val"
      ],
      "metadata": {
        "id": "a7JBa9PAAyZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since `build_tree` depends on `gini_split`, we need to slightly modify it to call `gini_split_rf` instead."
      ],
      "metadata": {
        "id": "Naimaw_mBFrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def build_tree_rf(n_features, X, y, sample_weights, columns_dict, feature_names, depth,  max_depth=6, min_samples_leaf=5):\n",
        "    \"\"\"Build the decision tree according to the data.\n",
        "    Args:\n",
        "        X: (np.array) training features, of shape (N, p).\n",
        "        y: (np.array) vector of training labels, of shape (N,).\n",
        "        sample_weights: weights for each samples, of shape (N,).\n",
        "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
        "        feature_names (list): record the name of features in X in the original dataset.\n",
        "        depth (int): current depth for this node.\n",
        "    Returns:\n",
        "        (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n",
        "          1. 'feature_name': The column name of the split.\n",
        "          2. 'feature_index': The column index of the split.\n",
        "          3. 'value': The value used for the split.\n",
        "          4. 'categorical': indicator for categorical/numerical variables.\n",
        "          5. 'majority_label': For leaf nodes, this stores the dominant label. Otherwise, it is None.\n",
        "          6. 'left': The left sub-tree with the same structure.\n",
        "          7. 'right' The right sub-tree with the same structure.\n",
        "        Example:\n",
        "            mytree = {\n",
        "                'feature_name': 'petal length (cm)',\n",
        "                'feature_index': 2,\n",
        "                'value': 3.0,\n",
        "                'categorical': False,\n",
        "                'majority_label': None,\n",
        "                'left': {\n",
        "                    'feature_name': str,\n",
        "                    'feature_index': int,\n",
        "                    'value': float,\n",
        "                    'categorical': bool,\n",
        "                    'majority_label': None,\n",
        "                    'left': {..etc.},\n",
        "                    'right': {..etc.}\n",
        "                }\n",
        "                'right': {\n",
        "                    'feature_name': str,\n",
        "                    'feature_index': int,\n",
        "                    'value': float,\n",
        "                    'categorical': bool,\n",
        "                    'majority_label': None,\n",
        "                    'left': {..etc.},\n",
        "                    'right': {..etc.}\n",
        "                }\n",
        "            }\n",
        "    \"\"\"\n",
        "    # include a clause for the cases where (i) all lables are the same, (ii) depth exceed (iii) X is too small\n",
        "    if len(np.unique(y)) == 1 or depth>=max_depth or len(X)<=min_samples_leaf:\n",
        "        return {'majority_label': majority_vote(y, sample_weights)}\n",
        "\n",
        "    else:\n",
        "        GI, split_column, split_val = 0, 0, 0  ## <-- EDIT THIS LINE\n",
        "\n",
        "        # If GI is infinity, it means that samples are not seperable by the sampled features.\n",
        "        if GI == np.inf:\n",
        "            return {'majority_label': majority_vote(y, sample_weights)}\n",
        "        categorical = columns_dict[split_column]\n",
        "        (X_l, y_l, w_l), (X_r, y_r, w_r) = (X, y, sample_weights), (X, y, sample_weights) ## <-- EDIT THIS LINE\n",
        "        return {\n",
        "            'feature_name': feature_names[split_column],\n",
        "            'feature_index': split_column,\n",
        "            'value': split_val,\n",
        "            'categorical': categorical,\n",
        "            'majority_label': None,\n",
        "            'left': build_tree_rf(n_features, X_l, y_l, w_l, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf),\n",
        "            'right': build_tree_rf(n_features, X_r, y_r, w_r, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf)\n",
        "        }"
      ],
      "metadata": {
        "id": "-HMP1-Z_A7yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-2c\"></a>\n",
        "\n",
        "## (2c) Training: Bootstrapping Implementation [^](#outline)"
      ],
      "metadata": {
        "id": "Yc9C_hI6HhR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is time to write the training function the constructs multiple decision trees, each operating on a subset of samples (with replacement)."
      ],
      "metadata": {
        "id": "bjbNhpLwBbGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def train_rf(B, n_features, X, y,  columns_dict, sample_weights=None):\n",
        "    \"\"\"\n",
        "    Build the decision tree according to the training data.\n",
        "    Args:\n",
        "        B: number of decision trees.\n",
        "        X: (pd.Dataframe) training features, of shape (N, p). Each X[i] is a training sample.\n",
        "        y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n",
        "        an integer in the range 0 <= y[i] <= C. Here C = 1.\n",
        "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
        "        sample_weights: weights for each samples, of shape (N,).\n",
        "    \"\"\"\n",
        "    if sample_weights is None:\n",
        "        # if the sample weights is not provided, we assume the samples have uniform weights\n",
        "        sample_weights = np.ones(X.shape[0]) / X.shape[0]\n",
        "    else:\n",
        "        sample_weights = np.array(sample_weights) / np.sum(sample_weights)\n",
        "\n",
        "    feature_names = X.columns.tolist()\n",
        "    X = X.to_numpy()\n",
        "    y = y.to_numpy()\n",
        "    N = X.shape[0]\n",
        "    training_indices = np.arange(N)\n",
        "    trees = []\n",
        "\n",
        "    for _ in range(B):\n",
        "        # Sample the training_indices (with replacement)\n",
        "        sample = training_indices # <- EDIT THIS LINE\n",
        "\n",
        "        # Ensure the size of the random sample is the same size of training sample\n",
        "        assert len(sample) == len(training_indices)\n",
        "\n",
        "        X_sample = X[sample, :]\n",
        "        y_sample = y[sample]\n",
        "        w_sample = sample_weights[sample]\n",
        "        tree = build_tree_rf(n_features, X_sample, y_sample, w_sample,\n",
        "                             columns_dict, feature_names, depth=1)\n",
        "        trees.append(tree)\n",
        "\n",
        "    return trees"
      ],
      "metadata": {
        "id": "RTz5V66SBnK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-2d\"></a>\n",
        "\n",
        "## (2d) Classification: Aggregation [^](#outline)"
      ],
      "metadata": {
        "id": "9GcTy26LIsrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write the prediction function which aggregates the decision from all decision trees and returns the class with highest probability."
      ],
      "metadata": {
        "id": "wCqwdq_uJ_6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_rf(rf, X):\n",
        "    \"\"\"\n",
        "    Predict classification results for X.\n",
        "    Args:\n",
        "        rf: A trained random forest through train_rf function.\n",
        "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
        "    Returns:\n",
        "        (np.array): predicted testing sample labels, of shape (N,).\n",
        "    \"\"\"\n",
        "\n",
        "    # EDIT THIS FUNCTION\n",
        "    def aggregate(decisions):\n",
        "        \"\"\"\n",
        "        This function takes a list of predicted labels produced by a list\n",
        "        of decision trees and returns the label with the majority of votes.\n",
        "        \"\"\"\n",
        "        count = defaultdict(int)\n",
        "        for decision in decisions:\n",
        "            count[decision] += 1\n",
        "        return decisions[0] # <- EDIT THIS LINE\n",
        "\n",
        "    if len(X.shape) == 1:\n",
        "        # if we have one sample\n",
        "        return aggregate([classify(tree, X) for tree in rf])\n",
        "    else:\n",
        "        # if we have multiple samples\n",
        "        return np.array([aggregate([classify(tree, x) for tree in rf]) for x in X])"
      ],
      "metadata": {
        "id": "TurQWLyZJ_L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## EDIT THIS FUNCTION\n",
        "def rf_score(rf, X_test, y_test):\n",
        "    y_pred = y_test ## <-- EDIT THIS LINE\n",
        "    return np.mean(y_pred==y_test)"
      ],
      "metadata": {
        "id": "mkYPt9KNLT5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = int(np.sqrt(X_train.shape[1]))\n",
        "B = 30\n",
        "# fit the random forest with training data\n",
        "rf = train_rf(B, n_features, X_train, y_train, columns_dict)"
      ],
      "metadata": {
        "id": "4dBqmDLlJIQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_score(rf, X_train.to_numpy(), y_train.to_numpy())"
      ],
      "metadata": {
        "id": "04z7XrNxLmEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_score(rf, X_test.to_numpy(), y_test.to_numpy())"
      ],
      "metadata": {
        "id": "ta4lHvovRDGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions on Random Forest:\n",
        "\n",
        "1. Compare the test accuracy of a sole decision tree with random forest. What do you conclude?\n",
        "2. How to make Random Forest decisions ROC-curve-compatible (i.e. decisions with confidence scores). **Suggest** what can be used as score (or confidence) to be associated with each decision. Try to plot a diagram with two ROC-curves:\n",
        "  1. for decision tree model.\n",
        "  2. for random forest model.\n",
        "3. Can you replicate your results using [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)?\n",
        "6. Based on sklearn's documentation, can you see any differences in the algorithms that are implemented in sklearn?\n"
      ],
      "metadata": {
        "id": "ojYUTxEDmy1u"
      }
    }
  ]
}