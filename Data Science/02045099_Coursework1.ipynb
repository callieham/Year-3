{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb_IWF6UQRGk"
      },
      "source": [
        "# Coursework 1 - Supervised learning\n",
        "\n",
        "**Replace CID in the file name with your CID**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTeLZnrzE0Wy"
      },
      "source": [
        "# Outline\n",
        "\n",
        "\n",
        "- [Task 1](#task-1): Regression <a name=\"index-task-1\"></a>\n",
        "  - [(1.1)](#task-11) Random Forest <a name=\"index-task-11\"></a>\n",
        "    - [(1.1.1)](#task-111) <a name=\"index-task-111\"></a>\n",
        "    - [(1.1.2)](#task-112) <a name=\"index-task-112\"></a>\n",
        "    - [(1.1.3)](#task-113) <a name=\"index-task-113\"></a>\n",
        "  - [(1.2)](#task-12) Multi-layer Perceptron <a name=\"index-task-12\"></a>\n",
        "    - [(1.2.1)](#task-121) <a name=\"index-task-121\"></a>\n",
        "    - [(1.2.2)](#task-122) <a name=\"index-task-122\"></a>\n",
        "    - [(1.2.3)](#task-123) <a name=\"index-task-123\"></a>\n",
        "- [Task 2](#task-2): Classification <a name=\"index-task-2\"></a>\n",
        "  - [(2.1)](#task-21) k-Nearest Neighbours <a name=\"index-task-21\"></a>\n",
        "    - [(2.1.1)](#task-211)  <a name=\"index-task-211\"></a>\n",
        "    - [(2.1.2)](#task-212) <a name=\"index-task-212\"></a>\n",
        "    - [(2.1.3)](#task-213) <a name=\"index-task-213\"></a>\n",
        "    - [(2.1.4)](#task-214) <a name=\"index-task-214\"></a>\n",
        "  - [(2.2)](#task-22) Logistic regression vs kernel logistic regression <a name=\"index-task-22\"></a>\n",
        "    - [(2.2.1)](#task-221) <a name=\"index-task-221\"></a>\n",
        "    - [(2.2.2)](#task-222) <a name=\"index-task-222\"></a>\n",
        "    - [(2.2.3)](#task-223) <a name=\"index-task-223\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4LmL6R9N1B-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_QE32lOMff_"
      },
      "source": [
        "<a name=\"task-1\"></a>\n",
        "\n",
        "# (1) Task 1: Regression [(index)](#index-task-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLl66QsJMfzc"
      },
      "source": [
        "<a name=\"task-11\"></a>\n",
        "\n",
        "## (1.1) Random Forest [(index)](#index-task-11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import packages\n",
        "from collections import defaultdict\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load dataset\n",
        "data = pd.read_csv('nanoelectrodes_capacitance_samples.csv')\n",
        "\n",
        "X_train = data.drop(\"Capacitance ($\\mu F / cm^2$)\", axis = 'columns')\n",
        "y_train = data[\"Capacitance ($\\mu F / cm^2$)\"]\n",
        "\n",
        "test_data = pd.read_csv('nanoelectrodes_capacitance_test.csv')\n",
        "X_test = test_data.drop(\"Capacitance ($\\mu F / cm^2$)\", axis = 'columns')\n",
        "y_test = test_data[\"Capacitance ($\\mu F / cm^2$)\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T47skSuUMf8M"
      },
      "source": [
        "<a name=\"task-111\"></a>\n",
        "\n",
        "### (1.1.1) [(index)](#index-task-111)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 352,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss(y, y_pred):\n",
        "    return ((y - y_pred) ** 2).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mse(y, y_pred):\n",
        "    return ((y - y_pred) ** 2).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rsq(y, y_pred):\n",
        "    return 1 - (np.linalg.norm(y - y_pred) ** 2) / (np.linalg.norm(y - y.mean()) ** 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 355,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_samples(X, y, column, value):\n",
        "    \"\"\"\n",
        "    Return the split of data whose column-th feature:\n",
        "        less than value, in case `column` is not categorical (i.e. numerical)\n",
        "\n",
        "    Arguments:\n",
        "        X: training features, of shape (N, p).\n",
        "        y: vector of training labels, of shape (N,).\n",
        "        column: the column of the feature for splitting.\n",
        "        value: splitting threshold  the samples\n",
        "    Returns:\n",
        "        tuple(np.array, np.array): tuple of the left split data (X_l, y_l).\n",
        "        tuple(np.array, np.array): tuple of the right split data (X_l, y_l)\n",
        "    \"\"\"\n",
        "\n",
        "    left_mask = (X[:, column] < value)\n",
        "\n",
        "    # Using the binary masks `left_mask`, we split X and y.\n",
        "    X_l, y_l = X[left_mask], y[left_mask] \n",
        "    X_r, y_r = X[~left_mask], y[~left_mask] \n",
        "\n",
        "    return (X_l, y_l), (X_r, y_r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_split_value(X, y, column):\n",
        "    \"\"\"\n",
        "    Calculate the mse based on `column` with the split that minimizes the loss.\n",
        "    Arguments:\n",
        "        X: training features, of shape (N, p).\n",
        "        y: vector of training labels, of shape (N,).\n",
        "        column: the column of the feature for calculating. 0 <= column < D\n",
        "    Returns:\n",
        "        (float, float): the resulted mse and the corresponding value used in splitting.\n",
        "    \"\"\"\n",
        "\n",
        "    unique_vals = np.unique(X[:, column])\n",
        "\n",
        "    assert len(unique_vals) > 1, f\"There must be more than one distinct feature value. Given: {unique_vals}.\"\n",
        "\n",
        "    loss_val, threshold = np.inf, None\n",
        "\n",
        "    # split the values of i-th feature and calculate the cost\n",
        "    for value in unique_vals:\n",
        "        (X_l, y_l), (X_r, y_r) = split_samples(X, y, column, value) \n",
        "\n",
        "        # if one of the two sides is empty, skip this split.\n",
        "        if len(y_l) == 0 or len(y_r) == 0:\n",
        "            continue\n",
        "\n",
        "        new_loss = loss(y_l, y_l.mean()) + loss(y_r, y_r.mean()) \n",
        "        if new_loss < loss_val:\n",
        "              loss_val, threshold = new_loss, value\n",
        "\n",
        "    return loss_val, threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_split(X, y):\n",
        "    \"\"\"\n",
        "    Choose the best feature to split according to criterion.\n",
        "    Args:\n",
        "        X: training features, of shape (N, p).\n",
        "        y: vector of training labels, of shape (N,).\n",
        "    Returns:\n",
        "        (int, float): the best feature index and value used in splitting.\n",
        "        If the feature index is None, then no valid split for the current Node.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize `split_column` to None, so if None returned this means there is no valid split at the current node.\n",
        "    min_loss = np.inf\n",
        "    split_column = None\n",
        "    split_val = np.nan\n",
        "    m, n = X.shape\n",
        "\n",
        "    for col in range(n):\n",
        "        # skip column if samples are not seperable by that column.\n",
        "        if len(np.unique(X[:, col])) < 2:\n",
        "            continue\n",
        "        loss, current_split_val = loss_split_value(X, y, col)  \n",
        "\n",
        "        # To scan for the best split corresponding the minimum mse_index\n",
        "        if loss < min_loss: \n",
        "            # Keep track with:\n",
        "\n",
        "            # 1. the current minimum mse value,\n",
        "            min_loss = loss\n",
        "\n",
        "            # 2. corresponding column,\n",
        "            split_column = col\n",
        "\n",
        "            # 3. corresponding split threshold.\n",
        "            split_val = current_split_val \n",
        "\n",
        "    return split_column, split_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, 2.5)"
            ]
          },
          "execution_count": 358,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_split(X_train.to_numpy(), y_train.to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tree(X, y, feature_names, depth,  max_depth=10, min_samples_leaf=10):\n",
        "    \"\"\"Build the decision tree according to the data.\n",
        "    Args:\n",
        "        X: (np.array) training features, of shape (N, p).\n",
        "        y: (np.array) vector of training labels, of shape (N,).\n",
        "        feature_names (list): record the name of features in X in the original dataset.\n",
        "        depth (int): current depth for this node.\n",
        "    Returns:\n",
        "        (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n",
        "          1. 'feature_name': The column name of the split.\n",
        "          2. 'feature_index': The column index of the split.\n",
        "          3. 'value': The value used for the split.\n",
        "          4. 'mean_value': For leaf nodes, this stores the dominant label. Otherwise, it is None.\n",
        "          5. 'left': The left sub-tree with the same structure.\n",
        "          6. 'right' The right sub-tree with the same structure.\n",
        "    \"\"\"\n",
        "    # include a clause for the cases where (i) no feature, (ii) depth exceed, or (iii) X is too small\n",
        "    if len(np.unique(y))==1 or depth>=max_depth or len(X)<=min_samples_leaf:\n",
        "        return {'mean_value': np.mean(y)}\n",
        "\n",
        "    split_index, split_val = loss_split(X, y)\n",
        "\n",
        "    # If no valid split at this node, use mean.\n",
        "    if split_index is None:\n",
        "        return {'mean_value': np.mean(y)}\n",
        "\n",
        "    # Split samples (X, y) given column and split-value.\n",
        "    (X_l, y_l), (X_r, y_r) = split_samples(X, y, split_index, split_val) \n",
        "    return {\n",
        "        'feature_name': feature_names[split_index],\n",
        "        'feature_index': split_index,\n",
        "        'value': split_val,\n",
        "        'mean_value': None,\n",
        "        'left': build_tree(X_l, y_l, feature_names, depth + 1, max_depth, min_samples_leaf),\n",
        "        'right': build_tree(X_r, y_r, feature_names, depth + 1, max_depth, min_samples_leaf)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 360,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(X, y):\n",
        "    \"\"\"\n",
        "    Build the decision tree according to the training data.\n",
        "    Args:\n",
        "        X: (pd.Dataframe) training features, of shape (N, p). Each X[i] is a training sample.\n",
        "        y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n",
        "        an integer in the range 0 <= y[i] <= C. Here C = 1.\n",
        "    \"\"\"\n",
        "    feature_names = X.columns.tolist()\n",
        "    X = X.to_numpy()\n",
        "    y = y.to_numpy()\n",
        "    return build_tree(X, y, feature_names, depth=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 361,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree = train(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 503,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find(tree, x):\n",
        "    \"\"\"\n",
        "    Find the branch of a single sample with the fitted decision tree.\n",
        "    Args:\n",
        "        x: ((pd.Dataframe) a single sample features, of shape (D,).\n",
        "    Returns:\n",
        "        (int): predicted testing sample label.\n",
        "    \"\"\"\n",
        "    \n",
        "    if tree['mean_value'] is not None:\n",
        "        return tree['mean_value']\n",
        "\n",
        "    if x[tree['feature_index']] < tree['value']: \n",
        "        # go to left branch\n",
        "        return find(tree['left'], x)  \n",
        "    else:\n",
        "        # go to right branch\n",
        "        return find(tree['right'], x)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 492,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(tree, X):\n",
        "    \"\"\"\n",
        "    Predict regression results for X.\n",
        "    Args:\n",
        "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
        "    Returns:\n",
        "        (np.array): predicted testing sample labels, of shape (N,).\n",
        "    \"\"\"\n",
        "    if len(X.shape) == 1:\n",
        "        return find(tree, X)\n",
        "    else:\n",
        "        return np.array([find(tree, x) for x in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 493,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tree_results(tree, X, y):\n",
        "    \"\"\"\n",
        "    Return the R^2 and MSE score of the tree on the data.\n",
        "    Args:\n",
        "        tree: (dict) the decision tree.\n",
        "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
        "        y: (pd.Series) vector of testing labels, of shape (N,).\n",
        "    Returns:\n",
        "        (float): R^2 score of the tree on the data.\n",
        "        (float): MSE score of the tree on the data.\n",
        "    \"\"\"\n",
        "    y_pred = predict(tree, X.to_numpy())\n",
        "    return rsq(y, y_pred), mse(y, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 494,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.7503640060081045, 1654.4051506951573)\n",
            "(0.4896216113769649, 3369.5671069811706)\n"
          ]
        }
      ],
      "source": [
        "print(tree_results(tree, X_train, y_train))\n",
        "print(tree_results(tree, X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx4c9z5pMgDR"
      },
      "source": [
        "<a name=\"task-112\"></a>\n",
        "\n",
        "### (1.1.2) [(index)](#index-task-112)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 495,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_split_rf(n_features, X, y):\n",
        "    \"\"\"\n",
        "    Choose the best feature to split according to criterion.\n",
        "    Args:\n",
        "        n_features: number of sampled features.\n",
        "        X: training features, of shape (N, p).\n",
        "        y: vector of training labels, of shape (N,).\n",
        "    Returns:\n",
        "        (float, int, float): the minimized loss value.\n",
        "    \"\"\"\n",
        "\n",
        "    # The added sampling step.\n",
        "    columns = np.random.choice(list(range(12)), n_features, replace=False)\n",
        "\n",
        "\n",
        "    min_loss_val, split_column, split_val = np.inf, None, np.nan\n",
        "\n",
        "    # Only scan through the sampled columns in `columns_dict`.\n",
        "    for column in columns:\n",
        "        # skip column if samples are not seperable by that column.\n",
        "        if len(np.unique(X[:, column])) < 2:\n",
        "            continue\n",
        "\n",
        "        # search for the best splitting value for the given column.\n",
        "        loss_val, val = loss_split_value(X, y, column)\n",
        "        if loss_val < min_loss_val:\n",
        "            min_loss_val, split_column, split_val = loss_val, column, val\n",
        "\n",
        "    return min_loss_val, split_column, split_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 496,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tree_rf(n_features, X, y, feature_names, depth,  max_depth=10, min_samples_leaf=10):\n",
        "    \"\"\"Build the decision tree according to the data.\n",
        "    Args:\n",
        "        X: (np.array) training features, of shape (N, p).\n",
        "        y: (np.array) vector of training labels, of shape (N,).\n",
        "        feature_names (list): record the name of features in X in the original dataset.\n",
        "        depth (int): current depth for this node.\n",
        "    Returns:\n",
        "        (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n",
        "          1. 'feature_name': The column name of the split.\n",
        "          2. 'feature_index': The column index of the split.\n",
        "          3. 'value': The value used for the split.\n",
        "          4. 'categorical': indicator for categorical/numerical variables.\n",
        "          5. 'majority_label': For leaf nodes, this stores the dominant label. Otherwise, it is None.\n",
        "          6. 'left': The left sub-tree with the same structure.\n",
        "          7. 'right' The right sub-tree with the same structure.\n",
        "    \"\"\"\n",
        "    # include a clause for the cases where (i) all lables are the same, (ii) depth exceed (iii) X is too small\n",
        "    if len(np.unique(y)) == 1 or depth>=max_depth or len(X)<=min_samples_leaf:\n",
        "        return {'mean_value': np.mean(y)}\n",
        "\n",
        "    else:\n",
        "        loss, split_column, split_val = loss_split_rf(n_features, X, y)\n",
        "\n",
        "        # If loss is infinity, it means that samples are not seperable by the sampled features.\n",
        "        if loss == np.inf:\n",
        "            return {'mean_value': np.mean(y)}\n",
        "        (X_l, y_l), (X_r, y_r) = split_samples(X, y, split_column, split_val)\n",
        "        return {\n",
        "            'feature_name': feature_names[split_column],\n",
        "            'feature_index': split_column,\n",
        "            'value': split_val,\n",
        "            'mean_value': None,\n",
        "            'left': build_tree(X_l, y_l, feature_names, depth + 1, max_depth, min_samples_leaf),\n",
        "            'right': build_tree(X_r, y_r, feature_names, depth + 1, max_depth, min_samples_leaf)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 497,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_rf(B, n_features, X, y, feature_names):\n",
        "    \"\"\"\n",
        "    Build the decision tree according to the training data.\n",
        "    Args:\n",
        "        B: number of decision trees.\n",
        "        X: (pd.Dataframe) training features, of shape (N, p). Each X[i] is a training sample.\n",
        "        y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n",
        "        an integer in the range 0 <= y[i] <= C. Here C = 1.\n",
        "    \"\"\"\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        X = X.to_numpy()\n",
        "        y = y.to_numpy()\n",
        "        \n",
        "    N = X.shape[0]\n",
        "    training_indices = np.arange(N)\n",
        "    trees = []\n",
        "\n",
        "    for _ in range(B):\n",
        "        # Sample the training_indices (with replacement)\n",
        "        sample = np.random.choice(training_indices, N, replace=True) \n",
        "\n",
        "        # Ensure the size of the random sample is the same size of training sample\n",
        "        assert len(sample) == len(training_indices)\n",
        "\n",
        "        X_sample = X[sample, :]\n",
        "        y_sample = y[sample]\n",
        "        tree = build_tree_rf(n_features, X_sample, y_sample,\n",
        "                            feature_names, depth=1)\n",
        "        trees.append(tree)\n",
        "\n",
        "    return trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 498,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_rf(rf, X):\n",
        "    \"\"\"\n",
        "    Predict classification results for X.\n",
        "    Args:\n",
        "        rf: A trained random forest through train_rf function.\n",
        "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
        "    Returns:\n",
        "        (np.array): predicted testing sample labels, of shape (N,).\n",
        "    \"\"\"\n",
        "\n",
        "    if len(X.shape) == 1:\n",
        "        # if we have one sample\n",
        "        return np.mean([find(tree, X) for tree in rf])\n",
        "    else:\n",
        "        # if we have multiple samples\n",
        "        return np.array([np.mean([find(tree, x) for tree in rf]) for x in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 499,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rf_results(rf, X, y):\n",
        "    \"\"\"\n",
        "    Return the R^2 and MSE score of the random forest on the data.\n",
        "    Args:\n",
        "        rf: (list) the random forest.\n",
        "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
        "        y: (pd.Series) vector of testing labels, of shape (N,).\n",
        "    Returns:\n",
        "        (float): R^2 score of the rf on the data.\n",
        "        (float): MSE score of the rf on the data.\n",
        "    \"\"\"\n",
        "    y_pred = predict_rf(rf, X)\n",
        "    return rsq(y, y_pred), mse(y, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 500,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_features = int(X_train.shape[1]/3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 501,
      "metadata": {},
      "outputs": [],
      "source": [
        "B = 30 # this will change later after optimisation\n",
        "# fit the random forest with training data\n",
        "rf = train_rf(B, n_features, X_train, y_train, X_train.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 502,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'r'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/Users/calam/Desktop/Year 3/Data Science/02045099_Coursework1.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(rf_results(rf, X_train, y_train))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(rf_results(rf, X_test, y_test))\n",
            "\u001b[1;32m/Users/calam/Desktop/Year 3/Data Science/02045099_Coursework1.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrf_results\u001b[39m(rf, X, y):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    Return the R^2 and MSE score of the random forest on the data.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m        (float): MSE score of the rf on the data.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m predict_rf(rf, X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m rsq(y, y_pred), mse(y, y_pred)\n",
            "\u001b[1;32m/Users/calam/Desktop/Year 3/Data Science/02045099_Coursework1.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean([find(tree, X) \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m rf])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# if we have multiple samples\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39;49mmean([find(tree, x) \u001b[39mfor\u001b[39;49;00m tree \u001b[39min\u001b[39;49;00m rf]) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m X])\n",
            "\u001b[1;32m/Users/calam/Desktop/Year 3/Data Science/02045099_Coursework1.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean([find(tree, X) \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m rf])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# if we have multiple samples\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mmean([find(tree, x) \u001b[39mfor\u001b[39;49;00m tree \u001b[39min\u001b[39;49;00m rf]) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X])\n",
            "\u001b[1;32m/Users/calam/Desktop/Year 3/Data Science/02045099_Coursework1.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean([find(tree, X) \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m rf])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# if we have multiple samples\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mmean([find(tree, x) \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m rf]) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X])\n",
            "\u001b[1;32m/Users/calam/Desktop/Year 3/Data Science/02045099_Coursework1.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m tree[\u001b[39m'\u001b[39m\u001b[39mmean_value\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tree[\u001b[39m'\u001b[39m\u001b[39mmean_value\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mfloat\u001b[39m(x[tree[\u001b[39m'\u001b[39m\u001b[39mfeature_index\u001b[39m\u001b[39m'\u001b[39m]]) \u001b[39m<\u001b[39m \u001b[39mfloat\u001b[39m(tree[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m]): \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# go to left branch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m find(tree[\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m], x)  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y110sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# go to right branch\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'r'"
          ]
        }
      ],
      "source": [
        "print(rf_results(rf, X_train, y_train))\n",
        "print(rf_results(rf, X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 453,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fold_indices(N, n_folds):\n",
        "    fold_size = N // n_folds\n",
        "    shuffle_index = np.random.permutation(np.arange(N))\n",
        "    folds = []\n",
        "    for i in range(n_folds):\n",
        "        folds.append(shuffle_index[int(i * fold_size):int((i + 1) * fold_size)])\n",
        "    return folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 454,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_validation_score(X_train, y_train, n_folds, B):\n",
        "  scores = []\n",
        "  folds = fold_indices(X_train.shape[0], n_folds)\n",
        "  feature_names = X_train.columns.tolist()\n",
        "  X_train = X_train.to_numpy()\n",
        "  y_train = y_train.to_numpy()\n",
        "\n",
        "  for i in range(len(folds)):\n",
        "    val_indexes = folds[i]\n",
        "    train_indexes = list(set(range(y_train.shape[0])) - set(val_indexes))\n",
        "    \n",
        "    X_train_i = X_train[train_indexes, :]\n",
        "    y_train_i = y_train[train_indexes]\n",
        "\n",
        "\n",
        "    X_val_i = X_train[val_indexes, :] \n",
        "    y_val_i = y_train[val_indexes] \n",
        "\n",
        "    rf = train_rf(B, n_features, X_train_i, y_train_i, feature_names)\n",
        "    scores.append(rf_results(rf, X_val_i, y_val_i)[1])\n",
        "\n",
        "  # Return the average score\n",
        "  return np.mean(scores) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 455,
      "metadata": {},
      "outputs": [],
      "source": [
        "def choose_best_B(X_train, y_train, folds, B_range):\n",
        "  B_scores = np.zeros((len(B_range),))\n",
        "  \n",
        "  for i, B in enumerate(B_range):\n",
        "    B_scores[i] = cross_validation_score(X_train, y_train, folds, B)\n",
        "    print(f'CV_ACC@B={B}: {B_scores[i]:.3f}')\n",
        "\n",
        "  best_B_index = np.argmax(B_scores) \n",
        "  return B_range[best_B_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 456,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/Users/calam/Desktop/Year 3/Data Science/02045099_Coursework1.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m b_hat \u001b[39m=\u001b[39m choose_best_B(X_train, y_train, \u001b[39m5\u001b[39;49m, np\u001b[39m.\u001b[39;49marange(\u001b[39m1\u001b[39;49m, \u001b[39m31\u001b[39;49m))\n",
            "\u001b[1;32m/Users/calam/Desktop/Year 3/Data Science/02045099_Coursework1.ipynb Cell 34\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m B_scores \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(B_range),))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, B \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(B_range):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   B_scores[i] \u001b[39m=\u001b[39m cross_validation_score(X_train, y_train, folds, B)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCV_ACC@B=\u001b[39m\u001b[39m{\u001b[39;00mB\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mB_scores[i]\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m best_B_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(B_scores) \n",
            "\u001b[1;32m/Users/calam/Desktop/Year 3/Data Science/02045099_Coursework1.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m   y_val_i \u001b[39m=\u001b[39m y_train[val_indexes] \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m   rf \u001b[39m=\u001b[39m train_rf(B, n_features, X_train_i, y_train_i, feature_names)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m   scores\u001b[39m.\u001b[39mappend(rf_results(rf, X_val_i, y_val_i)[\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Return the average score\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(scores)\n",
            "\u001b[1;32m/Users/calam/Desktop/Year 3/Data Science/02045099_Coursework1.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrf_results\u001b[39m(rf, X, y):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    Return the R^2 and MSE score of the random forest on the data.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m        (float): MSE score of the rf on the data.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m predict_rf(rf, X\u001b[39m.\u001b[39;49mto_numpy())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/calam/Desktop/Year%203/Data%20Science/02045099_Coursework1.ipynb#Y114sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m rsq(y, y_pred), mse(y, y_pred)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
          ]
        }
      ],
      "source": [
        "b_hat = choose_best_B(X_train, y_train, 5, np.arange(1, 31))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEwNJHHTMgNG"
      },
      "source": [
        "<a name=\"task-113\"></a>\n",
        "\n",
        "### (1.1.3) [(index)](#index-task-113)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqN02H_YPwr0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In8XkuW_MgVV"
      },
      "source": [
        "<a name=\"task-12\"></a>\n",
        "\n",
        "## (1.2) Multi-layer Perceptron [(index)](#index-task-12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1aYyFt7MgeQ"
      },
      "source": [
        "<a name=\"task-121\"></a>\n",
        "\n",
        "### (1.2.1) [(index)](#index-task-121)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j1--gzeMgk9"
      },
      "source": [
        "<a name=\"task-122\"></a>\n",
        "\n",
        "### (1.2.2) [(index)](#index-task-122)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MccP5-sMgqU"
      },
      "source": [
        "<a name=\"task-123\"></a>\n",
        "\n",
        "### (1.2.3) [(index)](#index-task-123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzPd5qZZPzpM"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ1n70CNMguM"
      },
      "source": [
        "<a name=\"task-2\"></a>\n",
        "\n",
        "# (2) Task 2: Classification [(index)](#index-task-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snlMZuPqMgxd"
      },
      "source": [
        "<a name=\"task-21\"></a>\n",
        "\n",
        "## (2.1) k-Nearest Neighbours [(index)](#index-task-21)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVwRifs5Mg0s"
      },
      "source": [
        "<a name=\"task-211\"></a>\n",
        "\n",
        "### (2.1.1) [(index)](#index-task-211)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrF3U23EMg3k"
      },
      "source": [
        "<a name=\"task-212\"></a>\n",
        "\n",
        "### (2.1.2) [(index)](#index-task-212)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIUs15tlMg9c"
      },
      "source": [
        "<a name=\"task-213\"></a>\n",
        "\n",
        "### (2.1.3) [(index)](#index-task-213)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ3fJoMCPpgE"
      },
      "source": [
        "<a name=\"task-214\"></a>\n",
        "\n",
        "### (2.1.4) [(index)](#index-task-214)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6Zz2bcMP6Wk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0K6bbpHPUYE"
      },
      "source": [
        "<a name=\"task-22\"></a>\n",
        "\n",
        "## (2.2) Logistic regression vs kernel logistic regression [(index)](#index-task-22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pVdpUHZP-oE"
      },
      "source": [
        "<a name=\"task-221\"></a>\n",
        "\n",
        "### (2.2.1) [(index)](#index-task-221)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_MkiUD4QB0k"
      },
      "source": [
        "<a name=\"task-222\"></a>\n",
        "\n",
        "### (2.2.2) [(index)](#index-task-222)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzUYT08xQDV9"
      },
      "source": [
        "<a name=\"task-223\"></a>\n",
        "\n",
        "### (2.2.3) [(index)](#index-task-223)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrhLFEMnPTy9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYSz-NFcEoFA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
