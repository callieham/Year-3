{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb_IWF6UQRGk"
      },
      "source": [
        "# Coursework 1 - Supervised learning\n",
        "\n",
        "**Replace CID in the file name with your CID**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTeLZnrzE0Wy"
      },
      "source": [
        "# Outline\n",
        "\n",
        "\n",
        "- [Task 1](#task-1): Regression <a name=\"index-task-1\"></a>\n",
        "  - [(1.1)](#task-11) Random Forest <a name=\"index-task-11\"></a>\n",
        "    - [(1.1.1)](#task-111) <a name=\"index-task-111\"></a>\n",
        "    - [(1.1.2)](#task-112) <a name=\"index-task-112\"></a>\n",
        "    - [(1.1.3)](#task-113) <a name=\"index-task-113\"></a>\n",
        "  - [(1.2)](#task-12) Multi-layer Perceptron <a name=\"index-task-12\"></a>\n",
        "    - [(1.2.1)](#task-121) <a name=\"index-task-121\"></a>\n",
        "    - [(1.2.2)](#task-122) <a name=\"index-task-122\"></a>\n",
        "    - [(1.2.3)](#task-123) <a name=\"index-task-123\"></a>\n",
        "- [Task 2](#task-2): Classification <a name=\"index-task-2\"></a>\n",
        "  - [(2.1)](#task-21) k-Nearest Neighbours <a name=\"index-task-21\"></a>\n",
        "    - [(2.1.1)](#task-211)  <a name=\"index-task-211\"></a>\n",
        "    - [(2.1.2)](#task-212) <a name=\"index-task-212\"></a>\n",
        "    - [(2.1.3)](#task-213) <a name=\"index-task-213\"></a>\n",
        "    - [(2.1.4)](#task-214) <a name=\"index-task-214\"></a>\n",
        "  - [(2.2)](#task-22) Logistic regression vs kernel logistic regression <a name=\"index-task-22\"></a>\n",
        "    - [(2.2.1)](#task-221) <a name=\"index-task-221\"></a>\n",
        "    - [(2.2.2)](#task-222) <a name=\"index-task-222\"></a>\n",
        "    - [(2.2.3)](#task-223) <a name=\"index-task-223\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4LmL6R9N1B-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_QE32lOMff_"
      },
      "source": [
        "<a name=\"task-1\"></a>\n",
        "\n",
        "# (1) Task 1: Regression [(index)](#index-task-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLl66QsJMfzc"
      },
      "source": [
        "<a name=\"task-11\"></a>\n",
        "\n",
        "## (1.1) Random Forest [(index)](#index-task-11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load dataset\n",
        "data = pd.read_csv('nanoelectrodes_capacitance_samples.csv')\n",
        "\n",
        "X_train = data.drop(\"Capacitance ($\\mu F / cm^2$)\", axis = 'columns')\n",
        "y_train = data[\"Capacitance ($\\mu F / cm^2$)\"]\n",
        "\n",
        "test_data = pd.read_csv('nanoelectrodes_capacitance_test.csv')\n",
        "X_test = test_data.drop(\"Capacitance ($\\mu F / cm^2$)\", axis = 'columns')\n",
        "y_test = test_data[\"Capacitance ($\\mu F / cm^2$)\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T47skSuUMf8M"
      },
      "source": [
        "<a name=\"task-111\"></a>\n",
        "\n",
        "### (1.1.1) [(index)](#index-task-111)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss(y, y_pred):\n",
        "    return ((y - y_pred) ** 2).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mse(y, y_pred):\n",
        "    return ((y - y_pred) ** 2).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rsq(y, y_pred):\n",
        "    return 1 - (np.linalg.norm(y - y_pred) ** 2) / (np.linalg.norm(y - y.mean()) ** 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_samples(X, y, column, value):\n",
        "    \"\"\"\n",
        "    Return the split of data whose column-th feature:\n",
        "        less than value, in case `column` is not categorical (i.e. numerical)\n",
        "\n",
        "    Arguments:\n",
        "        X: training features, of shape (N, p).\n",
        "        y: vector of training labels, of shape (N,).\n",
        "        column: the column of the feature for splitting.\n",
        "        value: splitting threshold  the samples\n",
        "    Returns:\n",
        "        tuple(np.array, np.array): tuple of the left split data (X_l, y_l).\n",
        "        tuple(np.array, np.array): tuple of the right split data (X_l, y_l)\n",
        "    \"\"\"\n",
        "\n",
        "    left_mask = (X[:, column] < value)\n",
        "\n",
        "    # Using the binary masks `left_mask`, we split X and y.\n",
        "    X_l, y_l = X[left_mask], y[left_mask] \n",
        "    X_r, y_r = X[~left_mask], y[~left_mask] \n",
        "\n",
        "    return (X_l, y_l), (X_r, y_r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_split_value(X, y, column):\n",
        "    \"\"\"\n",
        "    Calculate the mse based on `column` with the split that minimizes the loss.\n",
        "    Arguments:\n",
        "        X: training features, of shape (N, p).\n",
        "        y: vector of training labels, of shape (N,).\n",
        "        column: the column of the feature for calculating. 0 <= column < D\n",
        "    Returns:\n",
        "        (float, float): the resulted mse and the corresponding value used in splitting.\n",
        "    \"\"\"\n",
        "\n",
        "    unique_vals = np.unique(X[:, column])\n",
        "\n",
        "    assert len(unique_vals) > 1, f\"There must be more than one distinct feature value. Given: {unique_vals}.\"\n",
        "\n",
        "    loss_val, threshold = np.inf, None\n",
        "\n",
        "    # split the values of i-th feature and calculate the cost\n",
        "    for value in unique_vals:\n",
        "        (X_l, y_l), (X_r, y_r) = split_samples(X, y, column, value) \n",
        "\n",
        "        # if one of the two sides is empty, skip this split.\n",
        "        if len(y_l) == 0 or len(y_r) == 0:\n",
        "            continue\n",
        "\n",
        "        new_loss = loss(y_l, y_l.mean()) + loss(y_r, y_r.mean()) \n",
        "        if new_loss < loss_val:\n",
        "              loss_val, threshold = new_loss, value\n",
        "\n",
        "    return loss_val, threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_split(X, y):\n",
        "    \"\"\"\n",
        "    Choose the best feature to split according to criterion.\n",
        "    Args:\n",
        "        X: training features, of shape (N, p).\n",
        "        y: vector of training labels, of shape (N,).\n",
        "    Returns:\n",
        "        (int, float): the best feature index and value used in splitting.\n",
        "        If the feature index is None, then no valid split for the current Node.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize `split_column` to None, so if None returned this means there is no valid split at the current node.\n",
        "    min_loss = np.inf\n",
        "    split_column = None\n",
        "    split_val = np.nan\n",
        "    m, n = X.shape\n",
        "\n",
        "    for col in range(n):\n",
        "        # skip column if samples are not seperable by that column.\n",
        "        if len(np.unique(X[:, col])) < 2:\n",
        "            continue\n",
        "        loss, current_split_val = loss_split_value(X, y, col)  \n",
        "\n",
        "        # To scan for the best split corresponding the minimum mse_index\n",
        "        if loss < min_loss: \n",
        "            # Keep track with:\n",
        "\n",
        "            # 1. the current minimum mse value,\n",
        "            min_loss = loss\n",
        "\n",
        "            # 2. corresponding column,\n",
        "            split_column = col\n",
        "\n",
        "            # 3. corresponding split threshold.\n",
        "            split_val = current_split_val \n",
        "\n",
        "    return split_column, split_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, 2.5)"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_split(X_train.to_numpy(), y_train.to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tree(X, y, feature_names, depth,  max_depth=10, min_samples_leaf=10):\n",
        "    \"\"\"Build the decision tree according to the data.\n",
        "    Args:\n",
        "        X: (np.array) training features, of shape (N, p).\n",
        "        y: (np.array) vector of training labels, of shape (N,).\n",
        "        feature_names (list): record the name of features in X in the original dataset.\n",
        "        depth (int): current depth for this node.\n",
        "    Returns:\n",
        "        (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n",
        "          1. 'feature_name': The column name of the split.\n",
        "          2. 'feature_index': The column index of the split.\n",
        "          3. 'value': The value used for the split.\n",
        "          4. 'mean_value': For leaf nodes, this stores the dominant label. Otherwise, it is None.\n",
        "          5. 'left': The left sub-tree with the same structure.\n",
        "          6. 'right' The right sub-tree with the same structure.\n",
        "    \"\"\"\n",
        "    # include a clause for the cases where (i) no feature, (ii) depth exceed, or (iii) X is too small\n",
        "    if len(np.unique(y))==1 or depth>=max_depth or len(X)<=min_samples_leaf:\n",
        "        return {'mean_value': np.mean(y)}\n",
        "\n",
        "    split_index, split_val = loss_split(X, y)\n",
        "\n",
        "    # If no valid split at this node, use mean.\n",
        "    if split_index is None:\n",
        "        return {'mean_value': np.mean(y)}\n",
        "\n",
        "    # Split samples (X, y) given column and split-value.\n",
        "    (X_l, y_l), (X_r, y_r) = split_samples(X, y, split_index, split_val) \n",
        "    return {\n",
        "        'feature_name': feature_names[split_index],\n",
        "        'feature_index': split_index,\n",
        "        'value': split_val,\n",
        "        'mean_value': None,\n",
        "        'left': build_tree(X_l, y_l, feature_names, depth + 1, max_depth, min_samples_leaf),\n",
        "        'right': build_tree(X_r, y_r, feature_names, depth + 1, max_depth, min_samples_leaf)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(X, y):\n",
        "    \"\"\"\n",
        "    Build the decision tree according to the training data.\n",
        "    Args:\n",
        "        X: (pd.Dataframe) training features, of shape (N, p). Each X[i] is a training sample.\n",
        "        y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n",
        "        an integer in the range 0 <= y[i] <= C. Here C = 1.\n",
        "    \"\"\"\n",
        "    feature_names = X.columns.tolist()\n",
        "    X = X.to_numpy()\n",
        "    y = y.to_numpy()\n",
        "    return build_tree(X, y, feature_names, depth=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree = train(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find(tree, x):\n",
        "    \"\"\"\n",
        "    Find the branch of a single sample with the fitted decision tree.\n",
        "    Args:\n",
        "        x: ((pd.Dataframe) a single sample features, of shape (D,).\n",
        "    Returns:\n",
        "        (int): predicted testing sample label.\n",
        "    \"\"\"\n",
        "    \n",
        "    if tree['mean_value'] is not None:\n",
        "        return tree['mean_value']\n",
        "\n",
        "    if x[tree['feature_index']] < tree['value']: \n",
        "        # go to left branch\n",
        "        return find(tree['left'], x)  \n",
        "    else:\n",
        "        # go to right branch\n",
        "        return find(tree['right'], x)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(tree, X):\n",
        "    \"\"\"\n",
        "    Predict regression results for X.\n",
        "    Args:\n",
        "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
        "    Returns:\n",
        "        (np.array): predicted testing sample labels, of shape (N,).\n",
        "    \"\"\"\n",
        "    if len(X.shape) == 1:\n",
        "        return find(tree, X)\n",
        "    else:\n",
        "        return np.array([find(tree, x) for x in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tree_results(tree, X, y):\n",
        "    \"\"\"\n",
        "    Return the R^2 and MSE score of the tree on the data.\n",
        "    Args:\n",
        "        tree: (dict) the decision tree.\n",
        "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
        "        y: (pd.Series) vector of testing labels, of shape (N,).\n",
        "    Returns:\n",
        "        (float): R^2 score of the tree on the data.\n",
        "        (float): MSE score of the tree on the data.\n",
        "    \"\"\"\n",
        "    y_pred = predict(tree, X.to_numpy())\n",
        "    return mse(y, y_pred), rsq(y, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1654.4051506951573, 0.7503640060081045)\n",
            "(3369.5671069811706, 0.4896216113769649)\n"
          ]
        }
      ],
      "source": [
        "tree_train_results = tree_results(tree, X_train, y_train)\n",
        "tree_test_results = tree_results(tree, X_test, y_test)\n",
        "print(tree_train_results)\n",
        "print(tree_test_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx4c9z5pMgDR"
      },
      "source": [
        "<a name=\"task-112\"></a>\n",
        "\n",
        "### (1.1.2) [(index)](#index-task-112)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_split_rf(n_features, X, y):\n",
        "    \"\"\"\n",
        "    Choose the best feature to split according to criterion.\n",
        "    Args:\n",
        "        n_features: number of sampled features.\n",
        "        X: training features, of shape (N, p).\n",
        "        y: vector of training labels, of shape (N,).\n",
        "    Returns:\n",
        "        (float, int, float): the minimized loss value.\n",
        "    \"\"\"\n",
        "    # The added sampling step.\n",
        "    columns = np.random.choice(list(range(12)), n_features, replace=False)\n",
        "\n",
        "\n",
        "    min_loss_val, split_column, split_val = np.inf, 0, 0\n",
        "\n",
        "    # Only scan through the sampled columns in `columns_dict`.\n",
        "    for column in columns:\n",
        "        # skip column if samples are not seperable by that column.\n",
        "        if len(np.unique(X[:, column])) < 2:\n",
        "            continue\n",
        "\n",
        "        # search for the best splitting value for the given column.\n",
        "        loss_val, val = loss_split_value(X, y, column)\n",
        "        if loss_val < min_loss_val:\n",
        "            min_loss_val, split_column, split_val = loss_val, column, val\n",
        "\n",
        "    return min_loss_val, split_column, split_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tree_rf(n_features, X, y, feature_names, depth,  max_depth=10, min_samples_leaf=10):\n",
        "    \"\"\"Build the decision tree according to the data.\n",
        "    Args:\n",
        "        X: (np.array) training features, of shape (N, p).\n",
        "        y: (np.array) vector of training labels, of shape (N,).\n",
        "        feature_names (list): record the name of features in X in the original dataset.\n",
        "        depth (int): current depth for this node.\n",
        "    Returns:\n",
        "        (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n",
        "          1. 'feature_name': The column name of the split.\n",
        "          2. 'feature_index': The column index of the split.\n",
        "          3. 'value': The value used for the split.\n",
        "          4. 'categorical': indicator for categorical/numerical variables.\n",
        "          5. 'majority_label': For leaf nodes, this stores the dominant label. Otherwise, it is None.\n",
        "          6. 'left': The left sub-tree with the same structure.\n",
        "          7. 'right' The right sub-tree with the same structure.\n",
        "    \"\"\"\n",
        "    # include a clause for the cases where (i) all lables are the same, (ii) depth exceed (iii) X is too small\n",
        "    if len(np.unique(y)) == 1 or depth>=max_depth or len(X)<=min_samples_leaf:\n",
        "        return {'mean_value': np.mean(y)}\n",
        "\n",
        "    else:\n",
        "        loss, split_column, split_val = loss_split_rf(n_features, X, y)\n",
        "\n",
        "        # If loss is infinity, it means that samples are not seperable by the sampled features.\n",
        "        if loss == np.inf:\n",
        "            return {'mean_value': np.mean(y)}\n",
        "        (X_l, y_l), (X_r, y_r) = split_samples(X, y, split_column, split_val)\n",
        "        return {\n",
        "            'feature_name': feature_names[split_column],\n",
        "            'feature_index': split_column,\n",
        "            'value': split_val,\n",
        "            'mean_value': None,\n",
        "            'left': build_tree_rf(n_features, X_l, y_l, feature_names, depth + 1, max_depth, min_samples_leaf),\n",
        "            'right': build_tree_rf(n_features, X_r, y_r, feature_names, depth + 1, max_depth, min_samples_leaf)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_rf(B, n_features, X, y):\n",
        "    \"\"\"\n",
        "    Build the decision tree according to the training data.\n",
        "    Args:\n",
        "        B: number of decision trees.\n",
        "        X: (pd.Dataframe) training features, of shape (N, p). Each X[i] is a training sample.\n",
        "        y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n",
        "        an integer in the range 0 <= y[i] <= C. Here C = 1.\n",
        "    \"\"\"\n",
        "    feature_names = X.columns.tolist()\n",
        "    X = X.to_numpy()\n",
        "    y = y.to_numpy()\n",
        "    N = X.shape[0]\n",
        "    training_indices = np.arange(N)\n",
        "    trees = []\n",
        "\n",
        "    for _ in range(B):\n",
        "        # Sample the training_indices (with replacement)\n",
        "        sample = np.random.choice(training_indices, N, replace=True) \n",
        "\n",
        "        X_sample = X[sample, :]\n",
        "        y_sample = y[sample]\n",
        "        tree = build_tree_rf(n_features, X_sample, y_sample, feature_names, depth=1)\n",
        "        trees.append(tree)\n",
        "\n",
        "    return trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_rf(rf, X):\n",
        "    \"\"\"\n",
        "    Predict classification results for X.\n",
        "    Args:\n",
        "        rf: A trained random forest through train_rf function.\n",
        "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
        "    Returns:\n",
        "        (np.array): predicted testing sample labels, of shape (N,).\n",
        "    \"\"\"\n",
        "\n",
        "    if len(X.shape) == 1:\n",
        "        # if we have one sample\n",
        "        return np.mean([find(tree, X) for tree in rf])\n",
        "    else:\n",
        "        # if we have multiple samples\n",
        "        return np.array([np.mean([find(tree, x) for tree in rf]) for x in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rf_results(rf, X, y):\n",
        "    \"\"\"\n",
        "    Return the R^2 and MSE score of the random forest on the data.\n",
        "    Args:\n",
        "        rf: (list) the random forest.\n",
        "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
        "        y: (pd.Series) vector of testing labels, of shape (N,).\n",
        "    Returns:\n",
        "        (float): R^2 score of the rf on the data.\n",
        "        (float): MSE score of the rf on the data.\n",
        "    \"\"\"\n",
        "    y_pred = predict_rf(rf, X)\n",
        "    return mse(y, y_pred), rsq(y, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_features = int(X_train.shape[1]/3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "B = 30 # this will change later after optimisation\n",
        "# fit the random forest with training data\n",
        "rf = train_rf(B, n_features, X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1273.5896658764423, 0.8078258992089812)\n",
            "(3157.544171218846, 0.5217360999358376)\n"
          ]
        }
      ],
      "source": [
        "print(rf_results(rf, X_train.to_numpy(), y_train.to_numpy()))\n",
        "print(rf_results(rf, X_test.to_numpy(), y_test.to_numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "def n_fold_cross_validation(X, y, B, n_features, n_folds=5):\n",
        "\n",
        "  N = X.shape[0]\n",
        "  fold_size = N // n_folds\n",
        "  scores = []\n",
        "\n",
        "  for i in range(n_folds):\n",
        "    test_indices = list(range(i * fold_size, (i + 1) * fold_size))\n",
        "    train_indices = list(set(range(N)) - set(test_indices))\n",
        "    X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
        "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
        "  \n",
        "  rf = train_rf(B, n_features, X_train, y_train)\n",
        "\n",
        "  scores.append(mse(predict_rf(rf, X_test.to_numpy()), y_test.to_numpy()))\n",
        "\n",
        "  # Return the average score\n",
        "  return np.mean(scores) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "def choose_best_B(X_train, y_train, n_folds, B_max):\n",
        "  wide_B_range = [1] + list(np.arange(10, B_max, 10))\n",
        "  B_scores_wide = np.zeros((len(wide_B_range)))\n",
        "  \n",
        "  for i, B in enumerate(wide_B_range):\n",
        "    B_scores_wide[i] = n_fold_cross_validation(X_train, y_train, B, 12, n_folds)\n",
        "    print(f'Loss@B={B}: {B_scores_wide[i]:.3f}')\n",
        "\n",
        "  best_B_index_wide = np.argmin(B_scores_wide)\n",
        "\n",
        "  B_range = np.arange(wide_B_range[best_B_index_wide] - 10, wide_B_range[best_B_index_wide] + 10)\n",
        "  B_scores = np.zeros((len(B_range)))\n",
        "\n",
        "  for i, B in enumerate(B_range):\n",
        "    B_scores[i] = n_fold_cross_validation(X_train, y_train, B, 12, n_folds)\n",
        "    print(f'Loss@B={B}: {B_scores[i]:.3f}')\n",
        "  \n",
        "  best_B_index = np.argmin(B_scores)\n",
        "\n",
        "  return B_range[best_B_index], B_scores, B_scores_wide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss@B=1: 10326.407\n",
            "Loss@B=10: 3737.147\n",
            "Loss@B=20: 4043.816\n",
            "Loss@B=30: 3904.991\n",
            "Loss@B=40: 4006.782\n",
            "Loss@B=50: 4060.517\n",
            "Loss@B=60: 3757.283\n",
            "Loss@B=70: 3818.586\n",
            "Loss@B=80: 3422.072\n",
            "Loss@B=90: 3628.504\n",
            "Loss@B=100: 3686.233\n",
            "Loss@B=70: 3995.910\n",
            "Loss@B=71: 3745.538\n",
            "Loss@B=72: 3760.494\n",
            "Loss@B=73: 3922.754\n",
            "Loss@B=74: 3833.622\n",
            "Loss@B=75: 3972.103\n",
            "Loss@B=76: 3614.372\n",
            "Loss@B=77: 3659.930\n",
            "Loss@B=78: 3933.700\n",
            "Loss@B=79: 3855.504\n",
            "Loss@B=80: 3791.956\n",
            "Loss@B=81: 3793.070\n",
            "Loss@B=82: 3775.853\n",
            "Loss@B=83: 3846.360\n",
            "Loss@B=84: 3669.030\n",
            "Loss@B=85: 3687.087\n",
            "Loss@B=86: 3881.294\n",
            "Loss@B=87: 3874.290\n",
            "Loss@B=88: 3797.071\n",
            "Loss@B=89: 3617.469\n"
          ]
        }
      ],
      "source": [
        "b_hat, scores_reduced, scores_range = choose_best_B(X_train, y_train, 5, 101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'scores_range' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mscores_range\u001b[49m)), scores_range)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'scores_range' is not defined"
          ]
        }
      ],
      "source": [
        "plt.plot(range(len(scores_range)), scores_range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "b_hat = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1322.6158147304207, 0.8004282606102285)\n",
            "(3323.5824440658394, 0.496586772602416)\n"
          ]
        }
      ],
      "source": [
        "rf_hat = train_rf(b_hat, n_features, X_train, y_train)\n",
        "rf_train_results = rf_results(rf_hat, X_train.to_numpy(), y_train.to_numpy())\n",
        "rf_test_results = rf_results(rf_hat, X_test.to_numpy(), y_test.to_numpy())\n",
        "print(rf_train_results)\n",
        "print(rf_test_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "def importance(rf, X):\n",
        "    \"\"\"\n",
        "    Calculate the importance of each feature in the random forest.\n",
        "    Args:\n",
        "        rf: A trained random forest through train_rf function.\n",
        "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
        "    Returns:\n",
        "        (np.array): the importance of each feature, of shape (p,).\n",
        "    \"\"\"\n",
        "    return np.array([np.mean([tree['feature_index'] for tree in rf]) for x in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_split_bag(X, y):\n",
        "    \"\"\"\n",
        "    Choose the best feature to split according to criterion.\n",
        "    Args:\n",
        "        X: training features, of shape (N, p).\n",
        "        y: vector of training labels, of shape (N,).\n",
        "    Returns:\n",
        "        (float, int, float): the minimized loss value.\n",
        "    \"\"\"\n",
        "    min_loss_val, split_column, split_val = np.inf, 0, 0\n",
        "\n",
        "    for column in range(12):\n",
        "        # skip column if samples are not seperable by that column.\n",
        "        if len(np.unique(X[:, column])) < 2:\n",
        "            continue\n",
        "\n",
        "        # search for the best splitting value for the given column.\n",
        "        loss_val, val = loss_split_value(X, y, column)\n",
        "        if loss_val < min_loss_val:\n",
        "            min_loss_val, split_column, split_val = loss_val, column, val\n",
        "\n",
        "    return min_loss_val, split_column, split_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tree_bag(X, y, feature_names, depth,  max_depth=10, min_samples_leaf=10):\n",
        "    \"\"\"Build the decision tree according to the data.\n",
        "    Args:\n",
        "        X: (np.array) training features, of shape (N, p).\n",
        "        y: (np.array) vector of training labels, of shape (N,).\n",
        "        feature_names (list): record the name of features in X in the original dataset.\n",
        "        depth (int): current depth for this node.\n",
        "    Returns:\n",
        "        (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n",
        "          1. 'feature_name': The column name of the split.\n",
        "          2. 'feature_index': The column index of the split.\n",
        "          3. 'value': The value used for the split.\n",
        "          4. 'categorical': indicator for categorical/numerical variables.\n",
        "          5. 'majority_label': For leaf nodes, this stores the dominant label. Otherwise, it is None.\n",
        "          6. 'left': The left sub-tree with the same structure.\n",
        "          7. 'right' The right sub-tree with the same structure.\n",
        "    \"\"\"\n",
        "    # include a clause for the cases where (i) all lables are the same, (ii) depth exceed (iii) X is too small\n",
        "    if len(np.unique(y)) == 1 or depth>=max_depth or len(X)<=min_samples_leaf:\n",
        "        return {'mean_value': np.mean(y)}\n",
        "\n",
        "    else:\n",
        "        loss, split_column, split_val = loss_split_bag(X, y)\n",
        "\n",
        "        # If loss is infinity, it means that samples are not seperable by the sampled features.\n",
        "        if loss == np.inf:\n",
        "            return {'mean_value': np.mean(y)}\n",
        "        (X_l, y_l), (X_r, y_r) = split_samples(X, y, split_column, split_val)\n",
        "        return {\n",
        "            'feature_name': feature_names[split_column],\n",
        "            'feature_index': split_column,\n",
        "            'value': split_val,\n",
        "            'mean_value': None,\n",
        "            'left': build_tree_bag(X_l, y_l, feature_names, depth + 1, max_depth, min_samples_leaf),\n",
        "            'right': build_tree_bag(X_r, y_r, feature_names, depth + 1, max_depth, min_samples_leaf)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_bag(B, X, y):\n",
        "    \"\"\"\n",
        "    Build the decision tree according to the training data.\n",
        "    Args:\n",
        "        B: number of decision trees.\n",
        "        X: (pd.Dataframe) training features, of shape (N, p). Each X[i] is a training sample.\n",
        "        y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n",
        "        an integer in the range 0 <= y[i] <= C. Here C = 1.\n",
        "    \"\"\"\n",
        "    feature_names = X.columns.tolist()\n",
        "    X = X.to_numpy()\n",
        "    y = y.to_numpy()\n",
        "    N = X.shape[0]\n",
        "    training_indices = np.arange(N)\n",
        "    trees = []\n",
        "\n",
        "    for _ in range(B):\n",
        "        # Sample the training_indices (with replacement)\n",
        "        sample = np.random.choice(training_indices, N, replace=True) \n",
        "\n",
        "        X_sample = X[sample, :]\n",
        "        y_sample = y[sample]\n",
        "        tree = build_tree_bag(X_sample, y_sample, feature_names, depth=1)\n",
        "        trees.append(tree)\n",
        "\n",
        "    return trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_bag(rf, X):\n",
        "    \"\"\"\n",
        "    Predict classification results for X.\n",
        "    Args:\n",
        "        rf: A trained random forest through train_rf function.\n",
        "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
        "    Returns:\n",
        "        (np.array): predicted testing sample labels, of shape (N,).\n",
        "    \"\"\"\n",
        "\n",
        "    if len(X.shape) == 1:\n",
        "        # if we have one sample\n",
        "        return np.mean([find(tree, X) for tree in rf])\n",
        "    else:\n",
        "        # if we have multiple samples\n",
        "        return np.array([np.mean([find(tree, x) for tree in rf]) for x in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bag_results(rf, X, y):\n",
        "    \"\"\"\n",
        "    Return the R^2 and MSE score of the random forest on the data.\n",
        "    Args:\n",
        "        rf: (list) the random forest.\n",
        "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
        "        y: (pd.Series) vector of testing labels, of shape (N,).\n",
        "    Returns:\n",
        "        (float): R^2 score of the rf on the data.\n",
        "        (float): MSE score of the rf on the data.\n",
        "    \"\"\"\n",
        "    y_pred = predict_bag(rf, X)\n",
        "    return mse(y, y_pred), rsq(y, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_bag = train_bag(b_hat, X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1057.2860205815705, 0.840464322435944)\n",
            "(3178.320484612229, 0.5185891730414978)\n"
          ]
        }
      ],
      "source": [
        "bag_train_results = bag_results(rf_bag, X_train.to_numpy(), y_train.to_numpy())\n",
        "bag_test_results = bag_results(rf_bag, X_test.to_numpy(), y_test.to_numpy())\n",
        "print(bag_train_results)\n",
        "print(bag_test_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '$R^2$ for each method')"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJEAAAHFCAYAAABCaWj0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABh7UlEQVR4nO3deVwVVePH8e8FAxS84AouiFsuuKGoSOVSkbg+Wqa5JKSmLVopZWaZuJSUWmlp+rSYZZpmT9mTlorkUkkuKOL+K1Ox9KJmgqKCwvz+6MV9vIEOlHhZPu/Xa145Z87MnMMduKfvnXvGYhiGIQAAAAAAAOA6XJzdAAAAAAAAABR9hEgAAAAAAAAwRYgEAAAAAAAAU4RIAAAAAAAAMEWIBAAAAAAAAFOESAAAAAAAADBFiAQAAAAAAABThEgAAAAAAAAwRYgEAAAAANeQkZGhoUOHqlatWrJarWrXrp3i4+Od3SwAcApCJAAAAAC4hitXrqh27dr6/vvvdfbsWY0ePVo9e/bU+fPnnd00ALjpCJEAAAAA4Bo8PT01ceJE1apVSy4uLurfv7/c3Nx08OBB0323bdum2267TZ6enrJYLEpMTCz8Bt9kkyZNksVi0enTp53dlL/tZvYh51xAcUWIBOAfu3Llip599ln5+/vLxcVFvXv3dnaTbriFCxfKYrFo+/btzm7K3/bQQw/Jy8vrppzLYrFo0qRJN+VcAAD8HWvWrJHFYrEvt9xyi+rXr69JkyYpMzPzmvv99NNPOnPmjOrXr3/d41++fFl9+/bVmTNn9MYbb2jRokUKCAi40d1APm3evFmTJk3S2bNnnd0UoFgjRAKKuJzwwmKx6Pvvv8+13TAM+fv7y2KxqEePHg7bzp8/r+joaDVt2lSenp6qVKmSgoKC9NRTT+n48eP2ejmfiFxrsdls123jggULNGPGDN1///368MMPNWbMmBvTeRTYhQsXNGnSJG3YsMHZTQEAoEjbtWuXJOn111/XokWLNHfuXNWpU0eTJ09WdHR0nvtcvHhRDz74oMaPHy9vb+/rHv/QoUM6evSonnnmGY0YMUIPPvigKlSocMP7gfzZvHmzJk+eTIgE/ENlnN0AAPnj4eGhJUuW6I477nAo37hxo3799Ve5u7s7lF++fFkdOnTQgQMHFBkZqSeeeELnz5/X3r17tWTJEt17772qXr26wz7z5s3L804VHx+f67bt22+/VY0aNfTGG2/8vc7hhrlw4YImT54sSerUqZNzGwMAQBGWlJQkDw8PPfnkk3J1dZX05127AQEBWrZsmWJiYhzq59xZVL9+fU2cONH0+CdPnpRkPo4qqPT0dHl6et7QYwJAfnEnElBMdOvWTcuXL9eVK1ccypcsWaLg4GD5+fk5lK9YsUI7d+7Ue++9p3nz5umRRx7R008/rQULFujYsWNq1apVrnPcf//9evDBB3MtHh4e123byZMnb+gAKTs7W5cuXbphxwMAAPirXbt2qUmTJvYASZLc3NxUvXp1paamOtTNzs7W4MGDZbFY9OGHH5rOafPQQw+pY8eOkqS+ffvKYrE4fLizc+dOde3aVVarVV5eXrr77rv1448/5jpOzt3i+/bt08CBA1WhQoVcHyj+1W+//aahQ4fK19dX7u7uatKkiRYsWOBQ5+jRo3r88cfVsGFDlS1bVpUqVVLfvn115MiRPI83bNgwVa9eXe7u7qpTp44ee+yxXF/5O3v2rB566CH5+PjI29tbQ4YM0YULF67b1qv7+H//93968MEH5e3trSpVqujFF1+UYRg6duyYevXqJavVKj8/P7322msF7vOkSZM0duxYSVKdOnXsd9v/tb/56UN+XztJ+v7779WmTRt5eHioXr16+ve//2368wCKOkIkoJgYMGCAfv/9d8XGxtrLMjMz9dlnn2ngwIG56h86dEiSdPvtt+fa5uHhIavV+o/bdOTIEVksFq1fv1579+61vyHnfJUqPT1dTz/9tPz9/eXu7q6GDRtq5syZMgzD4TgWi0WjRo3S4sWL1aRJE7m7u2v16tXXPfc333yj9u3by9PTU+XLl1f37t21d+9ehzpJSUl66KGHVLduXXl4eMjPz09Dhw7V77//nut4+R0gZWRkKCoqSlWqVJGnp6fuvfdenTp1yvRnlTMfUXJysnr06CEvLy/VqFFDc+fOlSTt3r1bd911lzw9PRUQEKAlS5bkOkbOE2Fyfp7169fXq6++quzsbEl/vh5VqlSRJE2ePNn+evx1bqLffvtNvXv3lpeXl6pUqaJnnnlGWVlZDnXy+9plZGRozJgxqlKlisqXL69//etf+vXXX01/HgAAOFNmZqYOHjyoFi1aOJQfP35c+/btU5s2bRzKH3nkEZ04cULLly9XmTLmX+Z45JFH9Pzzz0uSnnzySS1atEgvvPCCJGnv3r1q3769du3apWeffVYvvviiDh8+rE6dOmnLli15Hq9v3766cOGCpk2bpuHDh1/zvCkpKWrXrp3WrVunUaNGafbs2apfv76GDRumWbNm2ett27ZNmzdvVv/+/fXmm2/q0UcfVVxcnDp16uQQmhw/flxt27bV0qVL9cADD+jNN9/U4MGDtXHjxlzhSr9+/XTu3DnFxMSoX79+Wrhwof3u6Px44IEHlJ2drVdeeUUhISF66aWXNGvWLN1zzz2qUaOGXn31VdWvX1/PPPOMNm3aVKA+33fffRowYIAk2eenWrRokX3clN8+FOS12717tzp37qyTJ09q0qRJGjJkiKKjo/XFF1/k+2cCFEkGgCLtgw8+MCQZ27ZtM2677TZj8ODB9m0rVqwwXFxcjN9++80ICAgwunfvbt+2ZMkSQ5IxZcoUIzs7+7rniI6ONiQZBw8eNE6dOuWw/PHHH9fc7/z588aiRYuMRo0aGTVr1jQWLVpkLFq0yLDZbEZ2drZx1113GRaLxXj44YeNOXPmGD179jQkGaNHj3Y4jiSjcePGRpUqVYzJkycbc+fONXbu3HnN83700UeGxWIxunTpYrz11lvGq6++atSuXdvw8fExDh8+bK83c+ZMo3379saUKVOMd955x3jqqaeMsmXLGm3btnX4mfz2229G9erVjXLlyhmjR4825s+fb7z44otG48aN7f3PeR1atmxp3HXXXcZbb71lPP3004arq6vRr1+/6/58DcMwIiMjDQ8PDyMwMNB49NFHjblz5xq33XabIcn44IMPjOrVqxtjx4413nrrLaNJkyaGq6ur8csvv9j3T09PN5o3b25UqlTJeP7554358+cbERERhsViMZ566in76zFv3jxDknHvvffaX49du3Y5tKFJkybG0KFDjXnz5hl9+vQxJBlvv/22/VwFee0efPBBQ5IxcOBAY86cOcZ9991nNG/e3JBkREdHm/5cAABwhp07dxqSjKlTpxqnTp0yjh8/bqxevdpo0aKF4enpaWzbts1e98iRI4Ykw8PDw/D09LQvmzZtuu451q9fb0gyli9f7lDeu3dvw83NzTh06JC97Pjx40b58uWNDh06ONTNGaMNGDAgX/0aNmyYUa1aNeP06dMO5f379ze8vb2NCxcuGIZh2P97tfj4eEOS8dFHH9nLIiIiDBcXF4efR46csVROG4cOHeqw/d577zUqVapk2uac/UeMGGEvu3LlilGzZk3DYrEYr7zyir38jz/+MMqWLWtERkYWuM8zZswwJDmMFf/aBrM+FOS16927t+Hh4WEcPXrUXrZv3z7D1dXV4H/DUZxx9QJF3NUh0pw5c4zy5cvb3wz79u1r3HnnnYZhGLlCpAsXLhgNGzY0JBkBAQHGQw89ZLz//vtGSkpKrnPkvHHmtTRs2NC0jR07djSaNGniULZixQpDkvHSSy85lN9///2GxWIxfv75Z3uZJMPFxcXYu3ev6bnOnTtn+Pj4GMOHD3cot9lshre3t0N5XgOkTz75xJDkMPDLzwAp53UICwtzCKDGjBljuLq6GmfPnr1uuyMjIw1JxrRp0+xlOQMhi8ViLF261F5+4MCBXCHM1KlTDU9PT+P//u//HI773HPPGa6urkZycrJhGIZx6tSpawY4OW2YMmWKQ3nLli2N4OBg+3p+X7vExERDkvH444871Bs4cCAhEgCgSPvwww/zHPd06tTpuh9kFUReIdKVK1eMcuXK5fkB1COPPGK4uLgYqamp9rKcMdrGjRtNz5ednW34+PgYI0aMyPWhYM445vvvv8+1X2ZmpnH69Gnj1KlTho+Pj/0Do6ysLMNqtRq9evW67nlz2rh161aH8tdff92Q5NCfguzfu3dvQ5Jx6tQph/KgoCCjffv2Be5zfkKk6/WhIK/dlStXjLJlyxr9+/fPVbdbt26ESCjW+DobUIz069dPFy9e1MqVK3Xu3DmtXLkyz6+ySVLZsmW1ZcsW+/e/Fy5cqGHDhqlatWp64oknlJGRkWuf//znP4qNjXVYPvjgg7/V1q+//lqurq568sknHcqffvppGYahb775xqG8Y8eOCgwMND1ubGyszp49qwEDBuj06dP2xdXVVSEhIVq/fr29btmyZe3/vnTpkk6fPq127dpJknbs2CHpzzkOVqxYoZ49e6p169a5zvfXOQ9GjBjhUNa+fXtlZWXp6NGjpm2XpIcfftj+bx8fHzVs2FCenp7q16+fvbxhw4by8fHRL7/8Yi9bvny52rdvrwoVKjj0OywsTFlZWQ63dZt59NFHHdbbt2/vcK78vnZff/21JOWqN3r06Hy3BQAAZ8h5MtuqVasUGxurjz/+WE2aNFFCQoLpU9f+iVOnTunChQtq2LBhrm2NGzdWdna2jh07lmtbnTp18nXss2fP6p133lGVKlUcliFDhkj632TfFy9e1MSJE+1fW69cubKqVKmis2fP2ueDOnXqlNLS0tS0adN89a1WrVoO6zlPovvjjz/+1v7e3t7y8PBQ5cqVc5XnHLMgff6nfSjIa3fq1CldvHhRt956a666ee0PFCc8nQ0oRqpUqaKwsDAtWbJEFy5cUFZWlu6///5r1vf29tb06dM1ffp0HT16VHFxcZo5c6bmzJkjb29vvfTSSw71O3TokOuN+u86evSoqlevrvLlyzuUN27c2L79avkZHEnSTz/9JEm666678tx+9VxPZ86c0eTJk7V06dJcAwhnDJA8PDxyfffe29tbNWvWzBVWXT1Akv7sd1JSUq79c+R3gJRXGypUqOBwrvy+dkePHpWLi4vq1avnUI/BEQCgqEtKSlJAQIC6detmL2vVqpUCAwP19ttva8aMGU5sXW5XfzB2LTlzJD744IOKjIzMs07z5s0lSU888YQ++OADjR49WqGhofL29pbFYlH//v3txymoqycov5rxl/kUC7K/2TEL0ue/24arzweAEAkodgYOHKjhw4fLZrOpa9eu+X4qWkBAgIYOHap7771XdevW1eLFi3OFSM6Un8GR9L/BwqJFi3I9kU6Sw2SX/fr10+bNmzV27FgFBQXJy8tL2dnZ6tKli1MGSNfaNz/HzM7O1j333KNnn302z7oNGjQwPf/1zgUAQGmSlJSktm3bOpQ1btxYrVu31n/+859CC5GqVKmicuXK6eDBg7m2HThwQC4uLvL39//bxy5fvryysrIUFhZ23bqfffaZIiMjHZ50dunSJZ09e9bheFarVXv27Plb7bkZCtJnsyfq5edc+X3tqlSporJly9o//LxaXvsDxQkhElDM3HvvvXrkkUf0448/atmyZQXev0KFCqpXr16hDwgCAgK0bt06nTt3zuGOlgMHDti3/x05d71UrVr1uoOFP/74Q3FxcZo8ebImTpxoL//rm3lxGCBJf/b7/PnzhT5AkvL/2gUEBCg7O1uHDh1yuPuIwREAoCiz2Ww6efJknnchh4eH6+WXX9b+/fvtd+DeSK6ururcubO+/PJLHTlyRLVr15b05xPGlixZojvuuONvP0HX1dVVffr00ZIlS7Rnz55c/Tt16pT9bmRXV9dcH4C99dZbDk9rdXFxUe/evfXxxx9r+/btub72bxjGDRl3/BMF6bOnp6ckOQRlBT1Xfl87V1dXhYeHa8WKFUpOTrbfyb5//36tWbPmb50fKCqYEwkoZry8vDRv3jxNmjRJPXv2vGa9Xbt26fTp07nKjx49qn379hX6V466deumrKwszZkzx6H8jTfekMViUdeuXf/WccPDw2W1WjVt2jRdvnw51/ZTp05J+t8dN38dIF39eFvpfwOkr776Stu3b891vKJy+3K/fv0UHx+f58Dj7NmzunLliiSpXLly9rK/K7+vXc5/33zzTYd6f/0ZAwBQlOTMh9SsWbNc2zp37izpz7mSCstLL72kMmXK6I477tC0adM0ffp03XbbbcrIyND06dP/0bFfeeUVVatWTSEhIRo9erTeeecdvfLKK+rXr5/D2K9Hjx5atGiRvc6QIUP05ptvqlKlSg7HmzZtmqpWraqOHTtqzJgxeueddzR58mQ1bdrUPjWAs+W3z8HBwZKkF154QYsWLdLSpUuVnp5eoHMV5LWbPHmypD/nnnz11Vf18ssv684771STJk3+YY8B5+JOJKAYutZ3vq8WGxur6Oho/etf/1K7du3k5eWlX375RQsWLFBGRoYmTZqUa5/PPvtMXl5eucrvuece+fr6FqiNPXv21J133qkXXnhBR44cUYsWLbR27Vp9+eWXGj16dK55dPLLarVq3rx5Gjx4sFq1aqX+/furSpUqSk5O1qpVq3T77bdrzpw5slqt6tChg6ZPn67Lly+rRo0aWrt2rQ4fPpzrmNOmTdPatWvVsWNHjRgxQo0bN9aJEye0fPlyff/99/n+ymBhGjt2rP773/+qR48eeuihhxQcHKz09HTt3r1bn332mY4cOaLKlSurbNmyCgwM1LJly9SgQQNVrFhRTZs2zfecT1L+X7ugoCANGDBAb7/9tlJTU3XbbbcpLi5OP//8c2H9GAAA+MeSkpIkKc/3xtDQUJUvX15ff/21nnnmmUI5f5MmTfTdd99p/PjxiomJUXZ2tkJCQvTxxx8rJCTkHx3b19dXW7du1ZQpU/T555/r7bffVqVKldSkSRO9+uqr9nqzZ8+Wq6urFi9erEuXLun222/XunXrFB4e7nC8GjVqaMuWLXrxxRe1ePFipaWlqUaNGuratav9gytny2+f27Rpo6lTp2r+/PlavXq1srOzdfjwYfsdSvlRkNeuefPmWrNmjaKiojRx4kTVrFlTkydP1okTJ+zXIFAsOempcADyKefxpHk9fv5qAQEBRvfu3e3rv/zyizFx4kSjXbt2RtWqVY0yZcoYVapUMbp37258++23DvvmPNb0Wsv69euve+6OHTsaTZo0yVV+7tw5Y8yYMUb16tWNW265xbj11luNGTNmGNnZ2Q71JBkjR440+Uk4Wr9+vREeHm54e3sbHh4eRr169YyHHnrI2L59u73Or7/+atx7772Gj4+P4e3tbfTt29c4fvx4no+fP3r0qBEREWFUqVLFcHd3N+rWrWuMHDnSyMjIMAzj2q9DzuN7zX5GkZGRhqenZ67ya/3s/vp6GsafP8/x48cb9evXN9zc3IzKlSsbt912mzFz5kwjMzPTXm/z5s1GcHCw4ebm5tDXa7Uh5/X/67ny89pdvHjRePLJJ41KlSoZnp6eRs+ePY1jx47l+TMGAAAAULxZDKOIfFcDAAAAAAAARRZzIgEAAAAAAMAUIRIAAAAAAABMESIBAAAAAADAFCESAAAAAAAATBEiAQAAAAAAwBQhEgAAAAAAAEyVcXYDioPs7GwdP35c5cuXl8VicXZzAADAdRiGoXPnzql69epyceHzMmdh/AQAQPFQoLGT4URvv/220axZM6N8+fJG+fLljXbt2hlff/21fXvHjh0NSQ7LI4884nCMo0ePGt26dTPKli1rVKlSxXjmmWeMy5cvO9RZv3690bJlS8PNzc2oV6+e8cEHHxSonceOHcvVDhYWFhYWFpaivRw7duxvj1HwzzF+YmFhYWFhKV5LfsZOTr0TqWbNmnrllVd06623yjAMffjhh+rVq5d27typJk2aSJKGDx+uKVOm2PcpV66c/d9ZWVnq3r27/Pz8tHnzZp04cUIRERG65ZZbNG3aNEnS4cOH1b17dz366KNavHix4uLi9PDDD6tatWoKDw/PVzvLly8vSTp27JisVuuN6j4AACgEaWlp8vf3t79/wzkYPwEAUDwUZOxkMQzDuAltyreKFStqxowZGjZsmDp16qSgoCDNmjUrz7rffPONevTooePHj8vX11eSNH/+fI0bN06nTp2Sm5ubxo0bp1WrVmnPnj32/fr376+zZ89q9erV+WpTWlqavL29lZqayiAIAIAijvftooHXAQCA4qEg79lFZqKArKwsLV26VOnp6QoNDbWXL168WJUrV1bTpk01fvx4Xbhwwb4tPj5ezZo1swdIkhQeHq60tDTt3bvXXicsLMzhXOHh4YqPj79mWzIyMpSWluawAAAAAAAAlGZOn1h79+7dCg0N1aVLl+Tl5aUvvvhCgYGBkqSBAwcqICBA1atXV1JSksaNG6eDBw/q888/lyTZbDaHAEmSfd1ms123Tlpami5evKiyZcvmalNMTIwmT558w/sKAAAAAABQXDk9RGrYsKESExOVmpqqzz77TJGRkdq4caMCAwM1YsQIe71mzZqpWrVquvvuu3Xo0CHVq1ev0No0fvx4RUVF2ddzvh8IAAAAAABQWjn962xubm6qX7++goODFRMToxYtWmj27Nl51g0JCZEk/fzzz5IkPz8/paSkONTJWffz87tuHavVmuddSJLk7u4uq9XqsAAAAAAAAJRmTg+R/io7O1sZGRl5bktMTJQkVatWTZIUGhqq3bt36+TJk/Y6sbGxslqt9q/EhYaGKi4uzuE4sbGxDvMuAQAAAAAA4Pqc+nW28ePHq2vXrqpVq5bOnTunJUuWaMOGDVqzZo0OHTqkJUuWqFu3bqpUqZKSkpI0ZswYdejQQc2bN5ckde7cWYGBgRo8eLCmT58um82mCRMmaOTIkXJ3d5ckPfroo5ozZ46effZZDR06VN9++60+/fRTrVq1ypldBwAAAAAAKFacGiKdPHlSEREROnHihLy9vdW8eXOtWbNG99xzj44dO6Z169Zp1qxZSk9Pl7+/v/r06aMJEybY93d1ddXKlSv12GOPKTQ0VJ6enoqMjNSUKVPsderUqaNVq1ZpzJgxmj17tmrWrKn33ntP4eHhzugyAAAAAABAsWQxDMNwdiOKurS0NHl7eys1NZX5kQAAKOJ43y4aeB0AACgeCvKeXeTmRAIAAAAAAEDRQ4gEAAAAAAAAU4RIAAAAAAAAMEWIBAAAAAAAAFOESAAAAAAAADBFiAQAAAAAAABThEgAAAAAAAAwRYgEAAAAAAAAU2Wc3QBIFouzWwBnMgxntwAAAABAcWGZzP9AlmZGtHP/B5I7kQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApphYGyjlmNi9dGNidwAAAAD5xZ1IAAAAAAAAMEWIBAAAAAAAAFOESAAAAKXA3LlzVbt2bXl4eCgkJERbt269Zt2FCxfKYrE4LB4eHjextQAAoCgiRAIAACjhli1bpqioKEVHR2vHjh1q0aKFwsPDdfLkyWvuY7VadeLECfty9OjRm9hiAABQFBEiAQAAlHCvv/66hg8friFDhigwMFDz589XuXLltGDBgmvuY7FY5OfnZ198fX1vYosBAEBRRIgEAABQgmVmZiohIUFhYWH2MhcXF4WFhSk+Pv6a+50/f14BAQHy9/dXr169tHfv3pvRXAAAUIQRIgEAAJRgp0+fVlZWVq47iXx9fWWz2fLcp2HDhlqwYIG+/PJLffzxx8rOztZtt92mX3/99ZrnycjIUFpamsMCAABKFkIkAAAAOAgNDVVERISCgoLUsWNHff7556pSpYr+/e9/X3OfmJgYeXt72xd/f/+b2GIAAHAzECIBAACUYJUrV5arq6tSUlIcylNSUuTn55evY9xyyy1q2bKlfv7552vWGT9+vFJTU+3LsWPH/lG7AQBA0VPG2Q0AAABA4XFzc1NwcLDi4uLUu3dvSVJ2drbi4uI0atSofB0jKytLu3fvVrdu3a5Zx93dXe7u7jeiyUCRZ5lscXYT4ERGtOHsJgBOQ4gEAABQwkVFRSkyMlKtW7dW27ZtNWvWLKWnp2vIkCGSpIiICNWoUUMxMTGSpClTpqhdu3aqX7++zp49qxkzZujo0aN6+OGHndkNAADgZIRIAAAAJdwDDzygU6dOaeLEibLZbAoKCtLq1avtk20nJyfLxeV/sxz88ccfGj58uGw2mypUqKDg4GBt3rxZgYGBzuoCAAAoAiyGYXAvnom0tDR5e3srNTVVVqv1hh/fwt2wpZqzfwO5/ko3Z19/QGEo7Pdt5A+vA0oyvs5Wujn762xcf6VbYVx/BXnPZmJtAAAAAAAAmOLrbAAAp+FOuNKNO+HwT/BJfOnm7DtBAKC04k4kAAAAAAAAmCJEAgAAAAAAgClCJAAAAAAAAJgiRAIAAAAAAIApQiQAAAAAAACYIkQCAAAAAACAKUIkAAAAAAAAmCJEAgAAAAAAgClCJAAAAAAAAJgiRAIAAAAAAIApQiQAAAAAAACYIkQCAAAAAACAKUIkAAAAAAAAmCJEAgAAAAAAgCmnhkjz5s1T8+bNZbVaZbVaFRoaqm+++ca+/dKlSxo5cqQqVaokLy8v9enTRykpKQ7HSE5OVvfu3VWuXDlVrVpVY8eO1ZUrVxzqbNiwQa1atZK7u7vq16+vhQsX3ozuAQAAAAAAlBhODZFq1qypV155RQkJCdq+fbvuuusu9erVS3v37pUkjRkzRl999ZWWL1+ujRs36vjx47rvvvvs+2dlZal79+7KzMzU5s2b9eGHH2rhwoWaOHGivc7hw4fVvXt33XnnnUpMTNTo0aP18MMPa82aNTe9vwAAAAAAAMWVxTAMw9mNuFrFihU1Y8YM3X///apSpYqWLFmi+++/X5J04MABNW7cWPHx8WrXrp2++eYb9ejRQ8ePH5evr68kaf78+Ro3bpxOnTolNzc3jRs3TqtWrdKePXvs5+jfv7/Onj2r1atX56tNaWlp8vb2VmpqqqxW6w3vs8Vyww+JYsTZv4Fcf6Ub1x+cqbCuv8J+30b+FPr4aTJ/QEozI9q5b2Bcf6Ub1x+cqTCuv4K8ZxeZOZGysrK0dOlSpaenKzQ0VAkJCbp8+bLCwsLsdRo1aqRatWopPj5ekhQfH69mzZrZAyRJCg8PV1pamv1upvj4eIdj5NTJOQYAAAAAAADMlXF2A3bv3q3Q0FBdunRJXl5e+uKLLxQYGKjExES5ubnJx8fHob6vr69sNpskyWazOQRIOdtztl2vTlpami5evKiyZcvmalNGRoYyMjLs62lpaf+4nwAAAAAAAMWZ0+9EatiwoRITE7VlyxY99thjioyM1L59+5zappiYGHl7e9sXf39/p7YHAAAAAADA2ZweIrm5ual+/foKDg5WTEyMWrRoodmzZ8vPz0+ZmZk6e/asQ/2UlBT5+flJkvz8/HI9rS1n3ayO1WrN8y4kSRo/frxSU1Pty7Fjx25EVwEAAAAAAIotp4dIf5Wdna2MjAwFBwfrlltuUVxcnH3bwYMHlZycrNDQUElSaGiodu/erZMnT9rrxMbGymq1KjAw0F7n6mPk1Mk5Rl7c3d1ltVodFgAAAAAAgNLMqXMijR8/Xl27dlWtWrV07tw5LVmyRBs2bNCaNWvk7e2tYcOGKSoqShUrVpTVatUTTzyh0NBQtWvXTpLUuXNnBQYGavDgwZo+fbpsNpsmTJigkSNHyt3dXZL06KOPas6cOXr22Wc1dOhQffvtt/r000+1atUqZ3YdAAAAAACgWHFqiHTy5ElFREToxIkT8vb2VvPmzbVmzRrdc889kqQ33nhDLi4u6tOnjzIyMhQeHq63337bvr+rq6tWrlypxx57TKGhofL09FRkZKSmTJlir1OnTh2tWrVKY8aM0ezZs1WzZk299957Cg8Pv+n9BQAAAAAAKK4shmEYzm5EUZeWliZvb2+lpqYWylfbLJYbfkgUI87+DeT6K924/uBMhXX9Ffb7NvKn0MdPk/kDUpoZ0c59A+P6K924/uBMhXH9FeQ9u8jNiQQAAAAAAICihxAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAoBSYO3euateuLQ8PD4WEhGjr1q352m/p0qWyWCzq3bt34TYQAAAUeYRIAAAAJdyyZcsUFRWl6Oho7dixQy1atFB4eLhOnjx53f2OHDmiZ555Ru3bt79JLQUAAEUZIRIAAEAJ9/rrr2v48OEaMmSIAgMDNX/+fJUrV04LFiy45j5ZWVkaNGiQJk+erLp1697E1gIAgKKKEAkAAKAEy8zMVEJCgsLCwuxlLi4uCgsLU3x8/DX3mzJliqpWraphw4bl6zwZGRlKS0tzWAAAQMlCiAQAAFCCnT59WllZWfL19XUo9/X1lc1my3Of77//Xu+//77efffdfJ8nJiZG3t7e9sXf3/8ftRsAABQ9hEgAAACwO3funAYPHqx3331XlStXzvd+48ePV2pqqn05duxYIbYSAAA4QxlnNwAAAACFp3LlynJ1dVVKSopDeUpKivz8/HLVP3TokI4cOaKePXvay7KzsyVJZcqU0cGDB1WvXr1c+7m7u8vd3f0Gtx4AABQl3IkEAABQgrm5uSk4OFhxcXH2suzsbMXFxSk0NDRX/UaNGmn37t1KTEy0L//617905513KjExka+pAQBQinEnEgAAQAkXFRWlyMhItW7dWm3bttWsWbOUnp6uIUOGSJIiIiJUo0YNxcTEyMPDQ02bNnXY38fHR5JylQMAgNKFEAkAAKCEe+CBB3Tq1ClNnDhRNptNQUFBWr16tX2y7eTkZLm4cIM6AAC4PkIkAACAUmDUqFEaNWpUnts2bNhw3X0XLlx44xsEAACKHad+5BQTE6M2bdqofPnyqlq1qnr37q2DBw861OnUqZMsFovD8uijjzrUSU5OVvfu3VWuXDlVrVpVY8eO1ZUrVxzqbNiwQa1atZK7u7vq16/PYAgAAAAAAKAAnBoibdy4USNHjtSPP/6o2NhYXb58WZ07d1Z6erpDveHDh+vEiRP2Zfr06fZtWVlZ6t69uzIzM7V582Z9+OGHWrhwoSZOnGivc/jwYXXv3t0+IeTo0aP18MMPa82aNTetrwAAAAAAAMWZU7/Otnr1aof1hQsXqmrVqkpISFCHDh3s5eXKlcvzEbSStHbtWu3bt0/r1q2Tr6+vgoKCNHXqVI0bN06TJk2Sm5ub5s+frzp16ui1116TJDVu3Fjff/+93njjDYWHhxdeBwEAAAAAAEqIIjWDYmpqqiSpYsWKDuWLFy9W5cqV1bRpU40fP14XLlywb4uPj1ezZs3sE0NKUnh4uNLS0rR37157nbCwMIdjhoeHKz4+Ps92ZGRkKC0tzWEBAAAAAAAozYrMxNrZ2dkaPXq0br/9dofHxw4cOFABAQGqXr26kpKSNG7cOB08eFCff/65JMlmszkESJLs6zab7bp10tLSdPHiRZUtW9ZhW0xMjCZPnnzD+wgAAAAAAFBcFZkQaeTIkdqzZ4++//57h/IRI0bY/92sWTNVq1ZNd999tw4dOqR69eoVSlvGjx+vqKgo+3paWpr8/f0L5VwAAAAAAADFQZH4OtuoUaO0cuVKrV+/XjVr1rxu3ZCQEEnSzz//LEny8/NTSkqKQ52c9Zx5lK5Vx2q15roLSZLc3d1ltVodFgAAAAAAgNLMqSGSYRgaNWqUvvjiC3377beqU6eO6T6JiYmSpGrVqkmSQkNDtXv3bp08edJeJzY2VlarVYGBgfY6cXFxDseJjY1VaGjoDeoJAAAAAABAyebUEGnkyJH6+OOPtWTJEpUvX142m002m00XL16UJB06dEhTp05VQkKCjhw5ov/+97+KiIhQhw4d1Lx5c0lS586dFRgYqMGDB2vXrl1as2aNJkyYoJEjR8rd3V2S9Oijj+qXX37Rs88+qwMHDujtt9/Wp59+qjFjxjit7wAAAAAAAMWJU0OkefPmKTU1VZ06dVK1atXsy7JlyyRJbm5uWrdunTp37qxGjRrp6aefVp8+ffTVV1/Zj+Hq6qqVK1fK1dVVoaGhevDBBxUREaEpU6bY69SpU0erVq1SbGysWrRooddee03vvfeewsPDb3qfAQAAAAAAiiOnTqxtGMZ1t/v7+2vjxo2mxwkICNDXX3993TqdOnXSzp07C9Q+AAAAAAAA/KlITKwNAAAAAACAoo0QCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYMqpIVJMTIzatGmj8uXLq2rVqurdu7cOHjzoUOfSpUsaOXKkKlWqJC8vL/Xp00cpKSkOdZKTk9W9e3eVK1dOVatW1dixY3XlyhWHOhs2bFCrVq3k7u6u+vXra+HChYXdPQAAAAAAgBLDqSHSxo0bNXLkSP3444+KjY3V5cuX1blzZ6Wnp9vrjBkzRl999ZWWL1+ujRs36vjx47rvvvvs27OystS9e3dlZmZq8+bN+vDDD7Vw4UJNnDjRXufw4cPq3r277rzzTiUmJmr06NF6+OGHtWbNmpvaXwAAAAAAgOLKYhiG4exG5Dh16pSqVq2qjRs3qkOHDkpNTVWVKlW0ZMkS3X///ZKkAwcOqHHjxoqPj1e7du30zTffqEePHjp+/Lh8fX0lSfPnz9e4ceN06tQpubm5ady4cVq1apX27NljP1f//v119uxZrV692rRdaWlp8vb2VmpqqqxW6w3vt8Vyww+JYsTZv4Fcf6Ub1x+cqbCuv8J+30b+FPr4aTJ/QEozI9q5b2Bcf6Ub1x+cqTCuv4K8ZxepOZFSU1MlSRUrVpQkJSQk6PLlywoLC7PXadSokWrVqqX4+HhJUnx8vJo1a2YPkCQpPDxcaWlp2rt3r73O1cfIqZNzDAAAAAAAAFxfGWc3IEd2drZGjx6t22+/XU2bNpUk2Ww2ubm5ycfHx6Gur6+vbDabvc7VAVLO9pxt16uTlpamixcvqmzZsg7bMjIylJGRYV9PS0v75x0EAAAAAAAoxorMnUgjR47Unj17tHTpUmc3RTExMfL29rYv/v7+zm4SAAAAAACAUxWJEGnUqFFauXKl1q9fr5o1a9rL/fz8lJmZqbNnzzrUT0lJkZ+fn73OX5/WlrNuVsdqtea6C0mSxo8fr9TUVPty7Nixf9xHAAAAZ5o7d65q164tDw8PhYSEaOvWrdes+/nnn6t169by8fGRp6engoKCtGjRopvYWgAAUBQ5NUQyDEOjRo3SF198oW+//VZ16tRx2B4cHKxbbrlFcXFx9rKDBw8qOTlZoaGhkqTQ0FDt3r1bJ0+etNeJjY2V1WpVYGCgvc7Vx8ipk3OMv3J3d5fVanVYAAAAiqtly5YpKipK0dHR2rFjh1q0aKHw8HCH8dPVKlasqBdeeEHx8fFKSkrSkCFDNGTIEJ5sCwBAKefUEGnkyJH6+OOPtWTJEpUvX142m002m00XL16UJHl7e2vYsGGKiorS+vXrlZCQoCFDhig0NFTt2rWTJHXu3FmBgYEaPHiwdu3apTVr1mjChAkaOXKk3N3dJUmPPvqofvnlFz377LM6cOCA3n77bX366acaM2aM0/oOAABws7z++usaPny4hgwZosDAQM2fP1/lypXTggUL8qzfqVMn3XvvvWrcuLHq1aunp556Ss2bN9f3339/k1sOAACKEqeGSPPmzVNqaqo6deqkatWq2Zdly5bZ67zxxhvq0aOH+vTpow4dOsjPz0+ff/65fburq6tWrlwpV1dXhYaG6sEHH1RERISmTJlir1OnTh2tWrVKsbGxatGihV577TW99957Cg8Pv6n9BQAAuNkyMzOVkJDg8KRaFxcXhYWF5etJtYZhKC4uTgcPHlSHDh2uWS8jI0NpaWkOCwAAKFmc+nQ2wzBM63h4eGju3LmaO3fuNesEBATo66+/vu5xOnXqpJ07dxa4jQAAAMXZ6dOnlZWVleeTag8cOHDN/VJTU1WjRg1lZGTI1dVVb7/9tu65555r1o+JidHkyZNvWLsBAEDRUyQm1gYAAEDRUr58eSUmJmrbtm16+eWXFRUVpQ0bNlyzPg8mAQCg5HPqnUgAAAAoXJUrV5arq2ueT6rNeZJtXlxcXFS/fn1JUlBQkPbv36+YmBh16tQpz/ru7u72+SgBAEDJxJ1IAAAAJZibm5uCg4MdnlSbnZ2tuLi4az6pNi/Z2dnKyMgojCYCAIBigjuRAAAASrioqChFRkaqdevWatu2rWbNmqX09HQNGTJEkhQREaEaNWooJiZG0p/zG7Vu3Vr16tVTRkaGvv76ay1atEjz5s1zZjcAAICTESIBAACUcA888IBOnTqliRMnymazKSgoSKtXr7ZPtp2cnCwXl//doJ6enq7HH39cv/76q8qWLatGjRrp448/1gMPPOCsLgAAgCLAYuTnEWmlXFpamry9vZWamiqr1XrDj2+x3PBDohhx9m8g11/pxvUHZyqs66+w37eRP4U+fprMH5DSzIh27hsY11/pxvUHZyqM668g79nMiQQAAAAAAABThEgAAAAAAAAwVaAQafr06bp48aJ9/YcffnB4Sse5c+f0+OOP37jWAQAAAAAAoEgoUIg0fvx4nTt3zr7etWtX/fbbb/b1Cxcu6N///veNax0AAAAAAACKhAKFSH+dg5s5uQEAAAAAAEoH5kQCAAAAAACAKUIkAAAAJ7t48aLDFAE59u7d64TWAAAA5K1MQXd477335OXlJUm6cuWKFi5cqMqVK0uSw3xJAAAAMPfZZ59p9OjRqly5srKzs/Xuu+8qJCREkjR48GDt2LHDyS0EAAD4U4FCpFq1aundd9+1r/v5+WnRokW56gAAACB/XnrpJSUkJMjX11cJCQmKjIzU888/r4EDBzL/JAAAKFIKFCIdOXKkkJoBAABQOl2+fFm+vr6SpODgYG3atEn33nuvfv75Z1ksFie3DgAA4H+YEwkAAMCJqlatqqSkJPt6xYoVFRsbq/379zuUAwAAOFuBQqT4+HitXLnSoeyjjz5SnTp1VLVqVY0YMUIZGRk3tIEAAAAl2aJFi1S1alWHMjc3N33yySfauHGjk1oFAACQW4FCpClTpjg8JWT37t0aNmyYwsLC9Nxzz+mrr75STEzMDW8kAABASVWzZk35+fnlue3222+/ya0BAAC4tgKFSImJibr77rvt60uXLlVISIjeffddRUVF6c0339Snn356wxsJAABQWhw9elRr166VzWbLc/vx48dvcosAAAD+VKAQ6Y8//rBP/ChJGzduVNeuXe3rbdq00bFjx25c6wAAAEqRTz75RPXr11eXLl1Ut25d+1Nwk5OT9corrygkJIQn4QIAAKcpUIjk6+urw4cPS5IyMzO1Y8cOtWvXzr793LlzuuWWW25sCwEAAEqJqVOn6oknntDu3bt1zz336LHHHtOLL76oevXqaeHChWrdurWWL1/u7GYCAIBSqkxBKnfr1k3PPfecXn31Va1YsULlypVT+/bt7duTkpJUr169G95IAACA0uDQoUN66qmnFBAQoLlz56pWrVr64YcflJSUpMaNGzu7eQAAoJQrUIg0depU3XffferYsaO8vLy0cOFCubm52bcvWLBAnTt3vuGNBAAAKA0uX76ssmXLSvpzwm0PDw/NnDmTAAkAABQJBQqRKleurE2bNik1NVVeXl5ydXV12L58+XKVL1/+hjYQAACgNFmyZIm6dOmiRo0aydXVVRUqVHB2kwAAACQVMEQaOnRovuotWLDgbzUGAACgNGvfvr2io6P19NNPq0KFCrp06ZJmz56t2267TU2bNlWDBg1UpkyBhm8AAAA3TIFGIQsXLlRAQIBatmwpwzAKq00AAACl0saNGyVJP/30kxISErRjxw7t2LFDH330kc6ePSs3Nzc1aNBASUlJTm4pAAAojQoUIj322GP65JNPdPjwYQ0ZMkQPPvigKlasWFhtAwAAKJVuvfVW3Xrrrerfv7+97PDhw9q+fbt27tzpxJYBAIDSzKUglefOnasTJ07o2Wef1VdffSV/f3/169dPa9as4c4kAACAQlSnTh317dtX06ZNc3ZTAABAKVWgEEmS3N3dNWDAAMXGxmrfvn1q0qSJHn/8cdWuXVvnz58vjDYCAAAAAADAyQocIjns7OIii8UiwzCUlZV1o9oEAAAAAACAIqbAIVJGRoY++eQT3XPPPWrQoIF2796tOXPmKDk5WV5eXoXRRgAAAAAAADhZgSbWfvzxx7V06VL5+/tr6NCh+uSTT1S5cuXCahsAAAAAAACKiAKFSPPnz1etWrVUt25dbdy40f4Y2r/6/PPPb0jjAAAAAAAAUDQUKESKiIiQxWIprLYAAAAAAACgiCpQiLRw4cJCagYAAAAAAACKsn/0dDYAAAAAAACUDoRIAAAAAAAAMEWIBAAAAAAAAFOESAAAAAAAADBFiAQAAAAAAABThEgAAAAAAAAwRYgEAAAAAAAAU4RIAAAAAAAAMOXUEGnTpk3q2bOnqlevLovFohUrVjhsf+ihh2SxWByWLl26ONQ5c+aMBg0aJKvVKh8fHw0bNkznz593qJOUlKT27dvLw8ND/v7+mj59emF3DQAAAAAAoERxaoiUnp6uFi1aaO7cudes06VLF504ccK+fPLJJw7bBw0apL179yo2NlYrV67Upk2bNGLECPv2tLQ0de7cWQEBAUpISNCMGTM0adIkvfPOO4XWLwAAAAAAgJKmjDNP3rVrV3Xt2vW6ddzd3eXn55fntv3792v16tXatm2bWrduLUl666231K1bN82cOVPVq1fX4sWLlZmZqQULFsjNzU1NmjRRYmKiXn/9dYewCQAAAAAAANdW5OdE2rBhg6pWraqGDRvqscce0++//27fFh8fLx8fH3uAJElhYWFycXHRli1b7HU6dOggNzc3e53w8HAdPHhQf/zxR57nzMjIUFpamsMCAAAAAABQmhXpEKlLly766KOPFBcXp1dffVUbN25U165dlZWVJUmy2WyqWrWqwz5lypRRxYoVZbPZ7HV8fX0d6uSs59T5q5iYGHl7e9sXf3//G901AAAAAACAYsWpX2cz079/f/u/mzVrpubNm6tevXrasGGD7r777kI77/jx4xUVFWVfT0tLI0gCAAAAAAClWpG+E+mv6tatq8qVK+vnn3+WJPn5+enkyZMOda5cuaIzZ87Y51Hy8/NTSkqKQ52c9WvNteTu7i6r1eqwAAAAAAAAlGbFKkT69ddf9fvvv6tatWqSpNDQUJ09e1YJCQn2Ot9++62ys7MVEhJir7Np0yZdvnzZXic2NlYNGzZUhQoVbm4HAAAAAAAAiimnhkjnz59XYmKiEhMTJUmHDx9WYmKikpOTdf78eY0dO1Y//vijjhw5ori4OPXq1Uv169dXeHi4JKlx48bq0qWLhg8frq1bt+qHH37QqFGj1L9/f1WvXl2SNHDgQLm5uWnYsGHau3evli1bptmzZzt8XQ0AAAAAAADX59QQafv27WrZsqVatmwpSYqKilLLli01ceJEubq6KikpSf/617/UoEEDDRs2TMHBwfruu+/k7u5uP8bixYvVqFEj3X333erWrZvuuOMOvfPOO/bt3t7eWrt2rQ4fPqzg4GA9/fTTmjhxokaMGHHT+wsAAAAAAFBcOXVi7U6dOskwjGtuX7NmjekxKlasqCVLlly3TvPmzfXdd98VuH0AAAAAAAD4U7GaEwkAAAAAAADOQYgEAAAAAAAAU4RIAAAAAAAAMEWIBAAAAAAAAFOESAAAAAAAADBFiAQAAAAAAABThEgAAAAAAAAwRYgEAAAAAAAAU4RIAAAAAAAAMEWIBAAAAAAAAFOESAAAAAAAADBFiAQAAFAKzJ07V7Vr15aHh4dCQkK0devWa9Z999131b59e1WoUEEVKlRQWFjYdesDAIDSgRAJAACghFu2bJmioqIUHR2tHTt2qEWLFgoPD9fJkyfzrL9hwwYNGDBA69evV3x8vPz9/dW5c2f99ttvN7nlAACgKCFEAgAAKOFef/11DR8+XEOGDFFgYKDmz5+vcuXKacGCBXnWX7x4sR5//HEFBQWpUaNGeu+995Sdna24uLib3HIAAFCUECIBAACUYJmZmUpISFBYWJi9zMXFRWFhYYqPj8/XMS5cuKDLly+rYsWK16yTkZGhtLQ0hwUAAJQshEgAAAAl2OnTp5WVlSVfX1+Hcl9fX9lstnwdY9y4capevbpDEPVXMTEx8vb2ti/+/v7/qN0AAKDoIUQCAADANb3yyitaunSpvvjiC3l4eFyz3vjx45Wammpfjh07dhNbCQAAboYyzm4AAAAACk/lypXl6uqqlJQUh/KUlBT5+fldd9+ZM2fqlVde0bp169S8efPr1nV3d5e7u/s/bi8AACi6uBMJAACgBHNzc1NwcLDDpNg5k2SHhoZec7/p06dr6tSpWr16tVq3bn0zmgoAAIo47kQCAAAo4aKiohQZGanWrVurbdu2mjVrltLT0zVkyBBJUkREhGrUqKGYmBhJ0quvvqqJEydqyZIlql27tn3uJC8vL3l5eTmtHwAAwLkIkQAAAEq4Bx54QKdOndLEiRNls9kUFBSk1atX2yfbTk5OlovL/25QnzdvnjIzM3X//fc7HCc6OlqTJk26mU0HAABFCCESAABAKTBq1CiNGjUqz20bNmxwWD9y5EjhNwgAABQ7zIkEAAAAAAAAU4RIAAAAAAAAMEWIBAAAAAAAAFOESAAAAAAAADBFiAQAAAAAAABThEgAAAAAAAAwRYgEAAAAAAAAU4RIAAAAAAAAMEWIBAAAAAAAAFOESAAAAAAAADBFiAQAAAAAAABThEgAAAAAAAAwRYgEAAAAAAAAU4RIAAAAAAAAMEWIBAAAAAAAAFOESAAAAAAAADBFiAQAAAAAAABThEgAAAAAAAAwRYgEAAAAAAAAU4RIAAAAAAAAMEWIBAAAAAAAAFNODZE2bdqknj17qnr16rJYLFqxYoXDdsMwNHHiRFWrVk1ly5ZVWFiYfvrpJ4c6Z86c0aBBg2S1WuXj46Nhw4bp/PnzDnWSkpLUvn17eXh4yN/fX9OnTy/srgEAAAAAAJQoTg2R0tPT1aJFC82dOzfP7dOnT9ebb76p+fPna8uWLfL09FR4eLguXbpkrzNo0CDt3btXsbGxWrlypTZt2qQRI0bYt6elpalz584KCAhQQkKCZsyYoUmTJumdd94p9P4BAAAAAACUFGWcefKuXbuqa9eueW4zDEOzZs3ShAkT1KtXL0nSRx99JF9fX61YsUL9+/fX/v37tXr1am3btk2tW7eWJL311lvq1q2bZs6cqerVq2vx4sXKzMzUggUL5ObmpiZNmigxMVGvv/66Q9gEAAAAAACAayuycyIdPnxYNptNYWFh9jJvb2+FhIQoPj5ekhQfHy8fHx97gCRJYWFhcnFx0ZYtW+x1OnToIDc3N3ud8PBwHTx4UH/88Uee587IyFBaWprDAgAAAAAAUJoV2RDJZrNJknx9fR3KfX197dtsNpuqVq3qsL1MmTKqWLGiQ528jnH1Of4qJiZG3t7e9sXf3/+fdwgAAAAAAKAYK7IhkjONHz9eqamp9uXYsWPObhIAAAAAAIBTFdkQyc/PT5KUkpLiUJ6SkmLf5ufnp5MnTzpsv3Llis6cOeNQJ69jXH2Ov3J3d5fVanVYAAAAAAAASrMiGyLVqVNHfn5+iouLs5elpaVpy5YtCg0NlSSFhobq7NmzSkhIsNf59ttvlZ2drZCQEHudTZs26fLly/Y6sbGxatiwoSpUqHCTegMAAAAAAFC8OTVEOn/+vBITE5WYmCjpz8m0ExMTlZycLIvFotGjR+ull17Sf//7X+3evVsRERGqXr26evfuLUlq3LixunTpouHDh2vr1q364YcfNGrUKPXv31/Vq1eXJA0cOFBubm4aNmyY9u7dq2XLlmn27NmKiopyUq8BAAAAAACKnzLOPPn27dt155132tdzgp3IyEgtXLhQzz77rNLT0zVixAidPXtWd9xxh1avXi0PDw/7PosXL9aoUaN09913y8XFRX369NGbb75p3+7t7a21a9dq5MiRCg4OVuXKlTVx4kSNGDHi5nUUAAAAAACgmLMYhmE4uxFFXVpamry9vZWamloo8yNZLDf8kChGnP0byPVXunH9wZkK6/or7Pdt5E+hj58m8wekNDOinfsGxvVXunH9wZkK4/oryHt2kZ0TCQAAAAAAAEUHIRIAAAAAAABMESIBAAAAAADAFCESAAAAAAAATBEiAQAAAAAAwBQhEgAAAAAAAEwRIgEAAAAAAMAUIRIAAAAAAABMESIBAAAAAADAFCESAAAAAAAATBEiAQAAAAAAwBQhEgAAAAAAAEwRIgEAAAAAAMAUIRIAAAAAAABMESIBAAAAAADAFCESAAAAAAAATBEiAQAAAAAAwBQhEgAAAAAAAEwRIgEAAAAAAMAUIRIAAAAAAABMESIBAAAAAADAFCESAAAAAAAATBEiAQAAAAAAwBQhEgAAAAAAAEwRIgEAAAAAAMAUIRIAAAAAAABMESIBAAAAAADAFCESAABAKTB37lzVrl1bHh4eCgkJ0datW69Zd+/everTp49q164ti8WiWbNm3byGAgCAIosQCQAAoIRbtmyZoqKiFB0drR07dqhFixYKDw/XyZMn86x/4cIF1a1bV6+88or8/PxucmsBAEBRRYgEAABQwr3++usaPny4hgwZosDAQM2fP1/lypXTggUL8qzfpk0bzZgxQ/3795e7u/tNbi0AACiqCJEAAABKsMzMTCUkJCgsLMxe5uLiorCwMMXHxzuxZQAAoLgp4+wGAAAAoPCcPn1aWVlZ8vX1dSj39fXVgQMHbth5MjIylJGRYV9PS0u7YccGAABFA3ciAQAA4B+LiYmRt7e3ffH393d2kwAAwA1GiAQAAFCCVa5cWa6urkpJSXEoT0lJuaGTZo8fP16pqan25dixYzfs2AAAoGggRAIAACjB3NzcFBwcrLi4OHtZdna24uLiFBoaesPO4+7uLqvV6rAAAICShTmRAAAASrioqChFRkaqdevWatu2rWbNmqX09HQNGTJEkhQREaEaNWooJiZG0p+Tce/bt8/+799++02JiYny8vJS/fr1ndYPAADgXIRIAAAAJdwDDzygU6dOaeLEibLZbAoKCtLq1avtk20nJyfLxeV/N6gfP35cLVu2tK/PnDlTM2fOVMeOHbVhw4ab3XwAAFBEECIBAACUAqNGjdKoUaPy3PbXYKh27doyDOMmtAoAABQnzIkEAAAAAAAAU4RIAAAAAAAAMEWIBAAAAAAAAFOESAAAAAAAADBFiAQAAAAAAABThEgAAAAAAAAwVaRDpEmTJslisTgsjRo1sm+/dOmSRo4cqUqVKsnLy0t9+vRRSkqKwzGSk5PVvXt3lStXTlWrVtXYsWN15cqVm90VAAAAAACAYq2MsxtgpkmTJlq3bp19vUyZ/zV5zJgxWrVqlZYvXy5vb2+NGjVK9913n3744QdJUlZWlrp37y4/Pz9t3rxZJ06cUEREhG655RZNmzbtpvcFAAAAAACguCryIVKZMmXk5+eXqzw1NVXvv/++lixZorvuukuS9MEHH6hx48b68ccf1a5dO61du1b79u3TunXr5Ovrq6CgIE2dOlXjxo3TpEmT5ObmdrO7AwAAAAAAUCwV6a+zSdJPP/2k6tWrq27duho0aJCSk5MlSQkJCbp8+bLCwsLsdRs1aqRatWopPj5ekhQfH69mzZrJ19fXXic8PFxpaWnau3fvNc+ZkZGhtLQ0hwUAAAAAAKA0K9IhUkhIiBYuXKjVq1dr3rx5Onz4sNq3b69z587JZrPJzc1NPj4+Dvv4+vrKZrNJkmw2m0OAlLM9Z9u1xMTEyNvb2774+/vf2I4BAAAAAAAUM0X662xdu3a1/7t58+YKCQlRQECAPv30U5UtW7bQzjt+/HhFRUXZ19PS0giSAAAAAABAqVak70T6Kx8fHzVo0EA///yz/Pz8lJmZqbNnzzrUSUlJsc+h5Ofnl+tpbTnrec2zlMPd3V1Wq9VhAQAAAAAAKM2KVYh0/vx5HTp0SNWqVVNwcLBuueUWxcXF2bcfPHhQycnJCg0NlSSFhoZq9+7dOnnypL1ObGysrFarAgMDb3r7AQAAAAAAiqsi/XW2Z555Rj179lRAQICOHz+u6Ohoubq6asCAAfL29tawYcMUFRWlihUrymq16oknnlBoaKjatWsnSercubMCAwM1ePBgTZ8+XTabTRMmTNDIkSPl7u7u5N4BAAAAAAAUH0U6RPr11181YMAA/f7776pSpYruuOMO/fjjj6pSpYok6Y033pCLi4v69OmjjIwMhYeH6+2337bv7+rqqpUrV+qxxx5TaGioPD09FRkZqSlTpjirSwAAAAAAAMVSkQ6Rli5det3tHh4emjt3rubOnXvNOgEBAfr6669vdNMAAAAAAABKlWI1JxIAAAAAAACcgxAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgihAJAAAAAAAApgiRAAAAAAAAYIoQCQAAAAAAAKYIkQAAAAAAAGCKEAkAAAAAAACmCJEAAAAAAABgqlSFSHPnzlXt2rXl4eGhkJAQbd261dlNAgAAuCkKOg5avny5GjVqJA8PDzVr1kxff/31TWopAAAoqkpNiLRs2TJFRUUpOjpaO3bsUIsWLRQeHq6TJ086u2kAAACFqqDjoM2bN2vAgAEaNmyYdu7cqd69e6t3797as2fPTW45AAAoSkpNiPT6669r+PDhGjJkiAIDAzV//nyVK1dOCxYscHbTAAAAClVBx0GzZ89Wly5dNHbsWDVu3FhTp05Vq1atNGfOnJvccgAAUJSUcXYDbobMzEwlJCRo/Pjx9jIXFxeFhYUpPj4+V/2MjAxlZGTY11NTUyVJaWlphd9YlDpcVnAmrj84U2Fdfznv14ZhFM4JipmCjoMkKT4+XlFRUQ5l4eHhWrFixTXPc9PHT5cK57AoHpw+Luf6K9W4/uBMhXH9FWTsVCpCpNOnTysrK0u+vr4O5b6+vjpw4ECu+jExMZo8eXKucn9//0JrI0ovb29ntwClGdcfnKmwr79z587Jm4u8wOMgSbLZbHnWt9ls1zwP4yfcTN6v8LsN5+H6gzMV5vWXn7FTqQiRCmr8+PEOn75lZ2frzJkzqlSpkiwWixNbVvKkpaXJ399fx44dk9VqdXZzUMpw/cGZuP4Kj2EYOnfunKpXr+7sppQqjJ9uHv5+wJm4/uBMXH+FoyBjp1IRIlWuXFmurq5KSUlxKE9JSZGfn1+u+u7u7nJ3d3co8/HxKcwmlnpWq5U/AnAarj84E9df4eAOpP8p6DhIkvz8/ApUX2L85Az8/YAzcf3Bmbj+brz8jp1KxcTabm5uCg4OVlxcnL0sOztbcXFxCg0NdWLLAAAACtffGQeFhoY61Jek2NhYxk0AAJRypeJOJEmKiopSZGSkWrdurbZt22rWrFlKT0/XkCFDnN00AACAQmU2DoqIiFCNGjUUExMjSXrqqafUsWNHvfbaa+revbuWLl2q7du365133nFmNwAAgJOVmhDpgQce0KlTpzRx4kTZbDYFBQVp9erVuSaNxM3l7u6u6OjoXLe/AzcD1x+ciesPN5PZOCg5OVkuLv+7Qf22227TkiVLNGHCBD3//PO69dZbtWLFCjVt2tRZXcBV+PsBZ+L6gzNx/TmfxeD5twAAAAAAADBRKuZEAgAAAAAAwD9DiAQAAAAAAABThEgAAAAAAAAwRYiEv6127dqaNWvWDa8LmLFYLFqxYoWzmwEUuk6dOmn06NHObgaAG4jxE5yBsRNKE8ZPhYsQqYR56KGHZLFYZLFYdMstt8jX11f33HOPFixYoOzs7Bt6rm3btmnEiBE3vO7fcXW/81pq165daOcujf56ndWpU0fPPvusLl265OymFaprXWc///yzU9vUu3dvp52/JPrr61ypUiV16dJFSUlJN70tn3/+uaZOnXrTzwuUNoyfGD8VNsZOjJ1KOsZPpQchUgnUpUsXnThxQkeOHNE333yjO++8U0899ZR69OihK1eu3LDzVKlSReXKlbvhdf+O2bNn68SJE/ZFkj744AP7+rZt2xzqZ2ZmFlpbSouc6+yXX37RG2+8oX//+9+Kjo52drMKXU6/r17q1Knzt47FdVh0Xf06x8XFqUyZMurRo8dNb0fFihVVvnz5m35eoDRi/MT4qbAxdmLsVNIxfiodCJFKIHd3d/n5+alGjRpq1aqVnn/+eX355Zf65ptvtHDhQnu9s2fP6uGHH1aVKlVktVp11113adeuXQ7H+uqrr9SmTRt5eHiocuXKuvfee+3brr7F2jAMTZo0SbVq1ZK7u7uqV6+uJ598Ms+6kpScnKxevXrJy8tLVqtV/fr1U0pKin37pEmTFBQUpEWLFql27dry9vZW//79de7cuTz77O3tLT8/P/siST4+Pvb1Nm3aaOrUqYqIiJDVarV/qvf999+rffv2Klu2rPz9/fXkk08qPT3dftyMjAw988wzqlGjhjw9PRUSEqINGzYU6PUoqXKuM39/f/Xu3VthYWGKjY21b//99981YMAA1ahRQ+XKlVOzZs30ySefOByjU6dOevLJJ/Xss8+qYsWK8vPz06RJkxzq/PTTT+rQoYM8PDwUGBjocI4cu3fv1l133aWyZcuqUqVKGjFihM6fP2/fnvOJ07Rp0+Tr6ysfHx9NmTJFV65c0dixY1WxYkXVrFlTH3zwQb77ffXi6uoqSdq4caPatm0rd3d3VatWTc8995zD/3h06tRJo0aN0ujRo1W5cmWFh4dLkvbs2aOuXbvKy8tLvr6+Gjx4sE6fPm3f77PPPlOzZs3s/QsLC1N6eromTZqkDz/8UF9++aX9Ux+uzxvj6tc5KChIzz33nI4dO6ZTp05JksaNG6cGDRqoXLlyqlu3rl588UVdvnzZ4RgvvfSSqlatqvLly+vhhx/Wc889p6CgIPv2K1eu6Mknn5SPj48qVaqkcePGKTIy0uHT0b/ejl27dm1NmzZNQ4cOVfny5VWrVi298847DufdvHmzgoKC5OHhodatW2vFihWyWCxKTEy80T8moERh/MT4qbAxdmLsVNIxfiodCJFKibvuukstWrTQ559/bi/r27evTp48qW+++UYJCQlq1aqV7r77bp05c0aStGrVKt17773q1q2bdu7cqbi4OLVt2zbP4//nP/+xf6Ly008/acWKFWrWrFmedbOzs9WrVy+dOXNGGzduVGxsrH755Rc98MADDvUOHTqkFStWaOXKlVq5cqU2btyoV1555W//DGbOnKkWLVpo586devHFF3Xo0CF16dJFffr0UVJSkpYtW6bvv/9eo0aNsu8zatQoxcfHa+nSpUpKSlLfvn3VpUsX/fTTT3+7HSXRnj17tHnzZrm5udnLLl26pODgYK1atUp79uzRiBEjNHjwYG3dutVh3w8//FCenp7asmWLpk+frilTptgHO9nZ2brvvvvk5uamLVu2aP78+Ro3bpzD/unp6QoPD1eFChW0bds2LV++XOvWrXN4HSXp22+/1fHjx7Vp0ya9/vrrio6OVo8ePVShQgVt2bJFjz76qB555BH9+uuvf+tn8Ntvv6lbt25q06aNdu3apXnz5un999/XSy+9lKu/bm5u+uGHHzR//nydPXtWd911l1q2bKnt27dr9erVSklJUb9+/SRJJ06c0IABAzR06FDt379fGzZs0H333SfDMPTMM8+oX79+Dp/63HbbbX+r/bi28+fP6+OPP1b9+vVVqVIlSVL58uW1cOFC7du3T7Nnz9a7776rN954w77P4sWL9fLLL+vVV19VQkKCatWqpXnz5jkc99VXX9XixYv1wQcf6IcfflBaWlq+5qt47bXX1Lp1a+3cuVOPP/64HnvsMR08eFCSlJaWpp49e6pZs2basWOHpk6dmut3BkD+MX5i/FRYGDsxdirpGD+VYAZKlMjISKNXr155bnvggQeMxo0bG4ZhGN99951htVqNS5cuOdSpV6+e8e9//9swDMMIDQ01Bg0adM1zBQQEGG+88YZhGIbx2muvGQ0aNDAyMzNN665du9ZwdXU1kpOT7dv37t1rSDK2bt1qGIZhREdHG+XKlTPS0tLsdcaOHWuEhIRcu/NXkWR88cUXDufv3bu3Q51hw4YZI0aMcCj77rvvDBcXF+PixYvG0aNHDVdXV+O3335zqHP33Xcb48ePz1c7SqrIyEjD1dXV8PT0NNzd3Q1JhouLi/HZZ59dd7/u3bsbTz/9tH29Y8eOxh133OFQp02bNsa4ceMMwzCMNWvWGGXKlHF4Db755huH1/edd94xKlSoYJw/f95eZ9WqVYaLi4ths9ns7Q0ICDCysrLsdRo2bGi0b9/evn7lyhXD09PT+OSTT/LV75zl/vvvNwzDMJ5//nmjYcOGRnZ2tr3+3LlzDS8vL/t5O3bsaLRs2dLhmFOnTjU6d+7sUHbs2DFDknHw4EEjISHBkGQcOXLkmm261u88/p6/vs6SjGrVqhkJCQnX3GfGjBlGcHCwfT0kJMQYOXKkQ53bb7/daNGihX3d19fXmDFjhn39ypUrRq1atRxez44dOxpPPfWUfT0gIMB48MEH7evZ2dlG1apVjXnz5hmGYRjz5s0zKlWqZFy8eNFe59133zUkGTt37szvjwAodRg//YnxU+Fh7MTYqaRj/FR6lLn5sRWcxTAMWSwWSdKuXbt0/vx5eyqc4+LFizp06JAkKTExUcOHD8/Xsfv27atZs2apbt266tKli7p166aePXuqTJncl9j+/fvl7+8vf39/e1lgYKB8fHy0f/9+tWnTRtKftx1e/V3WatWq6eTJkwXr9FVat27tsL5r1y4lJSVp8eLF9jLDMJSdna3Dhw/rl19+UVZWlho0aOCwX0ZGRq6fW2l05513at68eUpPT9cbb7yhMmXKqE+fPvbtWVlZmjZtmj799FP99ttvyszMVEZGRq65HZo3b+6wfvXrnHOtVK9e3b49NDTUof7+/fvVokULeXp62stuv/12ZWdn6+DBg/L19ZUkNWnSRC4u/7v50tfXV02bNrWvu7q6qlKlSqbXWE6/c+Scd//+/QoNDbX/juW04/z58/r1119Vq1YtSVJwcLDD8Xbt2qX169fLy8sr17kOHTqkzp076+6771azZs0UHh6uzp076/7771eFChWu2078M1e/zn/88Yfefvttde3aVVu3blVAQICWLVumN998U4cOHdL58+d15coVWa1W+/4HDx7U448/7nDMtm3b6ttvv5UkpaamKiUlxeHuBFdXVwUHB5tO4nv174zFYpGfn5/9uj148KCaN28uDw8Ph/MC+PsYPzF+ulEYO/2JsVPJxfipdCBEKkX2799vn8Tu/PnzqlatWp7fAfbx8ZEklS1bNt/H9vf318GDB7Vu3TrFxsbq8ccf14wZM7Rx40bdcsstf6u9f93PYrH8oyekXP1GKf35M3jkkUcc5h7IUatWLSUlJcnV1VUJCQn2723nyOtNq7Tx9PRU/fr1JUkLFixQixYt9P7772vYsGGSpBkzZmj27NmaNWuWmjVrJk9PT40ePTrXhIg3+nW+lrzO83fOfXW//468rsOePXvq1VdfzVW3WrVqcnV1VWxsrDZv3qy1a9fqrbfe0gsvvKAtW7b87UkpYe6vr/N7770nb29vvfvuu+revbsGDRqkyZMnKzw8XN7e3lq6dKlee+21m9K2m/U7A+BPjJ8YP90ojJ3+HsZOxQfjp9KBOZFKiW+//Va7d++2f9rRqlUr2Ww2lSlTRvXr13dYKleuLOnPtDYuLi7f5yhbtqx69uypN998Uxs2bFB8fLx2796dq17jxo117NgxHTt2zF62b98+nT17VoGBgf+wp/nXqlUr7du3L1f/69evLzc3N7Vs2VJZWVk6efJkru05k0/iTy4uLnr++ec1YcIEXbx4UZL0ww8/qFevXnrwwQfVokUL1a1bV//3f/9XoOPmXCs5T4yRpB9//DFXnV27djlM6PnDDz/IxcVFDRs2/Ae9KpjGjRsrPj5ehmE4tKN8+fKqWbPmNfdr1aqV9u7dq9q1a+e6znIGTRaLRbfffrsmT56snTt3ys3NTV988YUkyc3NTVlZWYXbOchiscjFxUUXL17U5s2bFRAQoBdeeEGtW7fWrbfeqqNHjzrUb9iwYa6nGl297u3tLV9fX4eyrKws7dix4x+1s2HDhtq9e7cyMjLyPC+AgmH8lBvjpxuDsRNjp9KA8VPJRIhUAmVkZMhms+m3337Tjh07NG3aNPXq1Us9evRQRESEJCksLEyhoaHq3bu31q5dqyNHjmjz5s164YUXtH37dklSdHS0PvnkE0VHR2v//v3avXt3nom/JC1cuFDvv/++9uzZo19++UUff/yxypYtq4CAgFx1w8LC1KxZMw0aNEg7duzQ1q1bFRERoY4dO+a6ZbowjRs3Tps3b9aoUaOUmJion376SV9++aV9UsEGDRpo0KBBioiI0Oeff67Dhw9r69atiomJ0apVq25aO4uLvn37ytXVVXPnzpUk3XrrrfZPgfbv369HHnnE4Qky+REWFqYGDRooMjJSu3bt0nfffacXXnjBoc6gQYPk4eGhyMhI7dmzR+vXr9cTTzyhwYMH22/Hvhkef/xxHTt2TE888YQOHDigL7/8UtHR0YqKinK4FfyvRo4cqTNnzmjAgAHatm2bDh06pDVr1mjIkCHKysrSli1bNG3aNG3fvl3Jycn6/PPPderUKTVu3FjSn19bSEpK0sGDB3X69OlcT7jA35Pzd9Rms2n//v164okn7J983nrrrUpOTtbSpUt16NAhvfnmm/aBaY4nnnhC77//vj788EP99NNPeumll5SUlORwy/4TTzyhmJgYffnllzp48KCeeuop/fHHHw51CmrgwIHKzs7WiBEjtH//fq1Zs0YzZ86UpH90XKA0YPyUP4yfbhzGToydShrGT6UDIVIJtHr1alWrVk21a9dWly5dtH79er355pv68ssv7bcVWywWff311+rQoYOGDBmiBg0aqH///jp69Kj9zaNTp05avny5/vvf/yooKEh33XVXrqdD5PDx8dG7776r22+/Xc2bN9e6dev01Vdf5fndd4vFoi+//FIVKlRQhw4dFBYWprp162rZsmWF90PJQ/PmzbVx40b93//9n9q3b6+WLVtq4sSJDt8h/+CDDxQREaGnn35aDRs2VO/evbVt2zb7d7TxP2XKlNGoUaM0ffp0paena8KECWrVqpXCw8PVqVMn+fn5OTx6Mz9cXFz0xRdf6OLFi2rbtq0efvhhvfzyyw51ypUrpzVr1ujMmTNq06aN7r//ft19992aM2fODeyduRo1aujrr7/W1q1b1aJFCz366KMaNmyYJkyYcN39qlevrh9++EFZWVnq3LmzmjVrptGjR8vHx0cuLi6yWq3atGmTunXrpgYNGmjChAl67bXX1LVrV0nS8OHD1bBhQ7Vu3VpVqlTRDz/8cDO6W+Ll/B2tVq2aQkJC7E+v6dSpk/71r39pzJgxGjVqlIKCgrR582a9+OKLDvsPGjRI48eP1zPPPKNWrVrp8OHDeuihhxy+az9u3DgNGDBAERERCg0NlZeXl8LDwx3qFJTVatVXX32lxMREBQUF6YUXXtDEiRMl6R8dFygNGD/lD+OnG4exE2OnkobxU+lgMa6+fxAAABSKe+65R35+flq0aFGe27Ozs9W4cWP169dPU6dOvWHnXbx4sYYMGaLU1NQCzdUCAADgbIyfih4m1gYA4Aa7cOGC5s+fr/DwcLm6uuqTTz6xT5yb4+jRo1q7dq06duyojIwMzZkzR4cPH9bAgQP/0bk/+ugj1a1bVzVq1NCuXbs0btw49evXjwEQAAAo0hg/FQ+ESAAA3GA5X3l5+eWXdenSJTVs2FD/+c9/FBYWZq/j4uKihQsX6plnnpFhGGratKnWrVtnn7Ph77LZbJo4caJsNpuqVaumvn375voqAwAAQFHD+Kl44OtsAAAAAAAAMMXE2gAAAAAAADBFiAQAAAAAAABThEgAAAAAAAAwRYgEAAAAAAAAU4RIAAAAAAAAMEWIBAAAAAAAAFOESAAAAAAAADBFiAQAAAAAAABThEgAAAAAAAAw9f/0VAXcVZt37QAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize = (14, 5))\n",
        "\n",
        "labels = ('Decision Tree', 'Random Forest', 'Bagging')\n",
        "\n",
        "MSE_vals = [tree_test_results[0], rf_test_results[0], bag_test_results[0]]\n",
        "rsq_vals = [tree_test_results[1], rf_test_results[1], bag_test_results[1]]\n",
        "\n",
        "ax[0].bar(labels, MSE_vals, color = 'blue')\n",
        "ax[0].set_ylabel('MSE')\n",
        "ax[0].set_title('MSE for each method')\n",
        "\n",
        "ax[1].bar(labels, rsq_vals, color = 'green')\n",
        "ax[1].set_ylabel(r'$R^2$')\n",
        "ax[1].set_title(r'$R^2$ for each method')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEwNJHHTMgNG"
      },
      "source": [
        "<a name=\"task-113\"></a>\n",
        "\n",
        "### (1.1.3) [(index)](#index-task-113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqN02H_YPwr0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In8XkuW_MgVV"
      },
      "source": [
        "<a name=\"task-12\"></a>\n",
        "\n",
        "## (1.2) Multi-layer Perceptron [(index)](#index-task-12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1aYyFt7MgeQ"
      },
      "source": [
        "<a name=\"task-121\"></a>\n",
        "\n",
        "### (1.2.1) [(index)](#index-task-121)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "def standardise(X, X_train_=None):\n",
        "    \"\"\"Standardise features.\n",
        "\n",
        "    Parameters:\n",
        "        X (np.array): Feature matrix.\n",
        "        X_train_ (np.array): An optional feature matrix to compute the statistics\n",
        "            from before applying it to X. If None, just use X to compute the statistics.\n",
        "\n",
        "    Returns:\n",
        "        X_std (np.array): Standardised feature matrix\n",
        "    \"\"\"\n",
        "    if X_train_ is None:\n",
        "        X_train_ = X\n",
        "\n",
        "    mu = np.mean(X_train_, axis=0, keepdims=True) \n",
        "    sigma = np.std(X_train_, axis=0, keepdims=True) \n",
        "    X_std = (X - mu) / sigma \n",
        "    return X_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = standardise(X_train)\n",
        "X_test = standardise(X_test.to_numpy(), X_train_=X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dense(X, W, b):\n",
        "    \"\"\"Full-connected MLP layer.\n",
        "\n",
        "    Parameters:\n",
        "        X (np.ndarray): K x h_in array of inputs, where K is the batch size and h_in if the input features dimension.\n",
        "        W (np.ndarray): h_out x h_in array for kernel matrix parametersm, where h_out is the output dimension.\n",
        "        b (np.ndarray): Length h_out 1-D array for bias parameters\n",
        "\n",
        "    Returns:\n",
        "        a (np.ndarray): K x h_out array of pre-activations\n",
        "    \"\"\"\n",
        "    a = np.vstack([W @ x + b for x in X])\n",
        "    return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "def activation(a):\n",
        "    \"\"\"activation function.\n",
        "\n",
        "    Parameters:\n",
        "        a: K x h_out array of pre-activations\n",
        "\n",
        "    Returns:\n",
        "        h: K x h_out array of post-activations\n",
        "    \"\"\"\n",
        "    # compute post-activations\n",
        "    h = np.arctan(a) * np.log(np.abs(a) + 1)\n",
        "    return h\n",
        "\n",
        "## EDIT THIS FUNCTION\n",
        "def grad_activation(a):\n",
        "    \"\"\"Gradient of activation function.\n",
        "\n",
        "    Parameters:\n",
        "        a: K x h_out array of pre-activations\n",
        "\n",
        "    Returns:\n",
        "        grad: K x h_out gradient array of post-activations\n",
        "    \"\"\"\n",
        "    # compute gradient\n",
        "    grad = np.log(np.abs(a) + 1) / (1 + a ** 2) + (np.arctan(a) * np.sign(a)) / (np.abs(a) + 1)\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our MLP will need the following parameters:\n",
        "\n",
        "Input layer -> first hidden layer:\n",
        "* Kernel $\\boldsymbol{W}^{(0)} \\in\\mathbb{R}^{50 \\times 12}$\n",
        "* Bias $\\boldsymbol{b}^{(0)} \\in\\mathbb{R}^{50}$\n",
        "\n",
        "Hidden layer -> hidden layer:\n",
        "* Kernel $\\boldsymbol{W}^{(k)} \\in\\mathbb{R}^{50\\times 50}$, $k=1, 2$\n",
        "* Bias $\\boldsymbol{b}^{(k)} \\in\\mathbb{R}^{50}$, $k=1, 2$\n",
        "\n",
        "Hidden layer -> output layer:\n",
        "* Kernel $\\boldsymbol{W}^{(3)} \\in\\mathbb{R}^{1 \\times 50}$\n",
        "* Bias $\\boldsymbol{b}^{(3)} \\in\\mathbb{R}^{1}$\n",
        "\n",
        "We will create these parameters as numpy arrays, and initialise the kernel values with samples from a zero-mean Gaussian distribution with variance $2 / (n_{in} + n_{out})$, where $n_{in}$ and $n_{out}$ are the number of neurons going in and out of the dense layer respectively. This initialisation strategy is known as [Glorot initialisation](http://proceedings.mlr.press/v9/glorot10a.html). The bias parameters will be initialised to zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(54)\n",
        "\n",
        "# create the parameters using Glorot initialisation\n",
        "var0 = 2. / (50 + 12)\n",
        "W0 = np.random.randn(50, 12) * np.sqrt(var0)\n",
        "b0 = np.zeros(50)\n",
        "\n",
        "var1 = 2. / (50 + 50)\n",
        "W1 = np.random.randn(50, 50) * np.sqrt(var1)\n",
        "b1 = np.zeros(50)\n",
        "\n",
        "var2 = 2. / (50 + 50)\n",
        "W2 = np.random.randn(50, 50) * np.sqrt(var2)\n",
        "b2 = np.zeros(50)\n",
        "\n",
        "var3 = 2. / (1 + 50)\n",
        "W3 = np.random.randn(1, 50) * np.sqrt(var3)\n",
        "b3 = np.zeros(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A lookup table for activation functions by their names.\n",
        "activation_table = {\n",
        "    \"activation\": activation,\n",
        "    \"identity\": lambda x: x\n",
        "}\n",
        "\n",
        "# A lookup table for gradient of activation functions by their names.\n",
        "grad_activation_table = {\n",
        "    \"activation\": grad_activation,\n",
        "    \"identity\": lambda x: np.ones_like(x)\n",
        "}\n",
        "\n",
        "\n",
        "class MLP:\n",
        "    \"\"\"\n",
        "    This class represents a Multi-Layer Perceptron (MLP), that we are going\n",
        "    to use to encapsulate two components:\n",
        "        1. layers: the sequence of layers, where each layer is stored in\n",
        "            a dictionary in the format {\"W\": np.ndarray, \"b\": np.ndarray},\n",
        "            where \"W\" points to the weights array, and \"b\" points to\n",
        "            the bias vector.\n",
        "        2. rng: a pseudo random number generator (RNG) initialised to generate\n",
        "            the random weights in a reproducible manner between different\n",
        "            runtime sessions.\n",
        "    This class is also shipped with methods that perform essential operations\n",
        "    with a MLP, including:\n",
        "        - add_layers: which creates a new layer with specified dimensions.\n",
        "        - predict: applies the MLP forward pass to make predictions and produces\n",
        "            a computational graph for the forward pass that can be used to\n",
        "            compute gradients using backpropagation algorithm.\n",
        "        in addition to other light functions that return simple statistics about\n",
        "        the MLP.\n",
        "    \"\"\"\n",
        "    def __init__(self, seed=54):\n",
        "        self.layers = []\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def n_parameters(self):\n",
        "        \"\"\"Return the total number of parameters of weights and biases.\"\"\"\n",
        "        return sum(l[\"b\"].size + l[\"W\"].size for l in self.layers)\n",
        "\n",
        "    def n_layers(self):\n",
        "        \"\"\"Return current number of MLP layers.\"\"\"\n",
        "        return len(self.layers)\n",
        "\n",
        "    def layer_dim(self, index):\n",
        "        \"\"\"Retrieve the dimensions of the MLP layer at `index`.\"\"\"\n",
        "        return self.layers[index][\"W\"].shape\n",
        "\n",
        "    def add_layer(self, in_dim, out_dim, activation=\"identity\"):\n",
        "        \"\"\"Add fully connected layer to MLP.\n",
        "\n",
        "        Parameters:\n",
        "            in_dim (int): The output dimension of the layer.\n",
        "            out_dim (int): The input dimension of the layer.\n",
        "            activation (str): The activation function name.\n",
        "        \"\"\"\n",
        "        # check if input-dimension matches output-dimension of previous layer\n",
        "        if self.n_layers() > 0:\n",
        "            last_out_dim, _ = self.layer_dim(-1)\n",
        "            assert in_dim == last_out_dim, f\"Input-dimension {in_dim} does not match output-dimension {last_out_dim} of previous layer.\"\n",
        "\n",
        "        # the first layer, in our convention illustrated, does not apply activation on the input features X.\n",
        "        if self.n_layers() == 0:\n",
        "            assert activation == \"identity\", \"Should not apply activations on the input features X, use Identity function for the first layer.\"\n",
        "\n",
        "\n",
        "        # store each layer as a dictionary in the list, as shown in the\n",
        "        # attached diagram.\n",
        "        self.layers.append({\n",
        "            # only for debugging.\n",
        "            \"index\": len(self.layers),\n",
        "            # apply Glorot initialisation for weights.\n",
        "            \"W\": self.rng.normal(size=(out_dim, in_dim)) * np.sqrt(2. / (in_dim + out_dim)),\n",
        "            # initialise bias vector with zeros.\n",
        "            \"b\": np.zeros(out_dim),\n",
        "            # store the activation function (as string)\n",
        "            \"activation\": activation\n",
        "        })\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Apply the forward pass on the input X and produce prediction and the\n",
        "        forward computation graph.\n",
        "\n",
        "        Parameters:\n",
        "            X (np.ndarray): Feature matrix.\n",
        "\n",
        "        Returns:\n",
        "            (np.ndarray, List[Dict[str, np.ndarray]]): A tuple of the\n",
        "            predictions and the computation graph as a sequence of intermediate\n",
        "            values through the MLP, specifically each layer will have a corresponding\n",
        "            intermediate values {\"a\": np.ndarray, \"h\": np.ndarray}, as shown in the\n",
        "            attached diagram above.\n",
        "        \"\"\"\n",
        "        # We assume that we work with a batch of examples (ndim==2).\n",
        "        if X.ndim == 1:\n",
        "            # If one example passed, add a dummy dimension for the batch.\n",
        "            X = X.reshape(1, -1)\n",
        "\n",
        "        # store pre- and post-activations in list\n",
        "        forward_pass = [{\"index\": 0, \"a\": X, \"h\": X}]\n",
        "\n",
        "        # iterate through hidden layers\n",
        "        for k in range(1, len(self.layers)):\n",
        "            # compute pre-activations\n",
        "            a = dense(forward_pass[k - 1][\"h\"], self.layers[k - 1][\"W\"], self.layers[k - 1][\"b\"]) \n",
        "            activation = activation_table[self.layers[k][\"activation\"]] \n",
        "            forward_pass.append({\"index\": k, \"a\" : a, \"h\" : activation(a)}) \n",
        "\n",
        "        y_hat = dense(forward_pass[-1][\"h\"], self.layers[-1][\"W\"], self.layers[-1][\"b\"]) \n",
        "        # predicted target is output of last layer\n",
        "        return y_hat, forward_pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of layers: 3\n",
            "Number of trainable parameters: 3251\n"
          ]
        }
      ],
      "source": [
        "mlp = MLP(seed=54)\n",
        "mlp.add_layer(12, 50)\n",
        "mlp.add_layer(50, 50, \"activation\")\n",
        "mlp.add_layer(50, 1, \"activation\")\n",
        "print(\"Number of layers:\", mlp.n_layers())\n",
        "print(\"Number of trainable parameters:\", mlp.n_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_hat_train, forward_pass = mlp.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "def grad_mse_loss(y_true, y_pred):\n",
        "    \"\"\"Compute gradient of MSE-loss\n",
        "\n",
        "    Parameters:\n",
        "        y_true: ground-truth values, shape: (K, ).\n",
        "        y_pred: prediction values, shape: (K, ).\n",
        "\n",
        "    Returns:\n",
        "        grad (np.ndarray): Gradient of MSE-loss, shape: (K, ).\n",
        "    \"\"\"\n",
        "    # Adjustment to avoid subtraction between (K,) and (1, K) arrays.\n",
        "    y_true = y_true.reshape(y_pred.shape)\n",
        "\n",
        "    return 2.0 * (y_pred - y_true) / y_true.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "def backpropagate(layers, forward_pass, delta_output):\n",
        "    \"\"\"\n",
        "    Apply the backpropagation algorithm to the MLP layers to compute the gradients starting from\n",
        "    the output layer to the input layer, and starting the chain rule from the\n",
        "    partial derivative of the loss function w.r.t the predictions $\\hat{y}$. The\n",
        "\n",
        "    Parameters:\n",
        "        layers (List[Dict[str, np.ndarray]]): The MLP sequence of layers, as shown in the diagrams.\n",
        "        forward_pass (List[Dict[str, np.ndarray]]): The forward pass intermediate values for\n",
        "            each layer, representing a computation graph.\n",
        "        delta_output (np.ndarray): the partial derivative of the loss function w.r.t the\n",
        "            predictions $\\hat{y}$, has the shape (K, 1), where K is the batch size.\n",
        "    Returns:\n",
        "        (List[Dict[str, np.ndarray]]): The computed gradient using a structure symmetric the layers, as shown\n",
        "            in the diagrams.\n",
        "\n",
        "    \"\"\"\n",
        "    #Create a list that will contain the gradients of all the layers.\n",
        "    delta = delta_output\n",
        "\n",
        "    assert len(layers) == len(forward_pass), \"Number of layers is expected to match the number of forward pass layers\"\n",
        "\n",
        "    # Iterate on layers backwardly, from output to input.\n",
        "    # Calculate gradients w.r.t. weights and biases of each level and store in list of dictionaries.\n",
        "    gradients = []\n",
        "    for layer, forward_computes in reversed(list(zip(layers, forward_pass))):\n",
        "        assert forward_computes[\"index\"] == layer[\"index\"], \"Mismatch in the index.\"\n",
        "\n",
        "        h = forward_computes[\"h\"]\n",
        "        assert delta.shape[0] == h.shape[0], \"Mismatch in the batch dimension.\"\n",
        "\n",
        "\n",
        "        gradients.append({\"W\" : delta.T @ h, # <-- SOLUTION\n",
        "                          \"b\" : delta.sum(axis=0)}) # <-- SOLUTION\n",
        "\n",
        "        # Update the delta for the next iteration\n",
        "        grad_activation_f = grad_activation_table[layer[\"activation\"]]\n",
        "        grad_activation = grad_activation_f(forward_computes[\"a\"])\n",
        "\n",
        "        # Calculate the delta for the backward layer.\n",
        "        delta = np.stack([np.diag(gi) @ layer[\"W\"].T @ di\n",
        "                           for (gi, di) in zip(grad_activation, delta)]) \n",
        "\n",
        "\n",
        "    # Return now ordered list matching the layers.\n",
        "    return list(reversed(gradients))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sgd_step(X, y, mlp, learning_rate = 5e-5, momentum=None, v_t=None):\n",
        "    \"\"\"\n",
        "    Apply a stochastic gradient descent step using the sampled batch.\n",
        "    Parameters:\n",
        "        X (np.ndarray): The input features array batch, with dimension (K, D).\n",
        "        y (np.ndarray): The ground-truth of the batch, with dimension (K, 1).\n",
        "        learning_rate (float): The learning rate multiplier for the update steps in SGD.\n",
        "    Returns:\n",
        "        (List[Dict[str, np.ndarray]]): The updated layers after applying SGD.\n",
        "    \"\"\"\n",
        "    # Compute the forward pass.\n",
        "    y_hat, forward_pass = mlp.predict(X) \n",
        "\n",
        "    # Compute the partial derivative of the loss w.r.t. to predictions `y_hat`.\n",
        "    delta_output = grad_mse_loss(y, y_hat) \n",
        "\n",
        "    # Apply backpropagation algorithm to compute the gradients of the MLP parameters.\n",
        "    gradients = backpropagate(mlp.layers, forward_pass, delta_output)  \n",
        "\n",
        "    # mlp.layers and gradients are symmetric, as shown in the figure.\n",
        "    if momentum is not None and v_t is not None:\n",
        "        updated_layers, update_v = [], []\n",
        "        for layer, grad, v in zip(mlp.layers, gradients, v_t):\n",
        "            adj_W = momentum * v[\"W\"] + learning_rate * grad[\"W\"]\n",
        "            W = layer[\"W\"] - adj_W\n",
        "\n",
        "            adj_b = momentum * v[\"b\"] + learning_rate * grad[\"b\"]\n",
        "            b = layer[\"b\"] - adj_b\n",
        "\n",
        "            updated_layers.append({\"W\": W, \"b\": b,\n",
        "                                   \"activation\": layer[\"activation\"],\n",
        "                                   \"index\": layer[\"index\"]})\n",
        "\n",
        "            update_v.append({\"W\": adj_W, \"b\": adj_b})\n",
        "        return updated_layers\n",
        "    \n",
        "    elif momentum is not None or v_t is not None:\n",
        "        raise ValueError(\"Need both momentum and v_t together.\")\n",
        "    \n",
        "    else:\n",
        "        updated_layers = []\n",
        "        for layer, grad in zip(mlp.layers, gradients):\n",
        "            W = layer[\"W\"] - learning_rate * grad[\"W\"] \n",
        "            b = layer[\"b\"] - learning_rate * grad[\"b\"] \n",
        "            updated_layers.append({\"W\": W, \"b\": b,\n",
        "                                # keep the activation function.\n",
        "                                \"activation\": layer[\"activation\"],\n",
        "                                # We use the index for asserts and debugging purposes only.\n",
        "                                \"index\": layer[\"index\"]})\n",
        "    return updated_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sgd(X_train, y_train, X_test, y_test, mlp, learning_rate = 5e-5,\n",
        "        n_epochs=300, minibatchsize=8, seed=42, momentum=None):\n",
        "    \"\"\"\n",
        "    Run the Stochastic Gradient Descent (SGD) algorithm to optimise the parameters of MLP model to fit it on\n",
        "    the training data using MSE loss.\n",
        "\n",
        "    Parameters:\n",
        "        X_train (np.ndarray): The training data features, with shape (|D_train|, D).\n",
        "        y_train (np.ndarray): The training data ground-truth, with shape (|D_train|, 1).\n",
        "        X_test (np.ndarray): The testing data features, with shape (|D_test|, D).\n",
        "        y_test (np.ndarray): The testing data ground-truth, with shape (|D_test|, 1).\n",
        "        mlp (MLP): The MLP object enacpsulating the MLP model.\n",
        "        learning_rate (float): The learning_rate multiplier used in updating the parameters at each iteration.\n",
        "        n_epochs (int): The number of training cycles that each covers the entire training examples.\n",
        "        minibatchsize (int): The batch size used in each SGD step.\n",
        "        seed (int): A seed for the RNG to ensure reproducibility across runtime sessions.\n",
        "    \"\"\"\n",
        "\n",
        "    # get random number generator\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # compute number of iterations per epoch\n",
        "    n_iterations = int(len(y_train) / minibatchsize)\n",
        "\n",
        "    # store losses\n",
        "    losses_train = []\n",
        "    losses_test = []\n",
        "\n",
        "    epochs_bar = tqdm(range(n_epochs))\n",
        "\n",
        "    if momentum is not None:\n",
        "        for i in epochs_bar:\n",
        "\n",
        "            # shuffle data\n",
        "            p = rng.permutation(len(y_train))\n",
        "            X_train_shuffled = X_train[p]\n",
        "            y_train_shuffled = y_train[p]\n",
        "\n",
        "            v_t = [{\"W\": np.zeros_like(layer[\"W\"]), \"b\": np.zeros_like(layer[\"b\"])} for layer in mlp.layers]\n",
        "\n",
        "            for j in range(n_iterations):\n",
        "                # get batch\n",
        "                X_batch = X_train_shuffled[j*minibatchsize : (j+1)*minibatchsize]\n",
        "                y_batch = y_train_shuffled[j*minibatchsize : (j+1)*minibatchsize]\n",
        "\n",
        "                # apply sgd step\n",
        "                updated_layers = sgd_step(X_batch, y_batch, mlp, learning_rate, momentum, v_t) \n",
        "\n",
        "                # update weights and biases of MLP\n",
        "                mlp.layers = updated_layers \n",
        "\n",
        "            # compute loss at the end of each epoch\n",
        "            y_hat_train, _ = mlp.predict(X_train)\n",
        "            losses_train.append(mse(y_train, y_hat_train).squeeze())\n",
        "            y_hat_test, _ = mlp.predict(X_test)\n",
        "            losses_test.append(mse(y_test, y_hat_test).squeeze())\n",
        "            epochs_bar.set_description(f'train_loss: {losses_train[-1]:.2f}, '\n",
        "                                    f'test_loss: {losses_test[-1]:.2f}, '\n",
        "                                    f'train_R^2: {rsq(y_train, y_hat_train):.2f} '\n",
        "                                    f'test_R^2: {rsq(y_test, y_hat_test):.2f} ')\n",
        "    else:\n",
        "        for i in epochs_bar:\n",
        "\n",
        "            # shuffle data\n",
        "            p = rng.permutation(len(y_train))\n",
        "            X_train_shuffled = X_train[p]\n",
        "            y_train_shuffled = y_train[p]\n",
        "\n",
        "            for j in range(n_iterations):\n",
        "                # get batch\n",
        "                X_batch = X_train_shuffled[j*minibatchsize : (j+1)*minibatchsize]\n",
        "                y_batch = y_train_shuffled[j*minibatchsize : (j+1)*minibatchsize]\n",
        "\n",
        "                # apply sgd step\n",
        "                updated_layers = sgd_step(X_batch, y_batch, mlp, learning_rate) \n",
        "\n",
        "                # update weights and biases of MLP\n",
        "                mlp.layers = updated_layers \n",
        "\n",
        "            # compute loss at the end of each epoch\n",
        "            y_hat_train, _ = mlp.predict(X_train)\n",
        "            losses_train.append(mse(y_train, y_hat_train).squeeze())\n",
        "            y_hat_test, _ = mlp.predict(X_test)\n",
        "            losses_test.append(mse(y_test, y_hat_test).squeeze())\n",
        "            epochs_bar.set_description(f'train_loss: {losses_train[-1]:.2f}, '\n",
        "                                    f'test_loss: {losses_test[-1]:.2f}, '\n",
        "                                    f'train_R^2: {rsq(y_train, y_hat_train):.2f} '\n",
        "                                    f'test_R^2: {rsq(y_test, y_hat_test):.2f} ')\n",
        "    return mlp, losses_train, losses_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of layers: 4\n",
            "Number of trainable parameters: 5801\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8b7580cf4cd43a9b07bf099dd3fbe69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "mlp = MLP(seed=2)\n",
        "mlp.add_layer(12, 50)\n",
        "mlp.add_layer(50, 50, \"activation\")\n",
        "mlp.add_layer(50, 50, \"activation\")\n",
        "mlp.add_layer(50, 1, \"activation\")\n",
        "print(\"Number of layers:\",mlp.n_layers())\n",
        "print(\"Number of trainable parameters:\",mlp.n_parameters())\n",
        "\n",
        "n_epochs = 300\n",
        "mlp, losses_train, losses_test = sgd(X_train, y_train.to_numpy(), X_test, y_test.to_numpy(),\n",
        "                                     mlp, learning_rate = 5e-5,\n",
        "                                     n_epochs=n_epochs,\n",
        "                                     minibatchsize=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_mlp(mlp, X):\n",
        "    \"\"\"\n",
        "    Apply the forward pass on the input X and produce prediction.\n",
        "\n",
        "    Parameters:\n",
        "        X (np.ndarray): Feature matrix.\n",
        "\n",
        "    Returns:\n",
        "        (np.ndarray): The predictions of the MLP model.\n",
        "    \"\"\"\n",
        "    y_pred, _ = mlp.predict(X)\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mlp_results(mlp, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Return the R^2 and MSE score of the MLP on the data.\n",
        "\n",
        "    Parameters:\n",
        "        mlp (MLP): The trained MLP model.\n",
        "        X (np.ndarray): The input features array, with shape (|D|, D).\n",
        "        y (np.ndarray): The ground-truth of the input features, with shape (|D|, 1).\n",
        "\n",
        "    Returns:\n",
        "        (float, float): The R^2 and MSE score of the MLP model.\n",
        "    \"\"\"\n",
        "    y_pred = predict_mlp(mlp, X_test)\n",
        "    return mse(y_test, y_pred), rsq(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j1--gzeMgk9"
      },
      "source": [
        "<a name=\"task-122\"></a>\n",
        "\n",
        "### (1.2.2) [(index)](#index-task-122)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MccP5-sMgqU"
      },
      "source": [
        "<a name=\"task-123\"></a>\n",
        "\n",
        "### (1.2.3) [(index)](#index-task-123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzPd5qZZPzpM"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ1n70CNMguM"
      },
      "source": [
        "<a name=\"task-2\"></a>\n",
        "\n",
        "# (2) Task 2: Classification [(index)](#index-task-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snlMZuPqMgxd"
      },
      "source": [
        "<a name=\"task-21\"></a>\n",
        "\n",
        "## (2.1) k-Nearest Neighbours [(index)](#index-task-21)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVwRifs5Mg0s"
      },
      "source": [
        "<a name=\"task-211\"></a>\n",
        "\n",
        "### (2.1.1) [(index)](#index-task-211)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrF3U23EMg3k"
      },
      "source": [
        "<a name=\"task-212\"></a>\n",
        "\n",
        "### (2.1.2) [(index)](#index-task-212)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIUs15tlMg9c"
      },
      "source": [
        "<a name=\"task-213\"></a>\n",
        "\n",
        "### (2.1.3) [(index)](#index-task-213)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ3fJoMCPpgE"
      },
      "source": [
        "<a name=\"task-214\"></a>\n",
        "\n",
        "### (2.1.4) [(index)](#index-task-214)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6Zz2bcMP6Wk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0K6bbpHPUYE"
      },
      "source": [
        "<a name=\"task-22\"></a>\n",
        "\n",
        "## (2.2) Logistic regression vs kernel logistic regression [(index)](#index-task-22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pVdpUHZP-oE"
      },
      "source": [
        "<a name=\"task-221\"></a>\n",
        "\n",
        "### (2.2.1) [(index)](#index-task-221)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_MkiUD4QB0k"
      },
      "source": [
        "<a name=\"task-222\"></a>\n",
        "\n",
        "### (2.2.2) [(index)](#index-task-222)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzUYT08xQDV9"
      },
      "source": [
        "<a name=\"task-223\"></a>\n",
        "\n",
        "### (2.2.3) [(index)](#index-task-223)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrhLFEMnPTy9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYSz-NFcEoFA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
