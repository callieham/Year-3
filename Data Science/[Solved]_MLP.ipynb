{"cells":[{"cell_type":"markdown","source":["# Multilayer Perceptron (MLP)\n","\n","The purpose of this notebook is to practice implementing the multilayer perceptron (MLP) model from scratch.\n","\n","\n","\n","\n","The MLP is a type of neural network model, that composes multiple affine transformations together, and applying a pointwise nonlinearity in between. Equations for the hidden layers $\\boldsymbol{h}^{(k)}\\in\\mathbb{R}^{n_k}$, $k=0,1,...,L$ with $L$ the number of hidden layers in the model, and the output $\\hat{\\boldsymbol{y}}\\in\\mathbb{R}^{n_{L+1}}$ are given in the following:\n","\n","$$\n","\\begin{align}\n","\\boldsymbol{h}^{(0)} &:= \\boldsymbol{x},\\\\\n","\\boldsymbol{a}^{(k)} &=  \\boldsymbol{W}^{(k-1)} \\boldsymbol{h}^{(k-1)} + \\boldsymbol{b}^{(k-1)},\\\\\n","\\boldsymbol{h}^{(k)} &= \\sigma( \\boldsymbol{a}^{(k)}),\\qquad k=1,\\ldots, L,\\\\\n","\\hat{\\boldsymbol{y}} &= \\sigma_{out}\\left( \\boldsymbol{W}^{(L)} \\boldsymbol{h}^{(L)} + \\boldsymbol{b}^{(L)} \\right),\n","\\end{align}\n","$$\n","\n","where $\\boldsymbol x\\in\\mathbb{R}^D$ is the input vector, $\\boldsymbol{W}^{(k)}\\in\\mathbb{R}^{n_{k+1}\\times n_{k}}$ are the weights, $\\boldsymbol{b}^{(k)}\\in\\mathbb{R}^{n_{k+1}}$ are the biases, $\\sigma, \\sigma_{out}: \\mathbb{R}\\mapsto\\mathbb{R}$ are activation functions that operate element-wise, $n_k$ is the number of units in the $k$-th hidden layer, and we have set $n_0 := D$. Moreover, $\\boldsymbol{a}^{(k)}$ is called the pre-activation and $\\boldsymbol{h}^{(k)}$ the post-activation of layer $k$.\n","\n","#### Test Cases\n","\n","Some of the tasks in this notebook are followed by cells that you can use to verify your solution."],"metadata":{"id":"NX_65NoOO6Qc"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Raz3J-NF3OQd","executionInfo":{"status":"ok","timestamp":1706904234961,"user_tz":0,"elapsed":390,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Imports used for testing.\n","import numpy.testing as npt"]},{"cell_type":"markdown","metadata":{"id":"gJVCL2IuQzY3"},"source":["## 1. Data preprocessing\n","\n","In this notebook we will use the California Housing data, which you have already seen in week 2 (kNN notebook)."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8pOVoN4Y3OQ6","outputId":"f0425e67-025e-436f-abc8-4b5becb56e4a","executionInfo":{"status":"ok","timestamp":1706904236007,"user_tz":0,"elapsed":763,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[{"output_type":"stream","name":"stdout","text":[".. _california_housing_dataset:\n","\n","California Housing dataset\n","--------------------------\n","\n","**Data Set Characteristics:**\n","\n","    :Number of Instances: 20640\n","\n","    :Number of Attributes: 8 numeric, predictive attributes and the target\n","\n","    :Attribute Information:\n","        - MedInc        median income in block group\n","        - HouseAge      median house age in block group\n","        - AveRooms      average number of rooms per household\n","        - AveBedrms     average number of bedrooms per household\n","        - Population    block group population\n","        - AveOccup      average number of household members\n","        - Latitude      block group latitude\n","        - Longitude     block group longitude\n","\n","    :Missing Attribute Values: None\n","\n","This dataset was obtained from the StatLib repository.\n","https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n","\n","The target variable is the median house value for California districts,\n","expressed in hundreds of thousands of dollars ($100,000).\n","\n","This dataset was derived from the 1990 U.S. census, using one row per census\n","block group. A block group is the smallest geographical unit for which the U.S.\n","Census Bureau publishes sample data (a block group typically has a population\n","of 600 to 3,000 people).\n","\n","A household is a group of people residing within a home. Since the average\n","number of rooms and bedrooms in this dataset are provided per household, these\n","columns may take surprisingly large values for block groups with few households\n","and many empty houses, such as vacation resorts.\n","\n","It can be downloaded/loaded using the\n",":func:`sklearn.datasets.fetch_california_housing` function.\n","\n",".. topic:: References\n","\n","    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n","      Statistics and Probability Letters, 33 (1997) 291-297\n","\n"]}],"source":["# Load the dataset\n","\n","from sklearn.datasets import fetch_california_housing\n","\n","california_housing_data = fetch_california_housing()\n","\n","# get features and target\n","X = california_housing_data.data\n","y = california_housing_data.target\n","\n","# Print details of dataset\n","print(california_housing_data['DESCR'])"]},{"cell_type":"markdown","metadata":{"id":"yBgz_tTOSOH1"},"source":["As usual, we create a train and test split of the data."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hYKEPz04SVlf","executionInfo":{"status":"ok","timestamp":1706904236007,"user_tz":0,"elapsed":5,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["# shuffling the rows in X and y\n","rng = np.random.default_rng(2)\n","p = rng.permutation(len(y))\n","Xp = X[p]\n","yp = y[p]\n","\n","# we split train to test as 80:20\n","split_rate = 0.8\n","X_train, X_test = np.split(Xp, [int(split_rate*(Xp.shape[0]))])\n","y_train, y_test = np.split(yp, [int(split_rate*(yp.shape[0]))])"]},{"cell_type":"markdown","metadata":{"id":"bPPCblSQT_6f"},"source":["We then standardise the train and test data. It is important to standardize after applying the train-test split to prevent any information leakage from the test set into the train set, which can lead to over-optimistic results and unrealistic performance evaluations."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"gO_zwEHgYWDR","executionInfo":{"status":"ok","timestamp":1706904236007,"user_tz":0,"elapsed":4,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def standardise(X, X_train_=None):\n","    \"\"\"Standardise features.\n","\n","    Parameters:\n","        X (np.array): Feature matrix.\n","        X_train_ (np.array): An optional feature matrix to compute the statistics\n","            from before applying it to X. If None, just use X to compute the statistics.\n","\n","    Returns:\n","        X_std (np.array): Standardised feature matrix\n","    \"\"\"\n","    if X_train_ is None:\n","        X_train_ = X\n","\n","    mu = np.mean(X_train_, axis=0, keepdims=True) ## <-- SOLUTION\n","    sigma = np.std(X_train_, axis=0, keepdims=True) ## <-- SOLUTION\n","    X_std = (X - mu) / sigma ## <-- SOLUTION\n","    return X_std"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"2CSVLJ-nT-Hx","executionInfo":{"status":"ok","timestamp":1706904236007,"user_tz":0,"elapsed":4,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["# standardize train and test data\n","X_train = standardise(X_train)\n","X_test = standardise(X_test, X_train_=X_train)"]},{"cell_type":"markdown","metadata":{"id":"FAMrZQ9n3OQ5"},"source":["## 2. Implementation of 3-layer MLP\n","\n","Now you should implement an MLP regression model in numpy. The model will have three hidden layers with 64 neurons each, and using a ReLU activation function. The final output layer will be a single neuron with no activation function to predict the target variable $y$.\n","\n","The building block of the model is the dense (or fully-connected) layer. The following function should implement the affine transformation of this layer, given kernel and bias parameters and the input to the layer. It should return the layer pre-activations (no activation function)."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"KEMpbrBS3OQ9","executionInfo":{"status":"ok","timestamp":1706904236007,"user_tz":0,"elapsed":4,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def dense(X, W, b):\n","    \"\"\"Full-connected MLP layer.\n","\n","    Parameters:\n","        X (np.ndarray): K x h_in array of inputs, where K is the batch size and h_in if the input features dimension.\n","        W (np.ndarray): h_out x h_in array for kernel matrix parametersm, where h_out is the output dimension.\n","        b (np.ndarray): Length h_out 1-D array for bias parameters\n","\n","    Returns:\n","        a (np.ndarray): K x h_out array of pre-activations\n","    \"\"\"\n","    a = np.vstack([W @ x + b for x in X]) ## <-- SOLUTION\n","    return a"]},{"cell_type":"markdown","metadata":{"id":"7A29pjM23OQ-"},"source":["The hidden layers of our model will use a *ReLU* activation function, given by:\n","\n","$$\\sigma_r(x)=\\max(0,x)$$\n","\n","In the following cell, you should implement the *ReLU* activation and its gradient, which will be required in the backpropagation of the MLP."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"y6YPF3_z3OQ-","executionInfo":{"status":"ok","timestamp":1706904236008,"user_tz":0,"elapsed":4,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def relu_activation(a):\n","    \"\"\"ReLU activation function.\n","\n","    Parameters:\n","        a: K x h_out array of pre-activations\n","\n","    Returns:\n","        h: K x h_out array of post-activations\n","    \"\"\"\n","    # compute post-activations\n","    h = np.maximum(a, 0.)  ## <-- SOLUTION\n","    return h\n","\n","## EDIT THIS FUNCTION\n","def grad_relu_activation(a):\n","    \"\"\"Gradient of ReLU activation function.\n","\n","    Parameters:\n","        a: K x h_out array of pre-activations\n","\n","    Returns:\n","        grad: K x h_out gradient array of post-activations\n","    \"\"\"\n","    # compute gradient\n","    grad = np.zeros_like(a) ## <-- SOLUTION\n","    grad[a>0] = 1 ## <-- SOLUTION\n","    return grad"]},{"cell_type":"markdown","metadata":{"id":"LPV29hKc3OQ_"},"source":["Our MLP will need the following parameters:\n","\n","Input layer -> first hidden layer:\n","* Kernel $\\boldsymbol{W}^{(0)} \\in\\mathbb{R}^{64 \\times 8}$\n","* Bias $\\boldsymbol{b}^{(0)} \\in\\mathbb{R}^{64}$\n","\n","Hidden layer -> hidden layer:\n","* Kernel $\\boldsymbol{W}^{(k)} \\in\\mathbb{R}^{64\\times 64}$, $k=1, 2$\n","* Bias $\\boldsymbol{b}^{(k)} \\in\\mathbb{R}^{64}$, $k=1, 2$\n","\n","Hidden layer -> output layer:\n","* Kernel $\\boldsymbol{W}^{(3)} \\in\\mathbb{R}^{1 \\times 64}$\n","* Bias $\\boldsymbol{b}^{(3)} \\in\\mathbb{R}^{1}$\n","\n","We will create these parameters as numpy arrays, and initialise the kernel values with samples from a zero-mean Gaussian distribution with variance $2 / (n_{in} + n_{out})$, where $n_{in}$ and $n_{out}$ are the number of neurons going in and out of the dense layer respectively. This initialisation strategy is known as [Glorot initialisation](http://proceedings.mlr.press/v9/glorot10a.html). The bias parameters will be initialised to zeros."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"DejTZWzh3ORA","executionInfo":{"status":"ok","timestamp":1706904236008,"user_tz":0,"elapsed":4,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["# fix seed for random numbers\n","np.random.seed(42)\n","\n","# create the parameters using Glorot initialisation\n","var0 = 2. / (64 + 8)\n","W0 = np.random.randn(64, 8) * np.sqrt(var0)\n","b0 = np.zeros(64)\n","\n","var1 = 2. / (64 + 64)\n","W1 = np.random.randn(64, 64) * np.sqrt(var1)\n","b1 = np.zeros(64)\n","\n","var2 = 2. / (64 + 64)\n","W2 = np.random.randn(64, 64) * np.sqrt(var2)\n","b2 = np.zeros(64)\n","\n","var3 = 2. / (1 + 64)\n","W3 = np.random.randn(1, 64) * np.sqrt(var3)\n","b3 = np.zeros(1)"]},{"cell_type":"markdown","metadata":{"id":"RS03WI7J3ORB"},"source":["You should use these parameters and your `dense` function to create the MLP model. Remember that the hidden layers of the model should use a ReLU activation, and the output of the model should not use an activation function."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"hYINNJAb3ORC","executionInfo":{"status":"ok","timestamp":1706904236008,"user_tz":0,"elapsed":4,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def mlp_3layers(X):\n","    \"\"\"3-layer MLP with ReLU activation\n","\n","    Parameters:\n","        X: K x 8 array of inputs\n","        seed (float): Seed for random numbers epsilon\n","\n","    Returns:\n","        y:  K x 1 output array\n","    \"\"\"\n","    if X.ndim == 1:\n","        # If one example passed, add a dummy dimension for the batch.\n","        X = X.reshape(1, -1)\n","\n","    # compose 3-layer MLP\n","    h = X\n","    a = dense(h, W0, b0)\n","    h = relu_activation(a)\n","    a = dense(h, W1, b1)\n","    h = relu_activation(a)\n","    a = dense(h, W2, b2)\n","    h = relu_activation(a)\n","    y = dense(h, W3, b3)\n","    return y"]},{"cell_type":"markdown","source":["We can now make a prediction with this simple MLP model."],"metadata":{"id":"TgTZAog6PgGj"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"ZJ7zSsmK3ORD","executionInfo":{"status":"ok","timestamp":1706904237043,"user_tz":0,"elapsed":1039,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["# Get the output of your initialised model\n","y_hat_train = mlp_3layers(X_train)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AqZZlCTOFu27","outputId":"ae5fc287-4cc9-47eb-805a-be949e5f8497","executionInfo":{"status":"ok","timestamp":1706904237043,"user_tz":0,"elapsed":8,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.01139598]])"]},"metadata":{},"execution_count":11}],"source":["mlp_3layers(X_train[100])"]},{"cell_type":"markdown","metadata":{"id":"hZMEUxz1D5gZ"},"source":["**Test Cases:** To verify your implementation of `relu_activation`, `dense`, `standardise`, you should expect the following cells to execute without error messages."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"3RtaRfCwd9tM","executionInfo":{"status":"ok","timestamp":1706904237043,"user_tz":0,"elapsed":7,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["# The two lines shoud verify your implementation of\n","# relu_activation, dense, and standardise functions.\n","npt.assert_allclose(mlp_3layers(X_train[10]), 0.2109034)\n","npt.assert_allclose(mlp_3layers(X_train[100]), -0.01139598)"]},{"cell_type":"markdown","metadata":{"id":"lj8yKLKxXQt6"},"source":["## 3. Implementation of general MLP model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Qpky3-o3muv4"},"source":["We also implement two alternative activation functions. The *sigmoid* activation function $\\sigma(x)$ is given by:\n","\n","$$\\sigma_s(x) = \\frac{1}{1+\\exp(-x)},$$\n","\n","and the *tanh* activation is given by:\n","\n","$$\\sigma_t(x)=\\frac{\\exp(x)-\\exp(-x)}{exp(x)+exp(-x)}=\\frac{2}{1+\\exp(-2x)}-1$$\n","\n","In the following you need to implement both activation functions and their gradients."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"TsK8QOqldyQw","executionInfo":{"status":"ok","timestamp":1706904237043,"user_tz":0,"elapsed":6,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def tanh_activation(a):\n","    \"\"\"Tanh activation function.\n","\n","    Parameters:\n","        a: K x h_out array of pre-activations\n","\n","    Returns:\n","        h: K x h_out array of post-activations\n","    \"\"\"\n","    # compute post-activations\n","    h = 2 / (1 + np.exp(-2*a)) - 1  ## <-- SOLUTION\n","    return h\n","\n","## EDIT THIS FUNCTION\n","def grad_tanh_activation(a):\n","    \"\"\"Gradient of Tanh activation function.\n","\n","    Parameters:\n","        a: K x h_out array of pre-activations\n","\n","    Returns:\n","        grad: K x h_out gradient array of post-activations\n","    \"\"\"\n","    # compute gradient\n","    grad = 1 - tanh_activation(a)**2 ## <-- SOLUTION\n","    return grad"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"TWh0yKi9m2ti","executionInfo":{"status":"ok","timestamp":1706904237043,"user_tz":0,"elapsed":6,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def sigmoid_activation(a):\n","    \"\"\"Sigmoid activation function.\n","\n","    Parameters:\n","        a: K x h_out array of pre-activations\n","\n","    Returns:\n","        h: K x h_out array of post-activations\n","    \"\"\"\n","    # compute post-activations\n","    h = 1 / (1 + np.exp(-a))  ## <-- SOLUTION\n","    return h\n","\n","## EDIT THIS FUNCTION\n","def grad_sigmoid_activation(a):\n","    \"\"\"Gradient of Sigmoid activation function.\n","\n","    Parameters:\n","        a: K x h_out array of pre-activations.\n","\n","    Returns:\n","        grad: K x h_out gradient array of post-activations.\n","    \"\"\"\n","    # compute gradient\n","    grad = sigmoid_activation(a) * (1 - sigmoid_activation(a))  ## <-- SOLUTION\n","    return grad"]},{"cell_type":"markdown","metadata":{"id":"DflYJX5wIb6W"},"source":["**Test Cases:** To verify your implementation of `tanh_activation`, `grad_tanh_activation`, `sigmoid_activation`, and `grad_sigmoid_activation` run the following cell:"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"qLyVJVnQIbN-","executionInfo":{"status":"ok","timestamp":1706904237043,"user_tz":0,"elapsed":6,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["### Test Cases:\n","\n","# tanh (test against np.tanh)\n","npt.assert_allclose(tanh_activation(3), np.tanh(3))\n","npt.assert_allclose(tanh_activation(-3), np.tanh(-3))\n","\n","# sigmoid\n","npt.assert_allclose(sigmoid_activation(2.5),  0.9241418199787566)\n","npt.assert_allclose(sigmoid_activation(-3), 0.04742587317756678)\n","\n","# gradient of tanh\n","npt.assert_allclose(grad_tanh_activation(2), 0.07065082485316443)\n","npt.assert_allclose(grad_tanh_activation(-1), 0.41997434161402614)\n","\n","# gradient of sigmoid\n","npt.assert_allclose(grad_sigmoid_activation(2), 0.10499358540350662)\n","npt.assert_allclose(grad_sigmoid_activation(-1), 0.19661193324148185)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"F1uF3al-KqHm"},"source":["To generalise our code, we write a MLP class where we can choose the number of layers and dimensions flexibely. Our strategy is to define `layers` attribute that stores all information on the layers as a list of dictionaries, with keys `\"W\"` corresponding to the weights $\\boldsymbol{W}^{(k)}\\in\\mathbb{R}^{n_{k+1}\\times n_{k}}$, `\"b\"` to the biases $\\boldsymbol{b}^{(k)}\\in\\mathbb{R}^{n_{k+1}}$ and `\"activation\"` for the activation functions $\\sigma^{(k)}: \\mathbb{R}\\mapsto\\mathbb{R}$. Below is an illustration for two layers. You will have to implement a method `add_layer` that adds a layer to to the MLP."]},{"cell_type":"markdown","source":["![layers](https://raw.githubusercontent.com/barahona-research-group/mfds-resources/main/images/mlp.drawio.svg)"],"metadata":{"id":"iG03l22q8RyV"}},{"cell_type":"markdown","source":["Once the architecture of the MLP is defined one can make predictions by computing a forward pass. You will implement this in the `predict` method of the MLP class, which returns the predication and the all the pre-activations and post-activations in the forward pass (again as a list of dictionaries with keys `\"a\"` and `\"h\"` as visualised below). We need the information from the forward pass later for the backpropagation."],"metadata":{"id":"KwX55ENdCAgB"}},{"cell_type":"markdown","source":["![forward-pass](https://raw.githubusercontent.com/barahona-research-group/mfds-resources/main/images/mlp-forward.drawio.svg)"],"metadata":{"id":"axJAXnMLCAjg"}},{"cell_type":"code","execution_count":16,"metadata":{"id":"DMERegwRB_3A","executionInfo":{"status":"ok","timestamp":1706904237043,"user_tz":0,"elapsed":6,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["# A lookup table for activation functions by their names.\n","activation_table = {\n","    \"relu\": relu_activation,\n","    \"sigmoid\": sigmoid_activation,\n","    \"tanh\": tanh_activation,\n","    # Identity function.\n","    \"identity\": lambda x: x\n","}\n","\n","# A lookup table for gradient of activation functions by their names.\n","grad_activation_table = {\n","    \"relu\": grad_relu_activation,\n","    \"sigmoid\": grad_sigmoid_activation,\n","    \"tanh\": grad_tanh_activation,\n","    # Identity function gradient.\n","    \"identity\": lambda x: np.ones_like(x)\n","}\n","\n","\n","class MLP:\n","    \"\"\"\n","    This class represents a Multi-Layer Perceptron (MLP), that we are going\n","    to use to encapsulate two components:\n","        1. layers: the sequence of layers, where each layer is stored in\n","            a dictionary in the format {\"W\": np.ndarray, \"b\": np.ndarray},\n","            where \"W\" points to the weights array, and \"b\" points to\n","            the bias vector.\n","        2. rng: a pseudo random number generator (RNG) initialised to generate\n","            the random weights in a reproducible manner between different\n","            runtime sessions.\n","    This class is also shipped with methods that perform essential operations\n","    with a MLP, including:\n","        - add_layers: which creates a new layer with specified dimensions.\n","        - predict: applies the MLP forward pass to make predictions and produces\n","            a computational graph for the forward pass that can be used to\n","            compute gradients using backpropagation algorithm.\n","        in addition to other light functions that return simple statistics about\n","        the MLP.\n","    \"\"\"\n","    def __init__(self, seed=42):\n","        self.layers = []\n","        self.rng = np.random.default_rng(seed)\n","\n","    def n_parameters(self):\n","        \"\"\"Return the total number of parameters of weights and biases.\"\"\"\n","        return sum(l[\"b\"].size + l[\"W\"].size for l in self.layers)\n","\n","    def n_layers(self):\n","        \"\"\"Return current number of MLP layers.\"\"\"\n","        return len(self.layers)\n","\n","    def layer_dim(self, index):\n","        \"\"\"Retrieve the dimensions of the MLP layer at `index`.\"\"\"\n","        return self.layers[index][\"W\"].shape\n","\n","    def add_layer(self, in_dim, out_dim, activation=\"identity\"):\n","        \"\"\"Add fully connected layer to MLP.\n","\n","        Parameters:\n","            in_dim (int): The output dimension of the layer.\n","            out_dim (int): The input dimension of the layer.\n","            activation (str): The activation function name.\n","        \"\"\"\n","        # check if input-dimension matches output-dimension of previous layer\n","        if self.n_layers() > 0:\n","            last_out_dim, _ = self.layer_dim(-1)\n","            assert in_dim == last_out_dim, f\"Input-dimension {in_dim} does not match output-dimension {last_out_dim} of previous layer.\"\n","\n","        # the first layer, in our convention illustrated, does not apply activation on the input features X.\n","        if self.n_layers() == 0:\n","            assert activation == \"identity\", \"Should not apply activations on the input features X, use Identity function for the first layer.\"\n","\n","\n","        # store each layer as a dictionary in the list, as shown in the\n","        # attached diagram.\n","        self.layers.append({\n","            # only for debugging.\n","            \"index\": len(self.layers),\n","            # apply Glorot initialisation for weights.\n","            \"W\": self.rng.normal(size=(out_dim, in_dim)) * np.sqrt(2. / (in_dim + out_dim)),\n","            # initialise bias vector with zeros.\n","            \"b\": np.zeros(out_dim), ## <-- SOLUTION\n","            # store the activation function (as string)\n","            \"activation\": activation\n","        })\n","\n","    def predict(self, X):\n","        \"\"\"Apply the forward pass on the input X and produce prediction and the\n","        forward computation graph.\n","\n","        Parameters:\n","            X (np.ndarray): Feature matrix.\n","\n","        Returns:\n","            (np.ndarray, List[Dict[str, np.ndarray]]): A tuple of the\n","            predictions and the computation graph as a sequence of intermediate\n","            values through the MLP, specifically each layer will have a corresponding\n","            intermediate values {\"a\": np.ndarray, \"h\": np.ndarray}, as shown in the\n","            attached diagram above.\n","        \"\"\"\n","        # We assume that we work with a batch of examples (ndim==2).\n","        if X.ndim == 1:\n","            # If one example passed, add a dummy dimension for the batch.\n","            X = X.reshape(1, -1)\n","\n","        # store pre- and post-activations in list\n","        forward_pass = [{\"index\": 0, \"a\": X, \"h\": X}]\n","\n","        # iterate through hidden layers\n","        for k in range(1, len(self.layers)):\n","            # compute pre-activations\n","            a = dense(forward_pass[k - 1][\"h\"], self.layers[k - 1][\"W\"], self.layers[k - 1][\"b\"]) ## <-- SOLUTION\n","            activation = activation_table[self.layers[k][\"activation\"]] ## <-- SOLUTION\n","            forward_pass.append({\"index\": k, \"a\" : a, \"h\" : activation(a)}) ## <-- SOLUTION\n","\n","        y_hat = dense(forward_pass[-1][\"h\"], self.layers[-1][\"W\"], self.layers[-1][\"b\"]) ## <-- SOLUTION\n","        # predicted target is output of last layer\n","        return y_hat, forward_pass"]},{"cell_type":"markdown","metadata":{"id":"e9_2H_90L2MM"},"source":["We can now implement the same MLP architecture as before."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VrjkCH9PdnIP","outputId":"9494ec88-07f2-4a99-8f10-c633d1a85b87","executionInfo":{"status":"ok","timestamp":1706904237044,"user_tz":0,"elapsed":6,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of layers: 4\n","Number of trainable parameters: 8961\n"]}],"source":["mlp = MLP(seed=2)\n","mlp.add_layer(8, 64)\n","mlp.add_layer(64, 64, \"relu\")\n","mlp.add_layer(64, 64, \"relu\")\n","mlp.add_layer(64, 1, \"relu\")\n","print(\"Number of layers:\", mlp.n_layers())\n","print(\"Number of trainable parameters:\", mlp.n_parameters())\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"a8Mp5B5ag_RJ","executionInfo":{"status":"ok","timestamp":1706904237529,"user_tz":0,"elapsed":489,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["y_hat_train, forward_pass = mlp.predict(X_train)\n"]},{"cell_type":"markdown","metadata":{"id":"tQBfNuJEG3bZ"},"source":["**Test Cases** Now we may verify the `MLP` implementation with input examples with expected outputs. If you struggle with getting it through, you can start with simpler test cases. For example, you can start from 1-layer and a toy two dimensional input and test against what you expect by manual derivation."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"FN2FucSY1lq1","executionInfo":{"status":"ok","timestamp":1706904237530,"user_tz":0,"elapsed":6,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["mlp = MLP(seed=2)\n","mlp.add_layer(8, 64)\n","mlp.add_layer(64, 64, \"relu\")\n","mlp.add_layer(64, 64, \"relu\")\n","mlp.add_layer(64, 1, \"relu\")\n","\n","\n","case_input = X_train[:5]\n","case_expect = np.array([-0.03983787, -0.04034381,  0.14796773, -0.02884643,  0.03103732])\n","npt.assert_allclose(mlp.predict(case_input)[0].squeeze(), case_expect)"]},{"cell_type":"markdown","metadata":{"id":"NMNLSVfXIQpq"},"source":["## 4. Backpropagation in MLP"]},{"cell_type":"markdown","source":["We use the MSE loss function to evaluate the performance of our model. You will implement it in the cell below, together with its gradient."],"metadata":{"id":"sf1nEVpYQHyU"}},{"cell_type":"code","execution_count":20,"metadata":{"id":"gaIxPxn5M-Oc","executionInfo":{"status":"ok","timestamp":1706904237530,"user_tz":0,"elapsed":5,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["def mse_loss(y_true, y_pred):\n","    \"\"\"Compute MSE-loss\n","\n","    Parameters:\n","        y_true: ground-truth array, with shape (K, )\n","        y_pred: predictions array, with shape (K, )\n","\n","    Returns:\n","        loss (float): MSE-loss\n","    \"\"\"\n","    assert y_true.size == y_pred.size, \"Ground-truth and predictions have different dimensions.\"\n","\n","    # Adjustment to avoid subtraction between (K,) and (1, K) arrays.\n","    y_true = y_true.reshape(y_pred.shape)\n","\n","\n","    return np.mean((y_true - y_pred)**2, keepdims=True) ## <-- SOLUTION"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"Jyqtb5P1j304","executionInfo":{"status":"ok","timestamp":1706904237530,"user_tz":0,"elapsed":5,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["def grad_mse_loss(y_true, y_pred):\n","    \"\"\"Compute gradient of MSE-loss\n","\n","    Parameters:\n","        y_true: ground-truth values, shape: (K, ).\n","        y_pred: prediction values, shape: (K, ).\n","\n","    Returns:\n","        grad (np.ndarray): Gradient of MSE-loss, shape: (K, ).\n","    \"\"\"\n","    # Adjustment to avoid subtraction between (K,) and (1, K) arrays.\n","    y_true = y_true.reshape(y_pred.shape)\n","\n","    return 2.0 * (y_pred - y_true) / y_true.size ## <-- SOLUTION"]},{"cell_type":"markdown","metadata":{"id":"UoMu1tckK6_V"},"source":["**Test Cases:** Let's test the implemented `mse_loss` and `grad_mse_loss`.\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"njtxQ06UK20e","executionInfo":{"status":"ok","timestamp":1706904237530,"user_tz":0,"elapsed":5,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["\n","# We will use the same test inputs for mse_loss and grad_mse_loss.\n","case_input_arg1 = y_train[:5]\n","case_input_arg2 = np.array([-0.03983787, -0.04034381,  0.14796773, -0.02884643,  0.03103732])\n","\n","case_expects = 10.791119\n","npt.assert_allclose(mse_loss(case_input_arg1, case_input_arg2),\n","                    case_expects, atol=1e-4)\n","\n","\n","\n","case_expects = np.array([-2.01593915, -1.72533752, -0.34321291, -0.73353857, -0.96758507])\n","npt.assert_allclose(grad_mse_loss(case_input_arg1, case_input_arg2),\n","                    case_expects, atol=1e-4)"]},{"cell_type":"markdown","source":["In order to train our MLP model with stochastic gradient descent (SGD) we need to compute the gradients of all model parameters for a given minibatch. To do so we use the backpropagation algorithm explained in Section 8.4 of the lecture notes. Below you will implement a `backpropagate` function that returns for each layer the gradients of its weights and biases stored in a list of dictionaries with keys `\"W\"` and `\"b\"`. This data structure is visualised in the following:"],"metadata":{"id":"4lSpUAHhQMxu"}},{"cell_type":"markdown","source":["![mlp-backward](https://raw.githubusercontent.com/barahona-research-group/mfds-resources/main/images/mlp-back.drawio.svg)"],"metadata":{"id":"6tyrbfnPCtR7"}},{"cell_type":"code","execution_count":23,"metadata":{"id":"ad5YJ0AabpaJ","executionInfo":{"status":"ok","timestamp":1706904237530,"user_tz":0,"elapsed":4,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["def backpropagate(layers, forward_pass, delta_output):\n","    \"\"\"\n","    Apply the backpropagation algorithm to the MLP layers to compute the gradients starting from\n","    the output layer to the input layer, and starting the chain rule from the\n","    partial derivative of the loss function w.r.t the predictions $\\hat{y}$. The\n","\n","    Parameters:\n","        layers (List[Dict[str, np.ndarray]]): The MLP sequence of layers, as shown in the diagrams.\n","        forward_pass (List[Dict[str, np.ndarray]]): The forward pass intermediate values for\n","            each layer, representing a computation graph.\n","        delta_output (np.ndarray): the partial derivative of the loss function w.r.t the\n","            predictions $\\hat{y}$, has the shape (K, 1), where K is the batch size.\n","    Returns:\n","        (List[Dict[str, np.ndarray]]): The computed gradient using a structure symmetric the layers, as shown\n","            in the diagrams.\n","\n","    \"\"\"\n","    # Create a list that will contain the gradients of all the layers.\n","    delta = delta_output\n","\n","    assert len(layers) == len(forward_pass), \"Number of layers is expected to match the number of forward pass layers\"\n","\n","    # Iterate on layers backwardly, from output to input.\n","    # Calculate gradients w.r.t. weights and biases of each level and store in list of dictionaries.\n","    gradients = []\n","    for layer, forward_computes in reversed(list(zip(layers, forward_pass))):\n","        assert forward_computes[\"index\"] == layer[\"index\"], \"Mismatch in the index.\"\n","\n","        h = forward_computes[\"h\"]\n","        assert delta.shape[0] == h.shape[0], \"Mismatch in the batch dimension.\"\n","\n","\n","        gradients.append({\"W\" : delta.T @ h, # <-- SOLUTION\n","                          \"b\" : delta.sum(axis=0)}) # <-- SOLUTION\n","\n","        # Update the delta for the next iteration\n","        grad_activation_f = grad_activation_table[layer[\"activation\"]]\n","        grad_activation = grad_activation_f(forward_computes[\"a\"])\n","\n","        # Calculate the delta for the backward layer.\n","        delta = np.stack([np.diag(gi) @ layer[\"W\"].T @ di\n","                           for (gi, di) in zip(grad_activation, delta)]) # <-- SOLUTION.\n","\n","\n","    # Return now ordered list matching the layers.\n","    return list(reversed(gradients))"]},{"cell_type":"markdown","metadata":{"id":"rvhflMcY1lq7"},"source":["**Test Cases:** Here we conclude our tests for the implemented `MLP` by testing the gradients computed with backpropagation algorithm, which is considered the most delicate part to implement properly."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"lZl3Np991lq8","executionInfo":{"status":"ok","timestamp":1706904237530,"user_tz":0,"elapsed":4,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["my_mlp_test = MLP(seed=42)\n","my_mlp_test.add_layer(8, 3)\n","my_mlp_test.add_layer(3, 1, \"relu\")\n","\n","\n","def test_grads(mlp, X, y):\n","    y_hat, forward_pass = mlp.predict(X)\n","    delta_output_test = grad_mse_loss(y, y_hat)\n","    return backpropagate(my_mlp_test.layers, forward_pass,\n","                                  delta_output_test)\n","\n","## Test the last layer gradients with a batch of size one.\n","grad = test_grads(my_mlp_test, X_train[0], y_train[0])\n","grad_W1 = grad[1][\"W\"]\n","npt.assert_allclose(grad_W1, np.array([[-0.05504819, -3.14611296, -0.22906985]]), atol=1e-4)\n","\n","# Test the last layer gradients with a batch of size two\n","grad = test_grads(my_mlp_test, X_train[0:2], y_train[0:2])\n","grad_W1 = grad[1][\"W\"]\n","npt.assert_allclose(grad_W1, np.array([[-0.02752409, -1.57305648, -0.11453492]]), atol=1e-4)\n","\n","# Test the first layer bias with the same batch\n","grad = test_grads(my_mlp_test, X_train[0:2], y_train[0:2])\n","grad_b0 = grad[0][\"b\"]\n","npt.assert_allclose(grad_b0, np.array([ 1.53569012,  1.26250966, -1.90849604]), atol=1e-4)\n","\n"]},{"cell_type":"markdown","source":["The implementation of backpropagation can be challenging and we will go through all the steps in the tutorials."],"metadata":{"id":"cNTAX1fvQWHc"}},{"cell_type":"markdown","metadata":{"id":"lAUzjw9nTZyx"},"source":["## 5. Training the MLP with Stochastic Gradient Descent"]},{"cell_type":"markdown","source":["After computing the gradients of weights for a minibatch we can update the weights according to the SGD algorithm explained in Section 8.5.1 of the lecture notes. The `sgd_step` function below implements this for a single minibatch."],"metadata":{"id":"EGZf-4vcQY_B"}},{"cell_type":"code","execution_count":25,"metadata":{"id":"OiFJlYhxTdCZ","executionInfo":{"status":"ok","timestamp":1706904237530,"user_tz":0,"elapsed":4,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["def sgd_step(X, y, mlp, learning_rate = 1e-3):\n","    \"\"\"\n","    Apply a stochastic gradient descent step using the sampled batch.\n","    Parameters:\n","        X (np.ndarray): The input features array batch, with dimension (K, D).\n","        y (np.ndarray): The ground-truth of the batch, with dimension (K, 1).\n","        learning_rate (float): The learning rate multiplier for the update steps in SGD.\n","    Returns:\n","        (List[Dict[str, np.ndarray]]): The updated layers after applying SGD.\n","    \"\"\"\n","    # Compute the forward pass.\n","    y_hat, forward_pass = mlp.predict(X) ## <-- SOLUTION.\n","\n","    # Compute the partial derivative of the loss w.r.t. to predictions `y_hat`.\n","    delta_output = grad_mse_loss(y, y_hat) ## <-- SOLUTION\n","\n","    # Apply backpropagation algorithm to compute the gradients of the MLP parameters.\n","    gradients = backpropagate(mlp.layers, forward_pass, delta_output)  ## <-- SOLUTION.\n","\n","    # mlp.layers and gradients are symmetric, as shown in the figure.\n","    updated_layers = []\n","    for layer, grad in zip(mlp.layers, gradients):\n","        W = layer[\"W\"] - learning_rate * grad[\"W\"] ## <-- SOLUTION.\n","        b = layer[\"b\"] - learning_rate * grad[\"b\"] ## <-- SOLUTION.\n","        updated_layers.append({\"W\": W, \"b\": b,\n","                               # keep the activation function.\n","                               \"activation\": layer[\"activation\"],\n","                               # We use the index for asserts and debugging purposes only.\n","                               \"index\": layer[\"index\"]})\n","    return updated_layers"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"ApSQylNg2h_d","executionInfo":{"status":"ok","timestamp":1706904237530,"user_tz":0,"elapsed":3,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["def r2_score(y, y_hat):\n","    \"\"\"R^2 score to assess regression performance.\"\"\"\n","\n","    # Adjustment to avoid subtraction between (K,) and (1, K) arrays.\n","    y = y.reshape(y_hat.shape)\n","    y_bar = y.mean()\n","\n","    ss_tot = ((y - y_bar)**2).sum()\n","    ss_res = ((y - y_hat)**2).sum()\n","    return 1 - (ss_res/ss_tot)\n"]},{"cell_type":"markdown","source":["A full training of the parameters over several epochs with SGD is then implemented in the next cell."],"metadata":{"id":"SXchjvr-QiI7"}},{"cell_type":"code","execution_count":27,"metadata":{"id":"i91Z9U0524gC","executionInfo":{"status":"ok","timestamp":1706904237892,"user_tz":0,"elapsed":365,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["from tqdm.notebook import tqdm # just a fancy progress bar\n"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"68H26z2e9ujN","executionInfo":{"status":"ok","timestamp":1706904237893,"user_tz":0,"elapsed":7,"user":{"displayName":"Asem Alaa","userId":"03028991525006543936"}}},"outputs":[],"source":["def sgd(X_train, y_train, X_test, y_test, mlp, learning_rate = 1e-3,\n","        n_epochs=10, minibatchsize=1, seed=42):\n","    \"\"\"\n","    Run the Stochastic Gradient Descent (SGD) algorithm to optimise the parameters of MLP model to fit it on\n","    the training data using MSE loss.\n","\n","    Parameters:\n","        X_train (np.ndarray): The training data features, with shape (|D_train|, D).\n","        y_train (np.ndarray): The training data ground-truth, with shape (|D_train|, 1).\n","        X_test (np.ndarray): The testing data features, with shape (|D_test|, D).\n","        y_test (np.ndarray): The testing data ground-truth, with shape (|D_test|, 1).\n","        mlp (MLP): The MLP object enacpsulating the MLP model.\n","        learning_rate (float): The learning_rate multiplier used in updating the parameters at each iteration.\n","        n_epochs (int): The number of training cycles that each covers the entire training examples.\n","        minibatchsize (int): The batch size used in each SGD step.\n","        seed (int): A seed for the RNG to ensure reproducibility across runtime sessions.\n","    \"\"\"\n","\n","    # get random number generator\n","    rng = np.random.default_rng(seed)\n","\n","    # compute number of iterations per epoch\n","    n_iterations = int(len(y_train) / minibatchsize)\n","\n","    # store losses\n","    losses_train = []\n","    losses_test = []\n","\n","    epochs_bar = tqdm(range(n_epochs))\n","    for i in epochs_bar:\n","\n","        # shuffle data\n","        p = rng.permutation(len(y_train))\n","        X_train_shuffled = X_train[p]\n","        y_train_shuffled = y_train[p]\n","\n","        for j in range(n_iterations):\n","            # get batch\n","            X_batch = X_train_shuffled[j*minibatchsize : (j+1)*minibatchsize]\n","            y_batch = y_train_shuffled[j*minibatchsize : (j+1)*minibatchsize]\n","\n","            # apply sgd step\n","            updated_layers = sgd_step(X_batch, y_batch, mlp, learning_rate) ## <-- SOLUTION.\n","\n","            # update weights and biases of MLP\n","            mlp.layers = updated_layers ## <-- SOLUTION.\n","\n","        # compute loss at the end of each epoch\n","        y_hat_train, _ = mlp.predict(X_train)\n","        losses_train.append(mse_loss(y_train, y_hat_train).squeeze())\n","        y_hat_test, _ = mlp.predict(X_test)\n","        losses_test.append(mse_loss(y_test, y_hat_test).squeeze())\n","        epochs_bar.set_description(f'train_loss: {losses_train[-1]:.2f}, '\n","                                   f'test_loss: {losses_test[-1]:.2f}, '\n","                                   f'train_R^2: {r2_score(y_train, y_hat_train):.2f} '\n","                                   f'test_R^2: {r2_score(y_test, y_hat_test):.2f} ')\n","    return mlp, losses_train, losses_test\n"]},{"cell_type":"markdown","source":["Below, we train our MLP model with increased number of dimensions and tanh-activation function in the hidden layers to achieve a good performance."],"metadata":{"id":"PldRZ0AnQwCw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"scVxSQuJ28Xl","colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["7858e3a083f645e28eb6de41181c898a","7fcfc6affd7c45e28acb23a71677a0ce","bc472f8a33d345ba94bce828691481a0","726e1cc7c67546c9bcf0955189f1bddd","85c9698a33c545cfa2c341187356debd","84e3be51d9a5493195c131224b91ab1d","f6086ec2f46b4f118f024dea07715866","d797dfe5695543ff80510629333829c8","df0f2c8e9a3c455084a14cc2cfc16383","9b0f44a6766144a88420db154bc0a783","796e7eeb39224544be90e95ffc6daf2e"]},"outputId":"22105f52-900e-47fd-d13e-2452dba9b8b3"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Number of layers: 4\n","Number of trainable parameters: 797185\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7858e3a083f645e28eb6de41181c898a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["mlp = MLP(seed=2)\n","mlp.add_layer(X_train.shape[1], 64)\n","mlp.add_layer(64, 128, \"relu\")\n","mlp.add_layer(128, 8, \"sigmoid\")\n","mlp.add_layer(8, 1, \"relu\")\n","print(\"Number of layers:\",mlp.n_layers())\n","print(\"Number of trainable parameters:\",mlp.n_parameters())\n","\n","n_epochs = 200\n","mlp, losses_train, losses_test = sgd(X_train, y_train, X_test, y_test,\n","                                     mlp, learning_rate = 5e-4,\n","                                     n_epochs=n_epochs,\n","                                     minibatchsize=64)"]},{"cell_type":"markdown","source":["We can plot the progress of the training below."],"metadata":{"id":"KSX_KxF2Qx-e"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wq50481I3WGb"},"outputs":[],"source":["# plot training progress\n","fig, ax = plt.subplots(figsize=(12, 8))\n","ax.plot(np.arange(1,n_epochs+1),losses_train, label=\"Train\")\n","ax.plot(np.arange(1,n_epochs+1),losses_test, label=\"Test\")\n","ax.set(title=\"Losses vs Epochs\", xlabel = \"Epoch\", ylabel = \"MSE\")\n","ax.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"E3zduFv_3ORM"},"source":["#### Questions\n","1. How do the training and test losses above compare?\n","2. Play around with different numbers of hidden layers and number of neurons in the MLP. How is the performance of the model affected?\n","3. Play around with activation functions other than ReLU."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7858e3a083f645e28eb6de41181c898a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7fcfc6affd7c45e28acb23a71677a0ce","IPY_MODEL_bc472f8a33d345ba94bce828691481a0","IPY_MODEL_726e1cc7c67546c9bcf0955189f1bddd"],"layout":"IPY_MODEL_85c9698a33c545cfa2c341187356debd"}},"7fcfc6affd7c45e28acb23a71677a0ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84e3be51d9a5493195c131224b91ab1d","placeholder":"​","style":"IPY_MODEL_f6086ec2f46b4f118f024dea07715866","value":"  0%"}},"bc472f8a33d345ba94bce828691481a0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_d797dfe5695543ff80510629333829c8","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df0f2c8e9a3c455084a14cc2cfc16383","value":0}},"726e1cc7c67546c9bcf0955189f1bddd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b0f44a6766144a88420db154bc0a783","placeholder":"​","style":"IPY_MODEL_796e7eeb39224544be90e95ffc6daf2e","value":" 0/200 [00:00&lt;?, ?it/s]"}},"85c9698a33c545cfa2c341187356debd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84e3be51d9a5493195c131224b91ab1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6086ec2f46b4f118f024dea07715866":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d797dfe5695543ff80510629333829c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df0f2c8e9a3c455084a14cc2cfc16383":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9b0f44a6766144a88420db154bc0a783":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"796e7eeb39224544be90e95ffc6daf2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}